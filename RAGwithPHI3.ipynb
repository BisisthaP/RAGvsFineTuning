{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isb5gTlwnjgY"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGHELlhYnlgw"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2mRrwyUqfzO"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb3QwyMZokgp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(\"numpy version:\", np.__version__)  # MUST be 2.0.2 or higher\n",
        "\n",
        "import torch\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"CUDA available?\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuZIlepjGN27"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4aCPbgZosMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "print(\"Loading Phi-3 (Applying RoPE Scaling Fix)...\")\n",
        "\n",
        "# 1. Load the config\n",
        "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# 2. Fix the RoPE Scaling dictionary\n",
        "# If it's \"default\", the remote script crashes. We remove it to use standard embeddings.\n",
        "if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n",
        "    scaling_type = config.rope_scaling.get(\"type\") or config.rope_scaling.get(\"rope_type\")\n",
        "    if scaling_type is None or scaling_type == \"default\":\n",
        "        config.rope_scaling = None  # This is the fix for \"Unknown RoPE scaling type default\"\n",
        "    else:\n",
        "        # Ensure 'type' is present if it's a valid type like 'su'\n",
        "        config.rope_scaling[\"type\"] = scaling_type\n",
        "\n",
        "# 3. Load tokenizer\n",
        "phi_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# 4. Load model\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "print(\"\\n✨ Phi-3 loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "e33fd63d278243aebb7adde55e01665e",
            "72eb2c14ca594f98878ca3f84337b165",
            "ed5599eaa3fb4b02997841e3fd6cb7dd",
            "b1944e664251489d893e52ad684c4c1b",
            "fb827edcbf0f417983fdb29feb1277d0",
            "0f954a96a0064a31906f8863c17b69ed",
            "c39283c812614936a324f3fca3ea1605",
            "dfda0b1886274063aadd6c0129d103d0",
            "99f670ff963840308318b8342866b45a",
            "a890de9023ab4c29b259eff46c1d6118",
            "3cc8f059ff2b43b1aa064d531797b7ff"
          ]
        },
        "id": "xerFWxenpdhe",
        "outputId": "769bef3f-e525-462f-b3c1-398e742075b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading FAISS index, embedder, and corpus metadata...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e33fd63d278243aebb7adde55e01665e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS and retrieve() fully loaded!\n",
            "Total chunks in index: 402\n",
            "Embedder model: BAAI/bge-small-en-v1.5\n",
            "retrieve function ready → test with retrieve('Bitcoin proof of work')\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"Loading FAISS index, embedder, and corpus metadata...\")\n",
        "\n",
        "# Load FAISS index\n",
        "index = faiss.read_index(\"crypto_rag_index.faiss\")\n",
        "\n",
        "# Load corpus metadata (texts, sources, pages, embedder model name)\n",
        "with open(\"crypto_corpus_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = json.load(f)\n",
        "\n",
        "texts = corpus[\"texts\"]\n",
        "sources = corpus[\"sources\"]\n",
        "pages = corpus[\"pages\"]\n",
        "embedder_model_name = corpus[\"embedder_model\"]\n",
        "\n",
        "# Reload embedder\n",
        "embedder = SentenceTransformer(embedder_model_name)\n",
        "\n",
        "# Define retrieve function (using the loaded index/embedder/texts)\n",
        "def retrieve(query, k=3, min_score=0.22):\n",
        "    \"\"\"\n",
        "    Retrieve top-k relevant chunks using FAISS.\n",
        "    Returns list of dicts with text_preview, score, source, page.\n",
        "    \"\"\"\n",
        "    query_emb = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n",
        "\n",
        "    # Search (FAISS returns distances as cosine distance = 1 - similarity)\n",
        "    distances, indices = index.search(query_emb.reshape(1, -1), k)\n",
        "\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        if idx == -1 or idx >= len(texts):\n",
        "            continue\n",
        "        similarity = 1 - dist  # convert to cosine similarity\n",
        "        if similarity < min_score:\n",
        "            continue\n",
        "\n",
        "        text = texts[idx]\n",
        "        preview = text[:300] + \"...\" if len(text) > 300 else text\n",
        "\n",
        "        results.append({\n",
        "            \"text_preview\": preview,\n",
        "            \"text\": text,\n",
        "            \"score\": round(float(similarity), 3),\n",
        "            \"source\": sources[idx],\n",
        "            \"page\": pages[idx] if pages and idx < len(pages) else None\n",
        "        })\n",
        "\n",
        "    return results[:k]  # ensure max k results\n",
        "\n",
        "print(\"FAISS and retrieve() fully loaded!\")\n",
        "print(f\"Total chunks in index: {index.ntotal}\")\n",
        "print(f\"Embedder model: {embedder_model_name}\")\n",
        "print(f\"retrieve function ready → test with retrieve('Bitcoin proof of work')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvzLI2OVp47C",
        "outputId": "32bcad2e-dac0-45f3-f641-e23870b59a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw FAISS distances & indices:\n",
            "Index 14: similarity = 0.2170, dist = 0.7830\n",
            "Index 13: similarity = 0.2178, dist = 0.7822\n",
            "Index 54: similarity = 0.2243, dist = 0.7757\n",
            "Index 15: similarity = 0.2266, dist = 0.7734\n",
            "Index 46: similarity = 0.2300, dist = 0.7700\n",
            "\n",
            "Retrieved 5 hits with min_score=0.15:\n",
            "Score 0.217 | Source bitcoin.pdf | Preview: . For our timestamp network, we implement the proof-of-work by incrementing a nonce in the block unt...\n",
            "Score 0.218 | Source bitcoin.pdf | Preview: . Proof-of-Work To implement a distributed timestamp server on a peer-to-peer basis, we will need to...\n",
            "Score 0.224 | Source solana.pdf | Preview: . That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architectur...\n",
            "Score 0.227 | Source bitcoin.pdf | Preview: . The proof-of-work also solves the problem of determining representation in majority decision makin...\n",
            "Score 0.230 | Source bitcoin.pdf | Preview: . To solve this, we proposed a peer-to-peer network using proof-of-work to record a public history o...\n"
          ]
        }
      ],
      "source": [
        "test_query = \"Bitcoin proof of work\"\n",
        "test_emb = embedder.encode([test_query], normalize_embeddings=True, convert_to_numpy=True)[0]\n",
        "\n",
        "# Search (get more candidates to see scores)\n",
        "distances, indices = index.search(test_emb.reshape(1, -1), k=5)\n",
        "\n",
        "print(\"Raw FAISS distances & indices:\")\n",
        "for d, idx in zip(distances[0], indices[0]):\n",
        "    if idx == -1:\n",
        "        continue\n",
        "    similarity = 1 - d  # assuming cosine distance\n",
        "    print(f\"Index {idx}: similarity = {similarity:.4f}, dist = {d:.4f}\")\n",
        "\n",
        "# Try with very low threshold\n",
        "hits = retrieve(test_query, k=5, min_score=0.15)  # lowered\n",
        "print(f\"\\nRetrieved {len(hits)} hits with min_score=0.15:\")\n",
        "for hit in hits:\n",
        "    print(f\"Score {hit['score']:.3f} | Source {hit['source']} | Preview: {hit['text_preview'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oU_buRp90q",
        "outputId": "26368567-7d73-47a0-f01c-7b35552519be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity between query and first chunk: 0.7109\n"
          ]
        }
      ],
      "source": [
        "# Take first chunk text\n",
        "sample_text = texts[0][:200]  # first 200 chars\n",
        "sample_emb = embedder.encode([sample_text], normalize_embeddings=True, convert_to_numpy=True)[0]\n",
        "\n",
        "query_emb = embedder.encode([\"Bitcoin proof of work\"], normalize_embeddings=True, convert_to_numpy=True)[0]\n",
        "\n",
        "cos_sim = np.dot(sample_emb, query_emb)  # should be ~0 if unrelated\n",
        "print(f\"Cosine similarity between query and first chunk: {cos_sim:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aopFUgSqKwo",
        "outputId": "56b0a0ab-f68a-4f5c-f01c-2e101a545cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: Bitcoin proof of work\n",
            "Retrieved 5 hits:\n",
            "Score 0.217 | Source bitcoin.pdf | Preview: . For our timestamp network, we implement the proof-of-work by incrementing a nonce in the block unt...\n",
            "Score 0.218 | Source bitcoin.pdf | Preview: . Proof-of-Work To implement a distributed timestamp server on a peer-to-peer basis, we will need to...\n",
            "Score 0.224 | Source solana.pdf | Preview: . That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architectur...\n",
            "Score 0.227 | Source bitcoin.pdf | Preview: . The proof-of-work also solves the problem of determining representation in majority decision makin...\n",
            "Score 0.230 | Source bitcoin.pdf | Preview: . To solve this, we proposed a peer-to-peer network using proof-of-work to record a public history o...\n",
            "\n",
            "Query: What is Proof of History in Solana?\n",
            "Retrieved 5 hits:\n",
            "Score 0.300 | Source solana.pdf | Preview: . That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architectur...\n",
            "Score 0.304 | Source solana.pdf | Preview: . Elections for the proposed PoS algorithm are covered in depth in Section 5.6. In terms of CAP theo...\n",
            "Score 0.316 | Source solana.pdf | Preview: . 7 Figure 3: Inserting data into Proof of History 4.3 Veriﬁcation The sequence can be veriﬁed corre...\n",
            "Score 0.320 | Source solana.pdf | Preview: . 7 System Architecture 7.1 Components 7.1.1 Leader, Proof of History generator The Leader is an ele...\n",
            "Score 0.325 | Source solana.pdf | Preview: . In the example in Figure 3, input cfd40df8... was inserted into the Proof of History sequence. The...\n",
            "\n",
            "Query: What is the purpose of the 'liquidity accumulator' in Uniswap v3?\n",
            "Retrieved 5 hits:\n",
            "Score 0.179 | Source uniswap.pdf | Preview: . This functionality relies on the fee accounting system to facilitate efficient payments. The fee p...\n",
            "Score 0.197 | Source uniswap.pdf | Preview: . In Uniswap v3, users were required to utilize the concentrated liquidity AMM introduced in the sam...\n",
            "Score 0.209 | Source uniswap.pdf | Preview: . Uniswap v3 introduced concentrated liquidity, enabling more capital efficient liquidity through po...\n",
            "Score 0.210 | Source uniswap.pdf | Preview: . These design choices came with increased gas costs for end users. In Uniswap v4, we improve on the...\n",
            "Score 0.224 | Source uniswap.pdf | Preview: . Hooks that can affect minting but not burning of liquidity are safer for liquidity providers, sinc...\n"
          ]
        }
      ],
      "source": [
        "# Test retrieval on 3 queries (run this in your notebook)\n",
        "test_queries = [\n",
        "    \"Bitcoin proof of work\",  # your test\n",
        "    \"What is Proof of History in Solana?\",  # from earlier\n",
        "    \"What is the purpose of the 'liquidity accumulator' in Uniswap v3?\"  # random from dataset\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    hits = retrieve(query, k=5, min_score=0.15)  # same as your test\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"Retrieved {len(hits)} hits:\")\n",
        "    for hit in hits:\n",
        "        print(f\"Score {hit['score']:.3f} | Source {hit['source']} | Preview: {hit['text_preview'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPFpesIBC0Bd"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T65mnaqOCy1L",
        "outputId": "0bc5c642-889e-4766-c7bd-c4cf305ae523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet pymupdf langchain-huggingface langchain-community accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG3U6sIQ6d6e",
        "outputId": "22555345-98aa-446b-f44f-1fbecf42f5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 40 evaluation pairs.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"crypto_rag_eval_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(eval_data)} evaluation pairs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZPG7lpPeF_rk",
        "outputId": "fe8538fd-a612-45c7-c947-1a73720f4354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.10.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.24.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate) (1.3.4)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
            "Downloading transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 0.36.2\n",
            "    Uninstalling huggingface_hub-0.36.2:\n",
            "      Successfully uninstalled huggingface_hub-0.36.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.4.1 transformers-5.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a0081fef3d254fefbea9125b95bbfde7",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install --upgrade transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "027c3a53838346c0a4eae1f76eeb39e1",
            "df1c6baea850426fab48a94df9080615",
            "09ce6250199f43d581a1d50b87fe3155",
            "42cf4ff9c2a94fa9ade1fc223f14867a",
            "8f38b5fe78984d16879ff2cc09f25773",
            "3d5eb4e4c2d341868158b805e416f59e",
            "a64892cb26004dd89715149ffe3c7a28",
            "c4d75f49830b49819985f0a142aee0c5",
            "c47cd6846f8f4a7bb553983df5a4b3be",
            "33221cc481fe4d1ba6609c0a0feb9582",
            "40a8da526c4e4c1d80aaeeaa2ac0155b"
          ]
        },
        "id": "Mx8emsAq6d9R",
        "outputId": "b3465efa-f4e4-4120-ad13-42d47beebcfe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "027c3a53838346c0a4eae1f76eeb39e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Phi-3 RAG generation:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 703, in format\n",
            "    record.message = record.getMessage()\n",
            "                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 392, in getMessage\n",
            "    msg = msg % self.args\n",
            "          ~~~~^~~~~~~~~~~\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1022382712.py\", line 39, in <cell line: 0>\n",
            "    outputs = phi_model.generate(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 124, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2668, in generate\n",
            "    result = decoding_method(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2863, in _sample\n",
            "    outputs = self._prefill(input_ids, generation_config, model_kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 3857, in _prefill\n",
            "    return self(**model_inputs, return_dict=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\", line 1243, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\", line 1091, in forward\n",
            "    attention_mask = _prepare_4d_causal_attention_mask(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py\", line 350, in _prepare_4d_causal_attention_mask\n",
            "    attn_mask_converter = AttentionMaskConverter(is_causal=True, sliding_window=sliding_window)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py\", line 74, in __init__\n",
            "    logger.warning_once(DEPRECATION_MESSAGE, FutureWarning)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/logging.py\", line 327, in warning_once\n",
            "    self.warning(*args, **kwargs)\n",
            "Message: 'The attention mask API under `transformers.modeling_attn_mask_utils` (`AttentionMaskConverter`) is deprecated and will be removed in Transformers v5.10. Please use the new API in `transformers.masking_utils`.'\n",
            "Arguments: (<class 'FutureWarning'>,)\n",
            "WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct.f39ac1d28e925b323eae81227eaba4464caced4e.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved partial at 20\n",
            "Saved partial at 40\n",
            "\n",
            "Done! 40 answers saved.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import DynamicCache\n",
        "\n",
        "phi_rag_results = []\n",
        "MAX_NEW_TOKENS = 160\n",
        "BATCH_SIZE = 4  # lower if memory tight\n",
        "\n",
        "for i in tqdm(range(0, len(eval_data), BATCH_SIZE), desc=\"Phi-3 RAG generation\"):\n",
        "    batch = eval_data[i:i + BATCH_SIZE]\n",
        "    prompts = []\n",
        "    batch_retrieved = []\n",
        "\n",
        "    for item in batch:\n",
        "        q = item[\"question\"]\n",
        "        hits = retrieve(q, k=3, min_score=0.22)\n",
        "\n",
        "        context = \"\\n\\n\".join([hit[\"text_preview\"] for hit in hits]) if hits else \"No relevant context found.\"\n",
        "        retrieved_preview = [hit[\"text_preview\"] for hit in hits] if hits else []\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant. Use only the provided context to answer accurately and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {q}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        prompts.append(prompt)\n",
        "        batch_retrieved.append(retrieved_preview)\n",
        "\n",
        "    inputs = phi_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(phi_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = phi_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "            pad_token_id=phi_tokenizer.eos_token_id,\n",
        "            use_cache=False  # ← FIXED HERE\n",
        "        )\n",
        "\n",
        "    answers = phi_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    cleaned = []\n",
        "    for p, a in zip(prompts, answers):\n",
        "        if a.startswith(p):\n",
        "            cleaned.append(a[len(p):].strip())\n",
        "        else:\n",
        "            cleaned.append(a.strip())\n",
        "\n",
        "    for j, item in enumerate(batch):\n",
        "        phi_rag_results.append({\n",
        "            \"question\": item[\"question\"],\n",
        "            \"ground_truth_excerpt\": item.get(\"ground_truth_excerpt\", \"\"),\n",
        "            \"phi_rag_answer\": cleaned[j],\n",
        "            \"retrieved_chunks_preview\": batch_retrieved[j],\n",
        "        })\n",
        "\n",
        "    current = i + len(batch)\n",
        "    if current % 10 == 0 or current >= len(eval_data):\n",
        "        with open(f\"phi_rag_partial_{current}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(phi_rag_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Saved partial at {current}\")\n",
        "\n",
        "with open(\"phi_rag_outputs_final.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(phi_rag_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nDone! {len(phi_rag_results)} answers saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHuxJVXf6d_0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027c3a53838346c0a4eae1f76eeb39e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df1c6baea850426fab48a94df9080615",
              "IPY_MODEL_09ce6250199f43d581a1d50b87fe3155",
              "IPY_MODEL_42cf4ff9c2a94fa9ade1fc223f14867a"
            ],
            "layout": "IPY_MODEL_8f38b5fe78984d16879ff2cc09f25773"
          }
        },
        "09ce6250199f43d581a1d50b87fe3155": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4d75f49830b49819985f0a142aee0c5",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c47cd6846f8f4a7bb553983df5a4b3be",
            "value": 5
          }
        },
        "0f954a96a0064a31906f8863c17b69ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33221cc481fe4d1ba6609c0a0feb9582": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc8f059ff2b43b1aa064d531797b7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d5eb4e4c2d341868158b805e416f59e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a8da526c4e4c1d80aaeeaa2ac0155b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42cf4ff9c2a94fa9ade1fc223f14867a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33221cc481fe4d1ba6609c0a0feb9582",
            "placeholder": "​",
            "style": "IPY_MODEL_40a8da526c4e4c1d80aaeeaa2ac0155b",
            "value": " 5/10 [10:37&lt;11:07, 133.59s/it]"
          }
        },
        "72eb2c14ca594f98878ca3f84337b165": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f954a96a0064a31906f8863c17b69ed",
            "placeholder": "​",
            "style": "IPY_MODEL_c39283c812614936a324f3fca3ea1605",
            "value": "Loading weights: 100%"
          }
        },
        "8f38b5fe78984d16879ff2cc09f25773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f670ff963840308318b8342866b45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a64892cb26004dd89715149ffe3c7a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a890de9023ab4c29b259eff46c1d6118": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1944e664251489d893e52ad684c4c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a890de9023ab4c29b259eff46c1d6118",
            "placeholder": "​",
            "style": "IPY_MODEL_3cc8f059ff2b43b1aa064d531797b7ff",
            "value": " 199/199 [00:00&lt;00:00, 385.59it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "c39283c812614936a324f3fca3ea1605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c47cd6846f8f4a7bb553983df5a4b3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4d75f49830b49819985f0a142aee0c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df1c6baea850426fab48a94df9080615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d5eb4e4c2d341868158b805e416f59e",
            "placeholder": "​",
            "style": "IPY_MODEL_a64892cb26004dd89715149ffe3c7a28",
            "value": "Phi-3 RAG generation:  50%"
          }
        },
        "dfda0b1886274063aadd6c0129d103d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e33fd63d278243aebb7adde55e01665e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72eb2c14ca594f98878ca3f84337b165",
              "IPY_MODEL_ed5599eaa3fb4b02997841e3fd6cb7dd",
              "IPY_MODEL_b1944e664251489d893e52ad684c4c1b"
            ],
            "layout": "IPY_MODEL_fb827edcbf0f417983fdb29feb1277d0"
          }
        },
        "ed5599eaa3fb4b02997841e3fd6cb7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfda0b1886274063aadd6c0129d103d0",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99f670ff963840308318b8342866b45a",
            "value": 199
          }
        },
        "fb827edcbf0f417983fdb29feb1277d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}