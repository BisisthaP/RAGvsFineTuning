{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75bdf5d344a6454aacff4f595a7e777b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fac1ddaf0264dc7b2656ad1e6b0d5e1",
              "IPY_MODEL_8dc5c179189c434baac77a1ff8a45f59",
              "IPY_MODEL_03ff41f7faec434b94f5a973261c0398"
            ],
            "layout": "IPY_MODEL_a7b1634488c342ff905c5be049e7eeec"
          }
        },
        "0fac1ddaf0264dc7b2656ad1e6b0d5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cd1bdf721de4535a61d9f03b8aec3b8",
            "placeholder": "​",
            "style": "IPY_MODEL_e2b5038b48a94296ac6ed0b2054206a5",
            "value": "Embedding batches: 100%"
          }
        },
        "8dc5c179189c434baac77a1ff8a45f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66556401a47d4c55ad2d68f23fc84633",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48c6492cae2243d593b494b7843e9adb",
            "value": 26
          }
        },
        "03ff41f7faec434b94f5a973261c0398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c367cd7e5504c9891fab88c72de5eee",
            "placeholder": "​",
            "style": "IPY_MODEL_8308e633c6d448c59df226ed058b17b3",
            "value": " 26/26 [01:09&lt;00:00,  1.78s/it]"
          }
        },
        "a7b1634488c342ff905c5be049e7eeec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cd1bdf721de4535a61d9f03b8aec3b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b5038b48a94296ac6ed0b2054206a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66556401a47d4c55ad2d68f23fc84633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c6492cae2243d593b494b7843e9adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c367cd7e5504c9891fab88c72de5eee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8308e633c6d448c59df226ed058b17b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed192991ff8c499e92bfcb1dc513cb06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e06cd9528c8403ea5c28b62b96f3095",
              "IPY_MODEL_6b461c2415b84864ad0a0f9a64a5602d",
              "IPY_MODEL_1c4d0a232f1841d29d9b84b4d54a0886"
            ],
            "layout": "IPY_MODEL_8836f5471f9a4a089031a3ffa65582f0"
          }
        },
        "3e06cd9528c8403ea5c28b62b96f3095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1447b34470e244b5a928cf969f95b1ed",
            "placeholder": "​",
            "style": "IPY_MODEL_1d0d953397ea4564bbc87430ee6b2827",
            "value": "Generating (batched): 100%"
          }
        },
        "6b461c2415b84864ad0a0f9a64a5602d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1df4fe9e330d429785366bb2f53556c3",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f42c9c3740746e5bb232bcfc52ef691",
            "value": 10
          }
        },
        "1c4d0a232f1841d29d9b84b4d54a0886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d139da5186c048ad86799b10ef959dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_20ca5384e91946ccb4760c1ab5022af4",
            "value": " 10/10 [04:02&lt;00:00, 20.80s/it]"
          }
        },
        "8836f5471f9a4a089031a3ffa65582f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1447b34470e244b5a928cf969f95b1ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0d953397ea4564bbc87430ee6b2827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1df4fe9e330d429785366bb2f53556c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f42c9c3740746e5bb232bcfc52ef691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d139da5186c048ad86799b10ef959dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ca5384e91946ccb4760c1ab5022af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries needed"
      ],
      "metadata": {
        "id": "AEfHWgQ2rGdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This codefile has been run in the google colab environment with a CPU based (standard) hardware"
      ],
      "metadata": {
        "id": "J0WOnw0S1qwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1KFz85urEHr"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers==4.41.2 sentence-transformers==3.0.1 faiss-cpu==1.8.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf -q"
      ],
      "metadata": {
        "id": "VUZnzzS2rFgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoGDdk2orFjs",
        "outputId": "0668950e-6b8d-47f8-f86a-3e92b67c3140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.2)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.2.12)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.19.1)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.21.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (26.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --quiet"
      ],
      "metadata": {
        "id": "V-qKoPsA3hn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07GgmYQxQiYw",
        "outputId": "9902a752-bee8-4453-bf01-15a0932bc0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: peft 0.18.1\n",
            "Uninstalling peft-0.18.1:\n",
            "  Successfully uninstalled peft-0.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart runtime\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "sZW74DPXrFmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "print(\"Ready – transformers should now import cleanly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8KkmKTGrFos",
        "outputId": "7d3af047-9910-49f9-8e47-b91efb0cfa4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready – transformers should now import cleanly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Silence most warnings (especially from transformers & sentence-transformers)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using device: cpu\")\n",
        "print(\"Environment looks ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvK3O60OrFrD",
        "outputId": "0aea5d93-206c-4dcb-e4a6-92313ddab8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "Using device: cpu\n",
            "Environment looks ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data PreProcessing\n",
        "Includes text extraction and cleaning, chunking"
      ],
      "metadata": {
        "id": "6ULN5bTr0vUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# function for cleaning the text present in the document\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'(?i)page\\s+\\d+(\\s+of\\s+\\d+)?', '', text)\n",
        "    text = re.sub(r'\\[\\d+(?:,\\s*\\d+|-?\\d+)*\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# finidng the document to process first and applying the function defined above\n",
        "def process_crypto_documents(doc_list):\n",
        "    document_library = {}\n",
        "    for doc_name in doc_list:\n",
        "        if not os.path.exists(doc_name):\n",
        "            print(f\"Warning: {doc_name} not found.\")\n",
        "            continue\n",
        "        print(f\"Processing: {doc_name}\")\n",
        "        try:\n",
        "            doc = fitz.open(doc_name)\n",
        "            raw_content = \"\"\n",
        "            for page in doc:\n",
        "                raw_content += page.get_text(\"text\") + \" \"\n",
        "            cleaned = clean_text(raw_content)\n",
        "            document_library[doc_name] = cleaned\n",
        "            doc.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {doc_name}: {e}\")\n",
        "    return document_library\n",
        "\n",
        "# The files must be present in the same directory as this codefile\n",
        "big_5_files = [\n",
        "    \"bitcoin.pdf\",\n",
        "    \"solana.pdf\",\n",
        "    \"uniswap.pdf\",\n",
        "    \"chainlink.pdf\"\n",
        "]\n",
        "\n",
        "processed_docs = process_crypto_documents(big_5_files)\n",
        "\n",
        "print(\"\\nExtraction Summary\")\n",
        "for name, content in processed_docs.items():\n",
        "    print(f\"{name}: {len(content):,} characters\")"
      ],
      "metadata": {
        "id": "vzJOsrJPsjL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190f3b6d-bb91-449b-a67f-223e302b4d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: bitcoin.pdf\n",
            "Processing: solana.pdf\n",
            "Processing: uniswap.pdf\n",
            "Processing: chainlink.pdf\n",
            "\n",
            "Extraction Summary\n",
            "bitcoin.pdf: 21,165 characters\n",
            "solana.pdf: 46,029 characters\n",
            "uniswap.pdf: 17,134 characters\n",
            "chainlink.pdf: 86,266 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxTgj4cSNCwV",
        "outputId": "f402c9a6-ce9c-4a53-f2e1-038aa0f63a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bitcoin.pdf': 'Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.com www.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they\\'ll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone. 1. Introduction Commerce on the Internet has come to rely almost exclusively on financial institutions serving as trusted third parties to process electronic payments. While the system works well enough for most transactions, it still suffers from the inherent weaknesses of the trust based model. Completely non-reversible transactions are not really possible, since financial institutions cannot avoid mediating disputes. The cost of mediation increases transaction costs, limiting the minimum practical transaction size and cutting off the possibility for small casual transactions, and there is a broader cost in the loss of ability to make non-reversible payments for non- reversible services. With the possibility of reversal, the need for trust spreads. Merchants must be wary of their customers, hassling them for more information than they would otherwise need. A certain percentage of fraud is accepted as unavoidable. These costs and payment uncertainties can be avoided in person by using physical currency, but no mechanism exists to make payments over a communications channel without a trusted party. What is needed is an electronic payment system based on cryptographic proof instead of trust, allowing any two willing parties to transact directly with each other without the need for a trusted third party. Transactions that are computationally impractical to reverse would protect sellers from fraud, and routine escrow mechanisms could easily be implemented to protect buyers. In this paper, we propose a solution to the double-spending problem using a peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions. The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes. 1 2. Transactions We define an electronic coin as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin. A payee can verify the signatures to verify the chain of ownership. The problem of course is the payee can\\'t verify that one of the owners did not double-spend the coin. A common solution is to introduce a trusted central authority, or mint, that checks every transaction for double spending. After each transaction, the coin must be returned to the mint to issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent. The problem with this solution is that the fate of the entire money system depends on the company running the mint, with every transaction having to go through them, just like a bank. We need a way for the payee to know that the previous owners did not sign any earlier transactions. For our purposes, the earliest transaction is the one that counts, so we don\\'t care about later attempts to double-spend. The only way to confirm the absence of a transaction is to be aware of all transactions. In the mint based model, the mint was aware of all transactions and decided which arrived first. To accomplish this without a trusted party, transactions must be publicly announced , and we need a system for participants to agree on a single history of the order in which they were received. The payee needs proof that at the time of each transaction, the majority of nodes agreed it was the first received. 3. Timestamp Server The solution we propose begins with a timestamp server. A timestamp server works by taking a hash of a block of items to be timestamped and widely publishing the hash, such as in a newspaper or Usenet post . The timestamp proves that the data must have existed at the time, obviously, in order to get into the hash. Each timestamp includes the previous timestamp in its hash, forming a chain, with each additional timestamp reinforcing the ones before it. 2 Block Item Item ... Hash Block Item Item ... Hash Transaction Owner 1\\'s Public Key Owner 0\\'s Signature Hash Transaction Owner 2\\'s Public Key Owner 1\\'s Signature Hash Verify Transaction Owner 3\\'s Public Key Owner 2\\'s Signature Hash Verify Owner 2\\'s Private Key Owner 1\\'s Private Key Sign Sign Owner 3\\'s Private Key 4. Proof-of-Work To implement a distributed timestamp server on a peer-to-peer basis, we will need to use a proof- of-work system similar to Adam Back\\'s Hashcash , rather than newspaper or Usenet posts. The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the hash begins with a number of zero bits. The average work required is exponential in the number of zero bits required and can be verified by executing a single hash. For our timestamp network, we implement the proof-of-work by incrementing a nonce in the block until a value is found that gives the block\\'s hash the required zero bits. Once the CPU effort has been expended to make it satisfy the proof-of-work, the block cannot be changed without redoing the work. As later blocks are chained after it, the work to change the block would include redoing all the blocks after it. The proof-of-work also solves the problem of determining representation in majority decision making. If the majority were based on one-IP-address-one-vote, it could be subverted by anyone able to allocate many IPs. Proof-of-work is essentially one-CPU-one-vote. The majority decision is represented by the longest chain, which has the greatest proof-of-work effort invested in it. If a majority of CPU power is controlled by honest nodes, the honest chain will grow the fastest and outpace any competing chains. To modify a past block, an attacker would have to redo the proof-of-work of the block and all blocks after it and then catch up with and surpass the work of the honest nodes. We will show later that the probability of a slower attacker catching up diminishes exponentially as subsequent blocks are added. To compensate for increasing hardware speed and varying interest in running nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour. If they\\'re generated too fast, the difficulty increases. 5. Network The steps to run the network are as follows: 1) New transactions are broadcast to all nodes. 2) Each node collects new transactions into a block. 3) Each node works on finding a difficult proof-of-work for its block. 4) When a node finds a proof-of-work, it broadcasts the block to all nodes. 5) Nodes accept the block only if all transactions in it are valid and not already spent. 6) Nodes express their acceptance of the block by working on creating the next block in the chain, using the hash of the accepted block as the previous hash. Nodes always consider the longest chain to be the correct one and will keep working on extending it. If two nodes broadcast different versions of the next block simultaneously, some nodes may receive one or the other first. In that case, they work on the first one they received, but save the other branch in case it becomes longer. The tie will be broken when the next proof- of-work is found and one branch becomes longer; the nodes that were working on the other branch will then switch to the longer one. 3 Block Prev Hash Nonce Tx Tx ... Block Prev Hash Nonce Tx Tx ... New transaction broadcasts do not necessarily need to reach all nodes. As long as they reach many nodes, they will get into a block before long. Block broadcasts are also tolerant of dropped messages. If a node does not receive a block, it will request it when it receives the next block and realizes it missed one. 6. Incentive By convention, the first transaction in a block is a special transaction that starts a new coin owned by the creator of the block. This adds an incentive for nodes to support the network, and provides a way to initially distribute coins into circulation, since there is no central authority to issue them. The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity that is expended. The incentive can also be funded with transaction fees. If the output value of a transaction is less than its input value, the difference is a transaction fee that is added to the incentive value of the block containing the transaction. Once a predetermined number of coins have entered circulation, the incentive can transition entirely to transaction fees and be completely inflation free. The incentive may help encourage nodes to stay honest. If a greedy attacker is able to assemble more CPU power than all the honest nodes, he would have to choose between using it to defraud people by stealing back his payments, or using it to generate new coins. He ought to find it more profitable to play by the rules, such rules that favour him with more new coins than everyone else combined, than to undermine the system and the validity of his own wealth. 7. Reclaiming Disk Space Once the latest transaction in a coin is buried under enough blocks, the spent transactions before it can be discarded to save disk space. To facilitate this without breaking the block\\'s hash, transactions are hashed in a Merkle Tree , with only the root included in the block\\'s hash. Old blocks can then be compacted by stubbing off branches of the tree. The interior hashes do not need to be stored. A block header with no transactions would be about 80 bytes. If we suppose blocks are generated every 10 minutes, 80 bytes * 6 * 24 * 365 = 4.2MB per year. With computer systems typically selling with 2GB of RAM as of 2008, and Moore\\'s Law predicting current growth of 1.2GB per year, storage should not be a problem even if the block headers must be kept in memory. 4 Block Block Block Header (Block Hash) Prev Hash Nonce Hash01 Hash0 Hash1 Hash2 Hash3 Hash23 Root Hash Hash01 Hash2 Tx3 Hash23 Block Header (Block Hash) Root Hash Transactions Hashed in a Merkle Tree After Pruning Tx0-2 from the Block Prev Hash Nonce Hash3 Tx0 Tx1 Tx2 Tx3 8. Simplified Payment Verification It is possible to verify payments without running a full network node. A user only needs to keep a copy of the block headers of the longest proof-of-work chain, which he can get by querying network nodes until he\\'s convinced he has the longest chain, and obtain the Merkle branch linking the transaction to the block it\\'s timestamped in. He can\\'t check the transaction for himself, but by linking it to a place in the chain, he can see that a network node has accepted it, and blocks added after it further confirm the network has accepted it. As such, the verification is reliable as long as honest nodes control the network, but is more vulnerable if the network is overpowered by an attacker. While network nodes can verify transactions for themselves, the simplified method can be fooled by an attacker\\'s fabricated transactions for as long as the attacker can continue to overpower the network. One strategy to protect against this would be to accept alerts from network nodes when they detect an invalid block, prompting the user\\'s software to download the full block and alerted transactions to confirm the inconsistency. Businesses that receive frequent payments will probably still want to run their own nodes for more independent security and quicker verification. 9. Combining and Splitting Value Although it would be possible to handle coins individually, it would be unwieldy to make a separate transaction for every cent in a transfer. To allow value to be split and combined, transactions contain multiple inputs and outputs. Normally there will be either a single input from a larger previous transaction or multiple inputs combining smaller amounts, and at most two outputs: one for the payment, and one returning the change, if any, back to the sender. It should be noted that fan-out, where a transaction depends on several transactions, and those transactions depend on many more, is not a problem here. There is never the need to extract a complete standalone copy of a transaction\\'s history. 5 Transaction In ... In Out ... Hash01 Hash2 Hash3 Hash23 Block Header Merkle Root Prev Hash Nonce Block Header Merkle Root Prev Hash Nonce Block Header Merkle Root Prev Hash Nonce Merkle Branch for Tx3 Longest Proof-of-Work Chain Tx3 10. Privacy The traditional banking model achieves a level of privacy by limiting access to information to the parties involved and the trusted third party. The necessity to announce all transactions publicly precludes this method, but privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone. This is similar to the level of information released by stock exchanges, where the time and size of individual trades, the \"tape\", is made public, but without telling who the parties were. As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner. Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner. 11. Calculations We consider the scenario of an attacker trying to generate an alternate chain faster than the honest chain. Even if this is accomplished, it does not throw the system open to arbitrary changes, such as creating value out of thin air or taking money that never belonged to the attacker. Nodes are not going to accept an invalid transaction as payment, and honest nodes will never accept a block containing them. An attacker can only try to change one of his own transactions to take back money he recently spent. The race between the honest chain and an attacker chain can be characterized as a Binomial Random Walk. The success event is the honest chain being extended by one block, increasing its lead by +1, and the failure event is the attacker\\'s chain being extended by one block, reducing the gap by -1. The probability of an attacker catching up from a given deficit is analogous to a Gambler\\'s Ruin problem. Suppose a gambler with unlimited credit starts at a deficit and plays potentially an infinite number of trials to try to reach breakeven. We can calculate the probability he ever reaches breakeven, or that an attacker ever catches up with the honest chain, as follows : p = probability an honest node finds the next block q = probability the attacker finds the next block qz = probability the attacker will ever catch up from z blocks behind q z={ 1 if p≤q \\ue09eq/ p\\ue09f z if p\\ue085q} 6 Identities Transactions Trusted Third Party Counterparty Public Identities Transactions Public New Privacy Model Traditional Privacy Model Given our assumption that p > q, the probability drops exponentially as the number of blocks the attacker has to catch up with increases. With the odds against him, if he doesn\\'t make a lucky lunge forward early on, his chances become vanishingly small as he falls further behind. We now consider how long the recipient of a new transaction needs to wait before being sufficiently certain the sender can\\'t change the transaction. We assume the sender is an attacker who wants to make the recipient believe he paid him for a while, then switch it to pay back to himself after some time has passed. The receiver will be alerted when that happens, but the sender hopes it will be too late. The receiver generates a new key pair and gives the public key to the sender shortly before signing. This prevents the sender from preparing a chain of blocks ahead of time by working on it continuously until he is lucky enough to get far enough ahead, then executing the transaction at that moment. Once the transaction is sent, the dishonest sender starts working in secret on a parallel chain containing an alternate version of his transaction. The recipient waits until the transaction has been added to a block and z blocks have been linked after it. He doesn\\'t know the exact amount of progress the attacker has made, but assuming the honest blocks took the average expected time per block, the attacker\\'s potential progress will be a Poisson distribution with expected value: \\ue0c1=z q p To get the probability the attacker could still catch up now, we multiply the Poisson density for each amount of progress he could have made by the probability he could catch up from that point: ∑ k=0 ∞\\ue0c1 ke −\\ue0c1 k! ⋅{ \\ue09eq/ p\\ue09f \\ue09ez−k\\ue09f if k≤z 1 if k\\ue085z} Rearranging to avoid summing the infinite tail of the distribution... 1−∑ k=0 z \\ue0c1 k e −\\ue0c1 k! \\ue09e1−\\ue09eq/ p\\ue09f \\ue09ez−k\\ue09f\\ue09f Converting to C code... #include <math.h> double AttackerSuccessProbability(double q, int z) { double p = 1.0 - q; double lambda = z * (q / p); double sum = 1.0; int i, k; for (k = 0; k <= z; k++) { double poisson = exp(-lambda); for (i = 1; i <= k; i++) poisson *= lambda / i; sum -= poisson * (1 - pow(q / p, z - k)); } return sum; } 7 Running some results, we can see the probability drop off exponentially with z. q=0.1 z=0 P=1.0000000 z=1 P=0.2045873 z=2 P=0.0509779 z=3 P=0.0131722 z=4 P=0.0034552 z=5 P=0.0009137 z=6 P=0.0002428 z=7 P=0.0000647 z=8 P=0.0000173 z=9 P=0.0000046 z=10 P=0.0000012 q=0.3 z=0 P=1.0000000 z=5 P=0.1773523 z=10 P=0.0416605 z=15 P=0.0101008 z=20 P=0.0024804 z=25 P=0.0006132 z=30 P=0.0001522 z=35 P=0.0000379 z=40 P=0.0000095 z=45 P=0.0000024 z=50 P=0.0000006 Solving for P less than 0.1%... P < 0.001 q=0.10 z=5 q=0.15 z=8 q=0.20 z=11 q=0.25 z=15 q=0.30 z=24 q=0.35 z=41 q=0.40 z=89 q=0.45 z=340 12. Conclusion We have proposed a system for electronic transactions without relying on trust. We started with the usual framework of coins made from digital signatures, which provides strong control of ownership, but is incomplete without a way to prevent double-spending. To solve this, we proposed a peer-to-peer network using proof-of-work to record a public history of transactions that quickly becomes computationally impractical for an attacker to change if honest nodes control a majority of CPU power. The network is robust in its unstructured simplicity. Nodes work all at once with little coordination. They do not need to be identified, since messages are not routed to any particular place and only need to be delivered on a best effort basis. Nodes can leave and rejoin the network at will, accepting the proof-of-work chain as proof of what happened while they were gone. They vote with their CPU power, expressing their acceptance of valid blocks by working on extending them and rejecting invalid blocks by refusing to work on them. Any needed rules and incentives can be enforced with this consensus mechanism. 8 References W. Dai, \"b-money,\" http://www.weidai.com/bmoney.txt, 1998. H. Massias, X.S. Avila, and J.-J. Quisquater, \"Design of a secure timestamping service with minimal trust requirements,\" In 20th Symposium on Information Theory in the Benelux, May 1999. S. Haber, W.S. Stornetta, \"How to time-stamp a digital document,\" In Journal of Cryptology, vol 3, no 2, pages 99-111, 1991. D. Bayer, S. Haber, W.S. Stornetta, \"Improving the efficiency and reliability of digital time-stamping,\" In Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993. S. Haber, W.S. Stornetta, \"Secure names for bit-strings,\" In Proceedings of the 4th ACM Conference on Computer and Communications Security, pages 28-35, April 1997. A. Back, \"Hashcash - a denial of service counter-measure,\" http://www.hashcash.org/papers/hashcash.pdf, 2002. R.C. Merkle, \"Protocols for public key cryptosystems,\" In Proc. 1980 Symposium on Security and Privacy, IEEE Computer Society, pages 122-133, April 1980. W. Feller, \"An introduction to probability theory and its applications,\" 1957. 9', 'solana.pdf': 'Solana: A new architecture for a high performance blockchain v0.8.13 Anatoly Yakovenko anatoly@solana.io Legal Disclaimer Nothing in this White Paper is an oﬀer to sell, or the solicitation of an oﬀer to buy, any tokens. Solana is publishing this White Paper solely to receive feedback and comments from the public. If and when Solana oﬀers for sale any tokens (or a Simple Agreement for Future Tokens), it will do so through deﬁnitive oﬀering documents, including a disclosure document and risk factors. Those deﬁnitive documents also are expected to include an updated version of this White Paper, which may diﬀer signiﬁcantly from the current version. If and when Solana makes such an oﬀering in the United States, the oﬀering likely will be available solely to accredited investors. Nothing in this White Paper should be treated or read as a guarantee or promise of how Solanas business or the tokens will develop or of the utility or value of the tokens. This White Paper outlines current plans, which could change at its discretion, and the success of which will depend on many factors outside Solanas control, including market-based factors and factors within the data and cryptocurrency industries, among others. Any statements about future events are based solely on Solanas analysis of the issues described in this White Paper. That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architecture based on Proof of History (PoH) - a proof for verifying order and passage of time between events. PoH is used to encode trustless passage of time into a ledger - an append only data structure. When used alongside a consensus algorithm such as Proof of Work (PoW) or Proof of Stake (PoS), PoH can reduce messaging overhead in a Byzantine Fault Tol- erant replicated state machine, resulting inn sub-second ﬁnality times. This paper also proposes two algorithms that leverage the time keep- ing properties of the PoH ledger - a PoS algorithm that can recover from partitions of any size and an eﬃcient streaming Proof of Replica- tion (PoRep). The combination of PoRep and PoH provides a defense against forgery of the ledger with respect to time (ordering) and stor- age. The protocol is analyzed on a 1 gbps network, and this paper shows that throughput up to 710k transactions per second is possible with todays hardware. 1 1 Introduction Blockchain is an implementation of a fault tolerant replicated state machine. Current publicly available blockchains do not rely on time, or make a weak assumption about the participants abilities to keep time . Each node in the network usually relies on their own local clock without knowledge of any other participants clocks in the network. The lack of a trusted source of time means that when a message timestamp is used to accept or reject a message, there is no guarantee that every other participant in the network will make the exact same choice. The PoH presented here is designed to create a ledger with veriﬁable passage of time, i.e. duration between events and message ordering. It is anticipated that every node in the network will be able to rely on the recorded passage of time in the ledger without trust. 2 Outline The remainder of this article is organized as follows. Overall system design is described in Section 3. In depth description of Proof of History is described in Section 4. In depth description of the proposed Proof of Stake consensus algorithm is described in Section 5. In depth description of the proposed fast Proof of Replication is described in Section 6. System Architecture and performance limits are analyzed in Section 7. A high performance GPU friendly smart contracts engine is described in Section 7.5 3 Network Design As shown in Figure 1, at any given time a system node is designated as Leader to generate a Proof of History sequence, providing the network global read consistency and a veriﬁable passage of time. The Leader sequences user messages and orders them such that they can be eﬃciently processed by other nodes in the system, maximizing throughput. It executes the transactions on the current state that is stored in RAM and publishes the transactions and a signature of the ﬁnal state to the replications nodes called Veriﬁers. Veriﬁers execute the same transactions on their copies of the state, and pub- lish their computed signatures of the state as conﬁrmations. The published conﬁrmations serve as votes for the consensus algorithm. 2 Figure 1: Transaction ﬂow throughout the network. In a non-partitioned state, at any given time, there is one Leader in the network. Each Veriﬁer node has the same hardware capabilities as a Leader and can be elected as a Leader, this is done through PoS based elections. Elections for the proposed PoS algorithm are covered in depth in Section 5.6. In terms of CAP theorem, Consistency is almost always picked over Avail- ability in an event of a Partition. In case of a large partition, this paper proposes a mechanism to recover control of the network from a partition of any size. This is covered in depth in Section 5.12. 4 Proof of History Proof of History is a sequence of computation that can provide a way to cryptographically verify passage of time between two events. It uses a cryp- tographically secure function written so that output cannot be predicted from the input, and must be completely executed to generate the output. The function is run in a sequence on a single core, its previous output as the 3 current input, periodically recording the current output, and how many times its been called. The output can then be re-computed and veriﬁed by external computers in parallel by checking each sequence segment on a separate core. Data can be timestamped into this sequence by appending the data (or a hash of some data) into the state of the function. The recording of the state, index and data as it was appended into the sequences provides a timestamp that can guarantee that the data was created sometime before the next hash was generated in the sequence. This design also supports horizontal scaling as multiple generators can synchronize amongst each other by mixing their state into each others sequences. Horizontal scaling is discussed in depth in Section 4.4 4.1 Description The system is designed to work as follows. With a cryptographic hash func- tion, whose output cannot be predicted without running the function (e.g. sha256, ripemd, etc.), run the function from some random starting value and take its output and pass it as the input into the same function again. Record the number of times the function has been called and the output at each call. The starting random value chosen could be any string, like the headline of the New York times for the day. For example: PoH Sequence Index Operation Output Hash 1 sha256(”any random starting value”) hash1 2 sha256(hash1) hash2 3 sha256(hash2) hash3 Where hashN represents the actual hash output. It is only necessary to publish a subset of the hashes and indices at an interval. For example: 4 PoH Sequence Index Operation Output Hash 1 sha256(”any random starting value”) hash1 200 sha256(hash199) hash200 300 sha256(hash299) hash300 As long as the hash function chosen is collision resistant, this set of hashes can only be computed in sequence by a single computer thread. This follows from the fact that there is no way to predict what the hash value at index 300 is going to be without actually running the algorithm from the starting value 300 times. Thus we can thus infer from the data structure that real time has passed between index 0 and index 300. In the example in Figure 2, hash 62f51643c1 was produced on count 510144806912 and hash c43d862d88 was produced on count 510146904064. Following the previously discussed properties of the PoH algorithm, we can trust that real time passed between count 510144806912 and count 510146904064. Figure 2: Proof of History sequence 5 4.2 Timestamp for Events This sequence of hashes can also be used to record that some piece of data was created before a particular hash index was generated. Using a ‘combine‘ function to combine the piece of data with the current hash at the current index. The data can simply be a cryptographically unique hash of arbitrary event data. The combine function can be a simple append of data, or any operation that is collision resistant. The next generated hash represents a timestamp of the data, because it could have only been generated after that speciﬁc piece of data was inserted. For example: PoH Sequence Index Operation Output Hash 1 sha256(”any random starting value”) hash1 200 sha256(hash199) hash200 300 sha256(hash299) hash300 Some external event occurs, like a photograph was taken, or any arbitrary digital data was created: PoH Sequence With Data Index Operation Output Hash 1 sha256(”any random starting value”) hash1 200 sha256(hash199) hash200 300 sha256(hash299) hash300 336 sha256(append(hash335, photograph sha256)) hash336 Hash336 is computed from the appended binary data of hash335 and the sha256 of the photograph. The index and the sha256 of the photograph are recorded as part of the sequence output. So anyone verifying this sequence can then recreate this change to the sequence. The verifying can still be done in parallel and its discussed in Section 4.3 Because the initial process is still sequential, we can then tell that things entered into the sequence must have occurred sometime before the future hashed value was computed. 6 POH Sequence Index Operation Output Hash 1 sha256(”any random starting value”) hash1 200 sha256(hash199) hash200 300 sha256(hash299) hash300 336 sha256(append(hash335, photograph1 sha256)) hash336 400 sha256(hash399) hash400 500 sha256(hash499) hash500 600 sha256(append(hash599, photograph2 sha256)) hash600 700 sha256(hash699) hash700 Table 1: PoH Sequence With 2 Events In the sequence represented by Table 1, photograph2 was created before hash600, and photograph1 was created before hash336. Inserting the data into the sequence of hashes results in a change to all subsequent values in the sequence. As long as the hash function used is collision resistant, and the data was appended, it should be computationally impossible to pre-compute any future sequences based on prior knowledge of what data will be inte- grated into the sequence. The data that is mixed into the sequence can be the raw data itself, or just a hash of the data with accompanying metadata. In the example in Figure 3, input cfd40df8... was inserted into the Proof of History sequence. The count at which it was inserted is 510145855488 and the state at which it was inserted it is 3d039eef3. All the future gen- erated hashes are modiﬁed by this change to the sequence, this change is indicated by the color change in the ﬁgure. Every node observing this sequence can determine the order at which all events have been inserted and estimate the real time between the insertions. 7 Figure 3: Inserting data into Proof of History 4.3 Veriﬁcation The sequence can be veriﬁed correct by a multicore computer in signiﬁcantly less time than it took to generate it. For example: Core 1 Index Data Output Hash 200 sha256(hash199) hash200 300 sha256(hash299) hash300 Core 2 Index Data Output Hash 300 sha256(hash299) hash300 400 sha256(hash399) hash400 Given some number of cores, like a modern GPU with 4000 cores, the veriﬁer can split up the sequence of hashes and their indexes into 4000 slices, and in parallel make sure that each slice is correct from the starting hash to the last hash in the slice. If the expected time to produce the sequence is going to be: 8 Figure 4: Veriﬁcation using multiple cores Total number of hashes Hashes per second for 1 core The expected time to verify that the sequence is correct is going to be: Total number of hashes (Hashes per second per core * Number of cores available to verify) In the example in Figure 4, each core is able to verify each slice of the sequence in parallel. Since all input strings are recorded into the output, with the counter and state that they are appended to, the veriﬁers can replicate each slice in parallel. The red colored hashes indicate that the sequence was modiﬁed by a data insertion. 4.4 Horizontal Scaling Its possible to synchronize multiple Proof of History generators by mixing the sequence state from each generator to each other generator, and thus achieve horizontal scaling of the Proof of History generator. This scaling is done without sharding. The output of both generators is necessary to reconstruct the full order of events in the system. 9 PoH Generator A Index Hash Data 1 hash1a 2 hash2a hash1b 3 hash3a 4 hash4a PoH Generator B Index Hash Data 1 hash1b 2 hash2b hash1a 3 hash3b 4 hash4b Given generators A and B, A receives a data packet from B (hash1b), which contains the last state from Generator B, and the last state generator B observed from Generator A. The next state hash in Generator A then de- pends on the state from Generator B, so we can derive that hash1b happened sometime before hash3a. This property can be transitive, so if three gener- ators are synchronized through a single common generator A ↔B ↔C, we can trace the dependency between A and C even though they were not synchronized directly. By periodically synchronizing the generators, each generator can then handle a portion of external traﬃc, thus the overall system can handle a larger amount of events to track at the cost of true time accuracy due to network latencies between the generators. A global order can still be achieved by picking some deterministic function to order any events that are within the synchronization window, such as by the value of the hash itself. In Figure 5, the two generators insert each others output state and record the operation. The color change indicates that data from the peer had mod- iﬁed the sequence. The generated hashes that are mixed into each stream are highlighted in bold. The synchronization is transitive. A ↔B ↔C There is a provable order of events between A and C through B. Scaling in this way comes at the cost of availability. 10 × 1 gbps connec- tions with availability of 0.999 would have 0.99910 = 0.99 availability. 4.5 Consistency Users are expected to be able to enforce consistency of the generated se- quence and make it resistant to attacks by inserting the last observed output of the sequence they consider valid into their input. 10 Figure 5: Two generators synchronizing PoH Sequence A Index Data Output Hash 10 hash10a 20 Event1 hash20a 30 Event2 hash30a 40 Event3 hash40a PoH Hidden Sequence B Index Data Output Hash 10 hash10b 20 Event3 hash20b 30 Event2 hash30b 40 Event1 hash40b A malicious PoH generator could produce a second hidden sequence with the events in reverse order, if it has access to all the events at once, or is able to generate a faster hidden sequence. To prevent this attack, each client-generated Event should contain within itself the latest hash that the client observed from what it considers to be a valid sequence. So when a client creates the ”Event1” data, they should append the last hash they have observed. 11 PoH Sequence A Index Data Output Hash 10 hash10a 20 Event1 = append(event1 data, hash10a) hash20a 30 Event2 = append(event2 data, hash20a) hash30a 40 Event3 = append(event3 data, hash30a) hash40a When the sequence is published, Event3 would be referencing hash30a, and if its not in the sequence prior to this Event, the consumers of the sequence know that its an invalid sequence. The partial reordering attack would then be limited to the number of hashes produced while the client has observed an event and when the event was entered. Clients should then be able to write software that does not assume the order is correct for the short period of hashes between the last observed and inserted hash. To prevent a malicious PoH generator from rewriting the client Event hashes, the clients can submit a signature of the event data and the last observed hash instead of just the data. PoH Sequence A Index Data Output Hash 10 hash10a 20 Event1 = sign(append(event1 data, hash10a), Client Private Key) hash20a 30 Event2 = sign(append(event2 data, hash20a), Client Private Key) hash30a 40 Event3 = sign(append(event3 data, hash30a), Client Private Key) hash40a Veriﬁcation of this data requires a signature veriﬁcation, and a lookup of the hash in the sequence of hashes prior to this one. Verify: (Signature, PublicKey, hash30a, event3 data) = Event3 Verify(Signature, PublicKey, Event3) Lookup(hash30a, PoHSequence) In Figure 6, the user-supplied input is dependent on hash 0xdeadbeef... existing in the generated sequence sometime before its inserted. The blue 12 Figure 6: Input with a back reference. top left arrow indicates that the client is referencing a previously produced hash. The clients message is only valid in a sequence that contains the hash 0xdeadbeef.... The red color in the sequence indicates that the sequence has been modiﬁed by the clients data. 4.6 Overhead 4000 hashes per second would generate an additional 160 kilobytes of data, and would require access to a GPU with 4000 cores and roughly 0.25-0.75 milliseconds of time to verify. 4.7 Attacks 4.7.1 Reversal Generating a reverse order would require an attacker to start the malicious sequence after the second event. This delay should allow any non malicious peer to peer nodes to communicate about the original order. 13 4.7.2 Speed Having multiple generators may make deployment more resistant to attacks. One generator could be high bandwidth, and receive many events to mix into its sequence, another generator could be high speed low bandwidth that periodically mixes with the high bandwidth generator. The high speed sequence would create a secondary sequence of data that an attacker would have to reverse. 4.7.3 Long Range Attacks Long range attacks involve acquiring old discarded client Private Keys, and generating a falsiﬁed ledger . Proof of History provides some protection against long range attacks. A malicious user that gains access to old private keys would have to recreate a historical record that takes as much time as the original one they are trying to forge. This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up in history length. Additionally, a single source of time allows for construction of a simpler Proof of Replication (more on that in Section 6). Since the network is de- signed so that all participants in the network will rely on a single historical record of events. PoRep and PoH together should provide a defense of both space and time against a forged ledger. 5 Proof of Stake Consensus 5.1 Description This speciﬁc instance of Proof of Stake is designed for quick conﬁrmation of the current sequence produced by the Proof of History generator, for voting and selecting the next Proof of History generator, and for punishing any mis- behaving validators. This algorithm depends on messages eventually arriving to all participating nodes within a certain timeout. 14 5.2 Terminology bonds Bonds are equivalent to a capital expense in Proof of Work. A miner buys hardware and electricity, and commits it to a single branch in a Proof of Work blockchain. A bond is coin that the validator commits as collateral while they are validating transactions. slashing The proposed solution to the nothing at stake problem in Proof of Stake systems . When a proof of voting for a diﬀerent branch is published, that branch can destroy the validators bond. This is an economic incentive designed to discourage validators from conﬁrming multiple branches. super majority A super majority is 2 3rds of the validators weighted by their bonds. A super majority vote indicates that the network has reached consensus, and at least 1 3rd of the network would have had to vote maliciously for this branch to be invalid. This would put the economic cost of an attack at 1 3rd of the market cap of the coin. 5.3 Bonding A bonding transaction takes a amount of coin and moves it to a bonding account under the users identity. Coins in the bonding account cannot be spent and have to remain in the account until the user removes them. The user can only remove stale coins that have timed out. Bonds are valid after super majority of the current stakeholders have conﬁrmed the sequence. 5.4 Voting It is anticipated that the Proof of History generator will be able to publish a signature of the state at a predeﬁned period. Each bonded identity must conﬁrm that signature by publishing their own signed signature of the state. The vote is a simple yes vote, without a no. If super majority of the bonded identities have voted within a timeout, then this branch would be accepted as valid. 15 5.5 Unbonding Missing N number of votes marks the coins as stale and no longer eligible for voting. The user can issue an unbonding transaction to remove them. N is a dynamic value based on the ratio of stale to active votes. N increases as the number of stale votes increases. In an event of a large network partition, this allows the larger branch to recover faster then the smaller branch. 5.6 Elections Election for a new PoH generator occur when the PoH generator failure is detected. The validator with the largest voting power, or highest public key address if there is a tie is picked as the new PoH generator. A super majority of conﬁrmations are required on the new sequence. If the new leader fails before a super majority conﬁrmations are available, the next highest validator is selected, and a new set of conﬁrmations is required. To switch votes, a validator needs to vote at a higher PoH sequence counter, and the new vote needs to contain the votes it wants to switch. Otherwise the second vote will be slashable. Vote switching is expected to be designed so that it can only occur at a height that does not have a super majority. Once a PoH generator is established, a Secondary can be elected to take over the transactional processing duties. If a Secondary exists, it will be considered as the next leader during a Primary failure. The platform is designed so that the Secondary becomes Primary and lower rank generators are promoted if an exception is detected or on a pre- deﬁned schedule. 5.7 Election Triggers 5.7.1 Forked Proof of History generator PoH generators are designed with an identity that signs the generated se- quence. A fork can only occur in case the PoH generators identity has been compromised. A fork is detected because two diﬀerent historical records have been published on the same PoH identity. 16 5.7.2 Runtime Exceptions A hardware failure or a bug, or a intentional error in the PoH generator could cause it to generate an invalid state and publish a signature of the state that does not match the local validators result. Validators will publish the correct signature via gossip and this event would trigger a new round of elections. Any validators who accept an invalid state will have their bonds slashed. 5.7.3 Network Timeouts A network timeout would trigger an election. 5.8 Slashing Slashing occurs when a validator votes two separate sequences. A proof of malicious vote will remove the bonded coins from circulation and add them to the mining pool. A vote that includes a previous vote on a contending sequence is not eligible as proof of malicious voting. Instead of slashing the bonds, this vote removes remove the currently cast vote on the contending sequence. Slashing also occurs if a vote is cast for an invalid hash generated by the PoH generator. The generator is expected to randomly generate an invalid state, which would trigger a fallback to Secondary. 5.9 Secondary Elections Secondary and lower ranked Proof of History generators can be proposed and approved. A proposal is cast on the primary generators sequence. The proposal contains a timeout, if the motion is approved by a super majority of the vote before the timeout, the Secondary is considered elected, and will take over duties as scheduled. Primary can do a soft handover to Secondary by inserting a message into the generated sequence indicating that a handover will occur, or inserting an invalid state and forcing the network to fallback to Secondary. If a Secondary is elected, and the primary fails, the Secondary will be considered as the ﬁrst fallback during an election. 17 5.10 Availability CAP systems that deal with partitions have to pick Consistency or Avail- ability. Our approach eventually picks Availability, but because we have an objective measure of time, Consistency is picked with reasonable human timeouts. Proof of Stake veriﬁers lock up some amount of coin in a stake, which allows them to vote for a particular set of transactions. Locking up coin is a transaction that is entered into a PoH stream, just like any other transaction. To vote, a PoS veriﬁer has to sign the hash of the state, as it was computed after processing all the transactions to a speciﬁc position in the PoH ledger. This vote is also entered as a transaction into the PoH stream. Looking at the PoH ledger, we can then infer how much time passed between each vote, and if a partition occurs, for how long each veriﬁer has been unavailable. To deal with partitions with reasonable human timeframes, we propose a dynamic approach to unstake unavailable veriﬁers. When the number of veriﬁers is high and above 2 3, the unstaking process can be fast. The number of hashes that must be generated into the ledger is low before the unavailable veriﬁers stake is fully unstaked and they are no longer counted for consensus. When the number of veriﬁers is below 2 3rds but above 1 2, the unstaking timer is slower, requiring a larger number of hashes to be generated before the missing veriﬁers are unstaked. In a large partition, like a partition that is missing 1 2 or more of the veriﬁers, the unstaking process is very very slow. Transactions can still be entered into the stream, and veriﬁers can still vote, but full 2 3rds consensus will not be achieved until a very large amount of hashes have been generated and the unavailable veriﬁers have been unstaked. The diﬀerence in time for a network to regain liveness allows us as customers of the network human timeframes to pick a partition that we want to continue using. 5.11 Recovery In the system we propose, the ledger can be fully recovered from any failure. That means, anyone in the world can pick any random spot in the ledger and create a valid fork by appending newly generated hashes and transactions. If all the veriﬁers are missing from this fork, it would take a very very long time for any additional bonds to become valid and for this branch to achieve 2 3rds super majority consensus. So full recovery with zero available validators would require a very large amount of hashes to be appended to the ledger, 18 and only after all the unavailable validators have been unstaked will any new bonds be able to validate the ledger. 5.12 Finality PoH allows veriﬁers of the network to observe what happened in the past with some degree of certainty of the time of those events. As the PoH generator is producing a stream of messages, all the veriﬁers are required to submit their signatures of the state within 500ms. This number can be reduced further depending on network conditions. Since each veriﬁcation is entered into the stream, everyone in the network can validate that every veriﬁer submitted their votes within the required timeout without actually observing the voting directly. 5.13 Attacks 5.13.1 Tragedy of Commons The PoS veriﬁers simply conﬁrm the state hash generated by the PoH gen- erator. There is an economic incentive for them to do no work and simply approve every generated state hash. To avoid this condition, the PoH gener- ator should inject an invalid hash at a random interval. Any voters for this hash should be slashed. When the hash is generated, the network should immediately promote the Secondary elected PoH generator. Each veriﬁer is required to respond within a small timeout - 500ms for example. The timeout should be set low enough that a malicious veriﬁer has a low probability of observing another veriﬁers vote and getting their votes into the stream fast enough. 5.13.2 Collusion with the PoH generator A veriﬁer that is colluding with the PoH generator would know in advance when the invalid hash is going to be produced and not vote for it. This scenario is really no diﬀerent than the PoH identity having a larger veriﬁer stake. The PoH generator still has to do all the work to produce the state hash. 19 5.13.3 Censorship Censorship or denial of service could occur when a 1 3rd of the bond holders refuse to validate any sequences with new bonds. The protocol can defend against this form of attack by dynamically adjusting how fast bonds become stale. In the event of a denial of service, the larger partition will be designed to fork and censor the Byzantine bond holders. The larger network will re- cover as the Byzantine bonds become stale with time. The smaller Byzantine partition would not be able to move forward for a longer period of time. The algorithm would work as follows. A majority of the network would elect a new Leader. The Leader would then censor the Byzantine bond holders from participating. Proof of History generator would have to continue generating a sequence, to prove the passage of time, until enough Byzantine bonds have become stale so the bigger network has a super majority. The rate at which bonds become stale would be dynamically based on what percentage of bonds are active. So the Byzantine minority fork of the network would have to wait much longer than the majority fork to recover a super majority. Once a super majority has been established, slashing could be used to permanently punish the Byzantine bond holders. 5.13.4 Long Range Attacks PoH provides a natural defense against long range attacks. Recovering the ledger from any point in the past would require the attacker to overtake the valid ledger in time by outpacing the speed of the PoH generator. The consensus protocol provides a second layer of defense, as any attack would have to take longer then the time it takes to unstake all the valid validators. It also creates an availability gap in the history of the ledger. When comparing two ledgers of the same height, the one with the smallest maximum partition can be objectively considered as valid. 5.13.5 ASIC Attacks Two opportunities for an ASIC attacks exist in this protocol - during parti- tion, and cheating timeouts in Finality. For ASIC attacks during Partitions, the Rate at which bonds are unstaked is non-linear, and for networks with large partitions the rate is orders of magnitude slower then expected gains from an ASIC attack. 20 For ASIC attacks during Finality, the vulnerability allows for byzantine validators who have a bonded stake wait for conﬁrmations from other nodes and inject their votes with a collaborating PoH generator. The PoH gener- ator can then use its faster ASIC to generate 500ms worth of hashes in less time, and allow for network communication between PoH generator and the collaborating nodes. But, if the PoH generator is also byzantine, there is no reason why the byzantine generator wouldnt have communicated the exact counter when they expect to insert the failure. This scenario is no diﬀerent than a PoH generator and all the collaborators sharing the same identity and having a single combined stake and only using 1 set of hardware. 6 Streaming Proof of Replication 6.1 Description Filecoin proposed a version of Proof of Replication . The goal of this version is to have fast and streaming veriﬁcations of Proof of Replication, which are enabled by keeping track of time in Proof of History generated sequence. Replication is not used as a consensus algorithm, but is a useful tool to account for the cost of storing the blockchain history or state at a high availability. 6.2 Algorithm As shown in Figure 7 CBC encryption encrypts each block of data in se- quence, using the previously encrypted block to XOR the input data. Each replication identity generates a key by signing a hash that has been generated Proof of History sequence. This ties the key to a replicators iden- tity, and to a speciﬁc Proof of History sequence. Only speciﬁc hashes can be selected. (See Section 6.5 on Hash Selection) The data set is fully encrypted block by block. Then to generate a proof, the key is used to seed a pseudorandom number generator that selects a random 32 bytes from each block. A merkle hash is computed with the selected PoH hash prepended to the each slice. The root is published, along with the key, and the selected hash that was generated. The replication node is required to publish another proof 21 Figure 7: Sequential CBC encryption Figure 8: Fast Proof of Replication 22 in N hashes as they are generated by Proof of History generator, where N is approximately 1 2 the time it takes to encrypt the data. The Proof of History generator will publish speciﬁc hashes for Proof of Replication at a predeﬁned periods. The replicator node must select the next published hash for generating the proof. Again, the hash is signed, and random slices are selected from the blocks to create the merkle root. After a period of N proofs, the data is re-encrypted with a new CBC key. 6.3 Veriﬁcation With N cores, each core can stream encryption for each identity. Total space required is 2blocks ∗Ncores, since the previous encrypted block is necessary to generate the next one. Each core can then be used to generate all the proofs that derived from the current encrypted block. Total time to verify proofs is expected to be equal to the time it takes to encrypt. The proofs themselves consume few random bytes from the block, so the amount of data to hash is signiﬁcantly lower then the encrypted block size. The number of replication identities that can be veriﬁed at the same time is equal to the number of available cores. Modern GPUs have 3500+ cores available to them, albeit at 1 2- 1 3 the clock speed of a CPU. 6.4 Key Rotation Without key rotation the same encrypted replication can generate cheap proofs for multiple Proof of History sequences. Keys are rotated periodically and each replication is re-encrypted with a new key that is tied to a unique Proof of History sequence. Rotation needs to be slow enough that its practical to verify replication proofs on GPU hardware, which is slower per core than CPUs. 6.5 Hash Selection Proof of History generator publishes a hash to be used by the entire network for encrypting Proofs of Replication, and for using as the pseudorandom number generator for byte selection in fast proofs. Hash is published at a periodic counter that is roughly equal to 1 2 the time it takes to encrypt the data set. Each replication identity must use the same 23 hash, and use the signed result of the hash as the seed for byte selection, or the encryption key. The period that each replicator must provide a proof must be smaller than the encryption time. Otherwise the replicator can stream the encryption and delete it for each proof. A malicious generator could inject data into the sequence prior to this hash to generate a speciﬁc hash. This attack is discussed more in 5.13.2. 6.6 Proof Validation The Proof of History node is not expected to validate the submitted Proof of Replication proofs. It is expected to keep track of number of pending and veriﬁed proofs submitted by the replicators identity. A proof is expected to be veriﬁed when the replicator is able to sign the proof by a super majority of the validators in the network. The veriﬁcations are collected by the replicator via p2p gossip network, and submitted as one packet that contains a super majority of the validators in the network. This packet veriﬁes all the proofs prior to a speciﬁc hash gen- erated by the Proof of History sequence, and can contain multiple replicator identities at once. 6.7 Attacks 6.7.1 Spam A malicious user could create many replicator identities and spam the net- work with bad proofs. To facilitate faster veriﬁcation, nodes are required to provide the encrypted data and the entire merkle tree to the rest of the network when they request veriﬁcation. The Proof of Replication that is designed in this paper allows for cheap veriﬁcation of any additional proofs, as they take no additional space. But each identity would consume 1 core of encryption time. The replication target should be set to a maximum size of readily available cores. Modern GPUs ship with 3500+ cores. 6.7.2 Partial Erasure A replicator node could attempt to partially erase some of the data to avoid storing the entire state. The number of proofs and the randomness of the 24 seed should make this attack diﬃcult. For example, a user storing 1 terabyte of data erases a single byte from each 1 megabyte block. A single proof that samples 1 byte out of every megabyte would have a likelihood of collision with any erased byte 1 −(1 − 1/1, 000, 0000)1,000,000 = 0.63. After 5 proofs the likelihood is 0.99. 6.7.3 Collusion with PoH generator The signed hash is expected to be used to seed the sample. If a replicator could select a speciﬁc hash in advance then the replicator could erase all bytes that are not going to be sampled. A replicator identity that is colluding with the Proof of History generator could inject a speciﬁc transaction at the end of the sequence before the pre- deﬁned hash for random byte selection is generated. With enough cores, an attacker could generate a hash that is preferable to the replicators identity. This attack could only beneﬁt a single replicator identity. Since all the identities have to use the same exact hash that is cryptographically signed with ECDSA (or equivalent), the resulting signature is unique for each repli- cator identity, and collision resistant. A single replicator identity would only have marginal gains. 6.7.4 Denial of Service The cost of adding an additional replicator identity is expected to be equal to the cost of storage. The cost of adding extra computational capacity to verify all the replicator identities is expected to be equal to the cost of a CPU or GPU core per replication identity. This creates an opportunity for a denial of service attack on the network by creating a large number of valid replicator identities. To limit this attack, the consensus protocol chosen for the network can select a replication target, and award the replication proofs that meet the de- sired characteristics, like availability on the network, bandwidth, geolocation etc... 6.7.5 Tragedy of Commons The PoS veriﬁers could simply conﬁrm PoRep without doing any work. The economic incentives should be lined up with the PoS veriﬁers to do work, 25 Figure 9: System Architecture like by splitting the mining payout between the PoS veriﬁers and the PoRep replication nodes. To further avoid this scenario, the PoRep veriﬁers can submit false proofs a small percentage of the time. They can prove the proof is false by providing the function that generated the false data. Any PoS veriﬁer that conﬁrmed a false proof would be slashed. 7 System Architecture 7.1 Components 7.1.1 Leader, Proof of History generator The Leader is an elected Proof of History generator. It consumes arbitrary user transactions and outputs a Proof of History sequence of all the transac- tions that guarantees a unique global order in the system. After each batch of transactions the Leader outputs a signature of the state that is the result of running the transactions in that order. This signature is signed with the identity of the Leader. 26 7.1.2 State A naive hash table indexed by the users address. Each cell contains the full users address and the memory required for this computation. For example Transaction table contains: 0 31 63 95 127 159 191 223 255 Ripemd of Users Public Key Account unused For a total of 32 bytes. Proof of Stake bonds table contains: 0 31 63 95 127 159 191 223 255 Ripemd of Users Public Key Bond Last Vote unused For a total of 64 bytes. 7.1.3 Veriﬁer, State Replication The Veriﬁer nodes replicate the blockchain state and provide high availability of the blockchain state. The replication target is selected by the consensus algorithm, and the validators in the consensus algorithm select and vote the Proof of Replication nodes they approve of based on oﬀ-chain deﬁned criteria. The network could be conﬁgured with a minimum Proof of Stake bond size, and a requirement for a single replicator identity per bond. 7.1.4 Validators These nodes are consuming bandwidth from Veriﬁers. They are virtual nodes, and can run on the same machines as the Veriﬁers or the Leader, or on separate machines that are speciﬁc to the consensus algorithm conﬁgured for this network. 7.2 Network Limits Leader is expected to be able to take incoming user packets, orders them the most eﬃcient way possible, and sequences them into a Proof of History sequence that is published to downstream Veriﬁers. Eﬃciency is based on 27 Figure 10: Generator network limits memory access patterns of the transactions, so the transactions are ordered to minimize faults and to maximize prefetching. Incoming packet format: 0 31 63 95 127 159 191 223 255 Last Valid Hash Counter u s Fee From \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe Signed Signature 1 of 2 Signature 2 of 2 Size 20 + 8 + 16 + 8 + 32 + 3232 = 148 bytes. 28 The minimal payload that can be supported would be 1 destination ac- count. With payload: 0 31 63 95 127 159 191 223 255 Last Valid Hash Counter u s To Amount Counter Fee From \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe Signed Signature 1 of 2 Signature 2 of 2 With payload the minimum size: 176 bytes The Proof of History sequence packet contains the current hash, counter, and the hash of all the new messages added to the PoH sequence and the state signature after processing all the messages. This packet is sent once every N messages are broadcast. Proof of History packet: 0 31 63 95 127 159 191 223 255 Current Hash Counter Messages Hash State Hash \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe Signed Signature 1 of 2 Signature 2 of 2 Minimum size of the output packet is: 132 bytes On a 1gbps network connection the maximum number of transactions 29 possible is 1 gigabit per second / 176 bytes = 710k tps max. Some loss 1−4% is expected due to Ethernet framing. The spare capacity over the target amount for the network can be used to increase availability by coding the output with Reed-Solomon codes and striping it to the available downstream Veriﬁers. 7.3 Computational Limits Each transaction requires a digest veriﬁcation. This operation does not use any memory outside of the transaction message itself and can be parallelized independently. Thus throughput is expected to be limited by the number of cores available on the system. GPU based ECDSA veriﬁcation servers have had experimental results of 900k operations per second . 7.4 Memory Limits A naive implementation of the state as a 50% full hashtable with 32 byte en- tries for each account, would theoretically ﬁt 10 billion accounts into 640GB. Steady state random access to this table is measured at 1.1 ∗107 writes or reads per second. Based on 2 reads and two writes per transaction, memory throughput can handle 2.75m transactions per second. This was measured on an Amazon Web Services 1TB x1.16xlarge instance. 7.5 High Performance Smart Contracts Smart contracts are a generalized form of transactions. These are programs that run on each node and modify the state. This design leverages extended Berkeley Packet Filter bytecode as fast and easy to analyze and JIT bytecode as the smart contracts language. One of its main advantages is a zero cost Foreign Function Interface. Intrinsics, or functions that are implemented on the platform directly, are callable by programs. Calling the intrinsics suspends that program and schedules the intrinsic on a high performance server. Intrinsics are batched together to execute in parallel on the GPU. In the above example, two diﬀerent user programs call the same intrinsic. Each program is suspended until the batch execution of the intrinsics is 30 Figure 11: Executing BPF programs. complete. An example intrinsic is ECDSA veriﬁcation. Batching these calls to execute on the GPU can increase throughput by thousands of times. This trampoline requires no native operating system thread context switches, since the BPF bytecode has a well deﬁned context for all the memory that it is using. eBPF backend has been included in LLVM since 2015, so any LLVM frontend language can be used to write smart contracts. Its been in the Linux kernel since 2015, and the ﬁrst iterations of the bytecode have been around since 1992. A single pass can check eBPF for correctness, ascertain its runtime and memory requirements and convert it to x86 instructions. References Liskov, Practical use of Clocks http://www.dainf.cefetpr.br/ tacla/SDII/PracticalUseOfClocks.pdf Google Spanner TrueTime consistency https://cloud.google.com/spanner/docs/true-time-external-consistency Solving Agreement with Ordering Oracles http://www.inf.usi.ch/faculty/pedone/Paper/2002/2002EDCCb.pdf 31 Tendermint: Consensus without Mining https://tendermint.com/static/docs/tendermint.pdf Hedera: A Governing Council & Public Hashgraph Network https://s3.amazonaws.com/hedera-hashgraph/hh-whitepaper-v1.0-180313.pdf Filecoin, proof of replication, https://filecoin.io/proof-of-replication.pdf Slasher, A punative Proof of Stake algorithm https://blog.ethereum.org/2014/01/15/slasher-a-punitive-proof-of-stake-algorith BitShares Delegated Proof of Stake https://github.com/BitShares/bitshares/wiki/Delegated-Proof-of-Stake An Eﬃcient Elliptic Curve Cryptography Signature Server With GPU Acceleration http://ieeexplore.ieee.org/document/7555336/ Casper the Friendly Finality Gadget https://arxiv.org/pdf/1710.09437.pdf 32', 'uniswap.pdf': 'Uniswap v4 Core August 2024 Hayden Adams hayden@uniswap.org Moody Salem moody.salem@gmail.com Noah Zinsmeister noah@uniswap.org Sara Reynolds sara@uniswap.org Austin Adams me@aada.ms Will Pote willpote@gmail.com Mark Toda mark@uniswap.org Alice Henshaw alice@uniswap.org Emily Williams emily@uniswap.org Dan Robinson dan@paradigm.xyz ABSTRACT Uniswap v4 is a non-custodial automated market maker imple- mented for the Ethereum Virtual Machine. Uniswap v4 offers cus- tomizability via arbitrary code hooks, allowing developers to aug- ment the concentrated liquidity model introduced in Uniswap v3 with new functionality. In Uniswap v4, anyone can create a new pool with a specified hook, which can run before or after pre- determined pool actions. Hooks can be used to implement features that were previously built into the protocol, like oracles, as well as new features that previously would have required independent implementations of the protocol. Uniswap v4 also offers improved gas efficiency and developer experience through a singleton imple- mentation, flash accounting, and support for native ETH. 1 INTRODUCTION Uniswap v4 is an automated market maker (AMM) facilitating effi- cient exchange of value on the Ethereum Virtual Machine (EVM). As with previous versions of the Uniswap Protocol, it is non- custodial, non-upgradable, and permissionless. The focus of Uniswap v4 is on additional customization for developers and architectural changes for gas efficiency improvements, building on the AMM model built by Uniswap v1 and v2 and the concentrated liquidity model introduced in Uniswap v3. Uniswap v1 and v2 were the first two iterations of the Uniswap Protocol, facilitating ERC-20 <> ETH and ERC-20 <> ERC-20 swaps, respectively, both using a constant product market maker (CPMM) model. Uniswap v3 introduced concentrated liquidity, enabling more capital efficient liquidity through positions that provide liquidity within a limited price range, and multiple fee tiers. While concentrated liquidity and fee tiers increased flexibility for liquidity providers and allowed for new liquidity provision strate- gies, Uniswap v3 lacks flexibility to support new functionalities invented as AMMs and DeFi have evolved. Some features, like the price oracle originally introduced in Uniswap v2 and included in Uniswap v3, allow integrators to uti- lize decentralized onchain pricing data, at the expense of increased gas costs for swappers and without customizability for integrators. Other possible enhancements, such as time-weighted average price orders (TWAP) through a time-weighted average market maker (TWAMM) , volatility oracles, limit orders, or dynamic fees, re- quire reimplementations of the core protocol, and can not be added to Uniswap v3 by third-party developers. Additionally, in previous versions of Uniswap, deployment of new pools involves deploying a new contract—where cost scales with the size of the bytecode—and trades with multiple Uniswap pools involve transfers and redundant state updates across multiple contracts. Additionally since Uniswap v2, Uniswap has required ETH to be wrapped into an ERC-20, rather than supporting native ETH. These design choices came with increased gas costs for end users. In Uniswap v4, we improve on these inefficiencies through a few notable features: • Hooks: Uniswap v4 allows anyone to deploy new concen- trated liquidity pools with custom functionality. For each pool, the creator can define a “hook contract” that imple- ments logic executed at specific points in a call’s lifecycle. These hooks can also manage the swap fee of the pool dynamically, implement custom curves, and adjust fees charged to liquidity providers and swappers though Cus- tom Accounting. • Singleton: Uniswap v4 moves away from the factory model used in previous versions, instead implementing a single contract that holds all pools. The singleton model reduces the cost of pool creation and multi-hop trades. • Flash accounting: The singleton uses “flash accounting,” which allows a caller to lock the pool and access any of its tokens, as long as no tokens are owed to or from the caller by the end of the lock. This functionality is made efficient by the transient storage opcodes described in EIP- 1153 . Flash accounting further reduces the gas cost of trades that cross multiple pools and supports more complex integrations with Uniswap v4. 1 Adams et al. • Native ETH: Uniswap v4 brings back support for native ETH, with support for pairs with native tokens inside v4 pools. ETH swappers and liquidity providers benefit from gas cost reductions from cheaper transfers and removal of additional wrapping costs. • Custom Accounting: The singleton supports both augment- ing and bypassing the native concentrated liquidity pools through hook-returned deltas, utilizing the singleton as an immutable settlement layer for connected pools. This feature can support use-cases like hook withdrawal fees, wrapping assets, or constant product market maker curves like Uniswap v2. The following sections provide in-depth explanations of these changes and the architectural changes that help make them possible. 2 HOOKS Hooks are externally deployed contracts that execute some developer- defined logic at a specified point in a pool’s execution. These hooks allow integrators to create a concentrated liquidity pool with flexi- ble and customizable execution. Optionally, hooks can also return custom deltas that allow the hook to change the behavior of the swap — described in detail in the Custom Accounting section (5). Hooks can modify pool parameters, or add new features and functionality. Example functionalities that could be implemented with hooks include: • Executing large orders over time through TWAMM • Onchain limit orders that fill at tick prices • Volatility-shifting dynamic fees • Mechanisms to internalize MEV for liquidity providers • Median, truncated, or other custom oracle implementations • Constant Product Market Makers (Uniswap v2 functional- ity) 2.1 Action Hooks When someone creates a pool on Uniswap v4, they can specify a hook contract. This hook contract implements custom logic that the pool will call out to during its execution. Uniswap v4 currently supports ten such hook callbacks: • beforeInitialize/afterInitialize • beforeAddLiquidity/afterAddLiquidity1 • beforeRemoveLiquidity/afterRemoveLiquidity • beforeSwap/afterSwap • beforeDonate/afterDonate The address of the hook contract determines which of these hook callbacks are executed. This creates a gas efficient and expressive methodology for determining the desired callbacks to execute, and ensures that even upgradeable hooks obey certain invariants. There are minimal requirements for creating a working hook. In Figure 1, we describe how the beforeSwap and afterSwap hooks work as part of swap execution flow. 1Having separate permissions for ‘beforeAddLiquidity‘ and ‘beforeRemoveLiquidity‘ reflects the difference in security assumptions between those two actions. Hooks that can affect minting but not burning of liquidity are safer for liquidity providers, since they are guaranteed to be able to withdraw their liquidity. 2.2 Hook-managed fees Uniswap v4 allows fees to be taken on swapping by the hook. Swap fees can be either static, or dynamically managed by a hook contract. The hook contract can also choose to allocate a percentage of the swap fees to itself. Fees that accrue to hook contracts can be allocated arbitrarily by the hook’s code, including to liquidity providers, swappers, hook creators, or any other party. The capabilities of the hook are limited by immutable flags cho- sen when the pool is created. For example, a pool creator can choose whether a pool has a static fee (and what that fee is) or dynamic fees. Governance also can take a capped percentage of swap fees, as discussed below in the Governance section (6.2). 3 SINGLETON AND FLASH ACCOUNTING Previous versions of the Uniswap Protocol use the factory/pool pattern, where the factory creates separate contracts for new token pairs. Uniswap v4 uses a singleton design pattern where all pools are managed by a single contract, making pool deployment 99% cheaper. The singleton design complements another architectural change in v4: flash accounting. In previous versions of the Uniswap Pro- tocol, most operations (such as swapping or adding liquidity to a pool) ended by transferring tokens. In v4, each operation updates an internal net balance, known as a delta, only making external trans- fers at the end of the lock. The new take() and settle() functions can be used to borrow or deposit funds to the pool, respectively. By requiring that no tokens are owed to the pool manager or to the caller by the end of the call, the pool’s solvency is enforced. Flash accounting simplifies complex pool operations, such as atomic swapping and adding. When combined with the singleton model, it also simplifies multi-hop trades or compound operations like swapping before adding liquidity. Before the Cancun hard fork, the flash accounting architecture was expensive because it required storage updates at every balance change. Even though the contract guaranteed that internal account- ing data is never actually serialized to storage, users would still pay those same costs once the storage refund cap was exceeded . But, because balances must be 0 by the end of the transaction, accounting for these balances can be implemented with transient storage, as specified by EIP-1153 . Together, singleton and flash accounting enable more efficient routing across multiple v4 pools, reducing the cost of liquidity fragmentation. This is especially useful given the introduction of hooks, which will greatly increase the number of pools. 4 NATIVE ETH Uniswap v4 is bringing back native ETH in trading pairs. While Uniswap v1 was strictly ETH paired against ERC-20 tokens, native ETH pairs were removed in Uniswap v2 due to implementation complexity and concerns of liquidity fragmentation across WETH and ETH pairs. Singleton and flash accounting mitigate these prob- lems, so Uniswap v4 allows for both WETH and ETH pairs. Native ETH transfers are about half the gas cost of ERC-20 trans- fers (21k gas for ETH and around 40k gas for ERC-20s). Currently Uniswap v2 and v3 require the vast majority of users to wrap 2 Uniswap v4 Core Start swap S0. Check beforeSwap flag H1. Run beforeSwap Hook S1. Execute swap S2. Check afterSwap flag H2. Run afterSwap Hook End swap True False Return True False Return Figure 1: Swap Hook Flow (unwrap) their ETH to (from) WETH before (after) trading on the Uniswap Protocol, requiring extra gas. According to transaction data, the majority of users start or end their transactions in ETH, adding this additional unneeded complexity. 5 CUSTOM ACCOUNTING Newly introduced in Uniswap v4 is custom accounting - which allows hook developers to alter end user actions utilizing hook- returned deltas, token amounts that are debited/credited to the user and credited/debited to the hook, respectively. This allows hook developers to potentially add withdrawal fees on LP positions, customized LP fee models, or match against some flow, all while ultimately utilizing the internal concentrated liquidity native to Uniswap v4. Importantly, hook developers can also forgo the concentrated liquidity model entirely, creating custom curves from the v4 swap parameters. This creates interface composability for integrators - allowing the hook to map the swap parameters to their internal logic. In Uniswap v3, users were required to utilize the concentrated liquidity AMM introduced in the same version. Since their intro- duction, concentrated liquidity AMMs have become widely used as the base liquidity provision strategy in the decentralized finance markets. While concentrated liquidity is able to support most ar- bitrary liquidity provision strategies, it may require increased gas overhead to implement specific strategies. One possible example is a Uniswap v2 on Uniswap v4 hook, which bypasses the internal concentrated liquidity model entirely - utilizing a constant product market maker fully inside of the hook. Using custom accounting is cheaper than creating a similar strategy in the concentrated liquidity math. The benefit of custom accounting for developers - compared to rolling a custom AMM - is the singleton, flash accounting, and ERC-6909. These features support cheaper multi-hop swaps, security benefits, and easier integration for flow. Developers should also benefit from a well-audited code-base for the basis of their AMM. Custom accounting will also support experimentation in liquidity provision strategies, which historically requires the creation of an entirely new AMM. Creating a custom AMM requires significant technical resources and investment, which may not be economically viable for many. 6 OTHER NOTABLE FEATURES 6.1 ERC-6909 Accounting Uniswap v4 supports the minting/burning of singleton-implemented ERC-6909 tokens for additional token accounting, described in the ERC-6909 specification . Users can now keep tokens within the singleton and avoid ERC-20 transfers to and from the contract. This will be especially valuable for users and hooks who continually use the same tokens over multiple blocks or transactions, like frequent swappers, liquidity providers, or custom accounting hooks. 6.2 Governance updates Similar to Uniswap v3, Uniswap v4 allows governance the ability to take up to a capped percentage of the swap fee on a particular pool, which are additive to LP fees. Unlike in Uniswap v3, governance does not control the permissible fee tiers or tick spacings. 6.3 Gas reductions As discussed above, Uniswap v4 introduces meaningful gas op- timizations through flash accounting, the singleton model, and support for native ETH. Additionally, the introduction of hooks makes the protocol-enshrined price oracle that was included in Uniswap v2 and Uniswap v3 unnecessary, which also means base pools forgo the oracle altogether and save around 15k gas on the first swap on a pool in each block. 3 Adams et al. 6.4 donate() donate() allows users, integrators, and hooks to directly pay in- range liquidity providers in either or both of the tokens of the pool. This functionality relies on the fee accounting system to facilitate efficient payments. The fee payment system can only support either of the tokens in the token pair for the pool. Potential use-cases could be tipping in-range liquidity providers on TWAMM orders or new types of fee systems. 7 SUMMARY In summary, Uniswap v4 is a non-custodial, non-upgradeable, and permissionless AMM protocol. It builds upon the concentrated liquidity model introduced in Uniswap v3 with customizable pools through hooks. Complementary to hooks are other architectural changes like the singleton contract which holds all pool state in one contract, and flash accounting which enforces pool solvency across each pool efficiently. Additionally, hook developers can elect to bypass the concentrated liquidity entirely, utilizing the v4 singleton as an arbitrary delta resolver. Some other improvements are native ETH support, ERC-6909 balance accounting, new fee mechanisms, and the ability to donate to in-range liquidity providers. REFERENCES Austin Adams, Ciamac Moallemi, Sara Reynolds, and Dan Robinson. 2024. am-AMM: An Auction-Managed Automated Market Maker. arXiv preprint arXiv:2403.03367 (2024). Hayden Adams. 2018. Uniswap v1 Core. Retrieved Jun 12, 2023 from https: //hackmd.io/@HaydenAdams/HJ9jLsfTz Hayden Adams, Noah Zinsmeister, and Dan Robinson. 2020. Uniswap v2 Core. Retrieved Jun 12, 2023 from https://uniswap.org/whitepaper.pdf Hayden Adams, Noah Zinsmeister, Moody Salem, River Keefer, and Dan Robin- son. 2021. Uniswap v3 Core. Retrieved Jun 12, 2023 from https://uniswap.org/ whitepaper-v3.pdf Alexey Akhunov and Moody Salem. 2018. EIP-1153: Transient storage opcodes. Retrieved Jun 12, 2023 from https://eips.ethereum.org/EIPS/eip-1153 Vitalik Buterin and Martin Swende. 2021. EIP-3529: Reduction in refunds. Retrieved Jun 12, 2023 from https://eips.ethereum.org/EIPS/eip-3529 JT Riley, Dillon, Sara, Vectorized, and Neodaoist. 2023. ERC-6909: Minimal Multi- Token Interface. Retrieved Aug 26, 2024 from https://eips.ethereum.org/EIPS/eip- 6909 Dave White, Dan Robinson, and Hayden Adams. 2021. TWAMM. Retrieved Jun 12, 2023 from https://www.paradigm.xyz/2021/07/twamm DISCLAIMER This paper is for general information purposes only. It does not constitute investment advice or a recommendation or solicitation to buy or sell any investment and should not be used in the evaluation of the merits of making any investment decision. It should not be relied upon for accounting, legal or tax advice or investment rec- ommendations. This paper reflects current opinions of the authors and is not made on behalf of Uniswap Labs, Paradigm, or their affiliates and does not necessarily reflect the opinions of Uniswap Labs, Paradigm, their affiliates or individuals associated with them. The opinions reflected herein are subject to change without being updated. 4', 'chainlink.pdf': 'ChainLink A Decentralized Oracle Network Steve Ellis, Ari Juels†, and Sergey Nazarov 4 September 2017 (v1.0) Abstract Smart contracts are poised to revolutionize many industries by replacing the need for both traditional legal agreements and centrally automated digital agreements. Both performance veriﬁcation and execution rely on manual actions from one of the contracting parties, or an automated system that programmat- ically retrieves and updates relevant changes. Unfortunately, because of their underlying consensus protocols, the blockchains on which smart contracts run cannot support native communication with external systems. Today, the solution to this problem is to introduce a new functionality, called an oracle, that provides connectivity to the outside world. Existing oracles are centralized services. Any smart contract using such services has a single point of failure, making it no more secure than a traditional, centrally run digital agreement. In this paper we present ChainLink, a decentralized oracle network. We de- scribe the on-chain components that ChainLink provides for contracts to gain external connectivity, and the software powering the nodes of the network. We present both a simple on-chain contract data aggregation system, and a more eﬃcient oﬀ-chain consensus mechanism. We also describe supporting reputation and security monitoring services for ChainLink that help users make informed provider selections and achieve robust service even under aggressively adver- sarial conditions. Finally, we characterize the properties of an ideal oracle as guidance for our security strategy, and lay out possible future improvements, including richly featured oracle programming, data-source infrastructure modi- ﬁcations, and conﬁdential smart-contract execution. 1 Contents 1 Introduction 3 2 Architectural Overview 4 2.1 On-Chain Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Oﬀ-Chain Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3 Oracle Security 7 4 ChainLink Decentralization Approach 11 4.1 Distributing sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.2 Distributing oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5 ChainLink Security Services 16 5.1 Validation System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.2 Reputation System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.3 Certiﬁcation Service . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.4 Contract-Upgrade Service . . . . . . . . . . . . . . . . . . . . . . . . 20 5.5 LINK token usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 6 Long-Term Technical Strategy 21 6.1 Conﬁdentiality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 6.2 Infrastructure changes . . . . . . . . . . . . . . . . . . . . . . . . . . 25 6.3 Oﬀ-chain computation . . . . . . . . . . . . . . . . . . . . . . . . . . 26 7 Existing Oracle Solutions 26 8 Conclusion 27 A Oﬀ-Chain Aggregation 33 A.1 OCA protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 A.2 Proof sketches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 A.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 B SGX Trust Assumptions 38 2 1 Introduction Smart contracts are applications that execute on decentralized infrastructure, such as a blockchain. They are tamperproof, in the sense that no party (even their cre- ator) can alter their code or interfere with their execution. Historically, contracts embodied in code have run in a centralized manner that leaves them subject to al- teration, termination, and even deletion by a privileged party. In contrast, smart contracts’ execution guarantees, which bind all parties to an agreement as written, create a new and powerful type of trust relationship that does not rely on trust in any one party. Because they are self-verifying and self-executing (i.e., tamperproof as explained above), smart contracts thus oﬀer a superior vehicle for realizing and administering digital agreements. The powerful new trust model that smart contracts embody, though, introduces a new technical challenge: connectivity. The vast majority of interesting1 smart contract applications rely on data about the real world that comes from key resources, speciﬁcally data feeds and APIs, that are external to the blockchain. Because of the mechanics of the consensus mechanisms underpinning blockchains, a blockchain cannot directly fetch such critical data. We propose a solution to the smart contract connectivity problem in the form of ChainLink, a secure oracle network. What diﬀerentiates ChainLink from other oracle solutions is its ability to operate as a fully decentralized network. This decentralized approach limits the trust in any single party, enabling the tamperproof quality valued in smart contracts to be extended to the end-to-end operation between smart contracts and the APIs they rely on. Making smart contracts externally aware, meaning capable of interacting with oﬀ-chain resources, is necessary if they are going to replace the digital agreements in use today. Today, the lion’s share of traditional contractual agreements that have been digi- tally automated use external data to prove contractual performance, and require data outputs to be pushed to external systems. When smart contracts replace these older contractual mechanisms, they will require high-assurance versions of the same types of data inputs and outputs. Examples of potential next-generation smart contracts and their data requirements include: • Securities smart contracts such as bonds, interest rate derivatives, and many others will require access to APIs reporting market prices and market reference data, e.g. interest rates. 1The main use of smart contracts in Ethereum today is management of tokens, which are a common functionality in most smart contract networks. We believe that the current focus on tokens to the exclusion of many other possible applications is due to a lack of adequate oracle services, a situation ChainLink speciﬁcally aims to remedy. 3 • Insurance smart contracts will need data feeds about IoT data related to the insurable event in question, e.g.: was the warehouse’s magnetic door locked at the time of breach, was the company’s ﬁrewall online, or did the ﬂight you had insurance for arrive on time. • Trade ﬁnance smart contracts will need GPS data about shipments, data from supply chain ERP systems, and customs data about the goods being shipped in order to conﬁrm fulﬁllment of contractual obligations. Another problem common to these examples is the inability for smart contracts to output data into oﬀ-chain systems. Such output often takes the form of a payment message routed to traditional centralized infrastructure in which users already have accounts, e.g., for bank payments, PayPal, and other payment networks. ChainLink’s ability to securely push data to APIs and various legacy systems on behalf of a smart contract permits the creation of externally-aware tamperproof contracts. Whitepaper roadmap In this whitepaper*, we review the ChainLink architecture (Section 2). We then explain how we deﬁne security for oracles (Section 3). We describe the ChainLink approach to decentralization / distribution of oracles and data sources (Section 4), and follow with a discussion of the four security services proposed by ChainLink, as well as the role played by LINK tokens (Section 5). We then describe a proposed long-term development strategy, which includes better conﬁdentiality protections, the use of trusted hardware, infrastructure changes, and general oracle programmability (Section 6). We brieﬂy review alternative oracle designs (Section 7), and conclude with a short discussion of the design principles and philosophy guiding ChainLink development (Section 8). 2 Architectural Overview ChainLink’s core functional objective is to bridge two environments: on-chain and oﬀ- chain. We describe the architecture of each ChainLink component below. ChainLink will initially be built on Ethereum , , but we intend for it to support all leading smart contract networks for both oﬀ-chain and cross-chain interactions. In both its on and oﬀ-chain versions, ChainLink has been designed with modularity in mind. Every piece of the ChainLink system is upgradable, so that diﬀerent components can be replaced as better techniques and competing implementations arise. 4 2.1 On-Chain Architecture As an oracle service, ChainLink nodes return replies to data requests or queries made by or on behalf of a user contract, which we refer to as requesting contracts and denote by USER-SC. ChainLink’s on-chain interface to requesting contracts is itself an on-chain contract that we denote by CHAINLINK-SC. Behind CHAINLINK-SC, ChainLink has an on-chain component consisting of three main contracts: a reputation contract, an order-matching contract, and an aggregating contract. The reputation contract keeps track of oracle-service-provider performance metrics. The order-matching smart contract takes a proposed service level agreement, logs the SLA parameters, and collects bids from oracle providers. It then selects bids using the reputation contract and ﬁnalizes the oracle SLA. The aggregating contract collects the oracle providers’ responses and calculates the ﬁnal collective result of the ChainLink query. It also feeds oracle provider metrics back into the reputation contract. ChainLink contracts are designed in a modular manner, allowing for them to be conﬁgured or replaced by users as needed. The on-chain work ﬂow has three steps: 1) oracle selection, 2) data reporting, 3) result aggregation. Oracle Selection An oracle services purchaser speciﬁes requirements that make up a service level agreement (SLA) proposal. The SLA proposal includes details such as query parameters and the number of oracles needed by the purchaser. Additionally, the purchaser speciﬁes the reputation and aggregating contracts to be used for the rest of the agreement. Using the reputation maintained on-chain, along with a more robust set of data gathered from logs of past contracts, purchasers can manually sort, ﬁlter, and select oracles via oﬀ-chain listing services. Our intention is for ChainLink to maintain one such listing service, collecting all ChainLink-related logs and verifying the binaries of listed oracle contracts. We further detail the listing service and reputation systems in Section 5. The data used to generate listings will be pulled from the blockchain, allowing for alternative oracle-listing services to be built. Purchasers will submit SLA proposals to oracles oﬀ-chain, and come to agreement before ﬁnalizing the SLA on-chain. Manual matching is not possible for all situations. For example, a contract may need to request oracle services dynamically in response to its load. Automated solu- tions solve this problem and enhance usability. For these reasons, automated oracle matching is also being proposed by ChainLink through the use of order-matching contracts. Once the purchaser has speciﬁed their SLA proposal, instead of contacting the ora- cles directly, they will submit the SLA to an order-matching contract. The submission of the proposal to the order-matching contract triggers a log that oracle providers can 5 monitor and ﬁlter based on their capabilities and service objectives. ChainLink nodes then choose whether to bid on the proposal or not, with the contract only accepting bids from nodes that meet the SLA’s requirements. When an oracle service provider bids on a contract, they commit to it, speciﬁcally by attaching the penalty amount that would be lost due to their misbehavior, as deﬁned in the SLA. Bids are accepted for the entirety of the bidding window. Once the SLA has received enough qualiﬁed bids and the bidding window has ended, the requested number of oracles is selected from the pool of bids. Penalty payments that were oﬀered during the bidding process are returned to oracles who were not selected, and a ﬁnalized SLA record is created. When the ﬁnalized SLA is recorded it triggers a log notifying the selected oracles. The oracles then perform the assignment detailed by the SLA. Data Reporting Once the new oracle record has been created, the oﬀ-chain oracles execute the agreement and report back on-chain. For more detail about oﬀ-chain interactions, see Sections 2.2 and 4. Result Aggregation Once the oracles have revealed their results to the oracle con- tract, their results will be fed to the aggregating contract. The aggregating contract tallies the collective results and calculates a weighted answer. The validity of each oracle response is then reported to the reputation contract. Finally, the weighted answer is returned to the speciﬁed contract function in USER-SC. Detecting outlying or incorrect values is a problem that is speciﬁc to each type of data feed and application. For instance, detecting and rejecting outlying answers before averaging may be necessary for numeric data but not boolean. For this reason, there will not be a speciﬁc aggregating contract, but a conﬁgurable contract address which is speciﬁed by the purchaser. ChainLink will include a standard set of ag- gregating contracts, but customized contracts may also be speciﬁed, provided they conform to the standard calculation interface. 2.2 Oﬀ-Chain Architecture Oﬀ-chain, ChainLink initially consists of a network of oracle nodes connected to the Ethereum network, and we intend for it to support all leading smart contract net- works. These nodes independently harvest responses to oﬀ-chain requests. As we explain below, their individual responses are aggregated via one of several possible consensus mechanisms into a global response that is returned to a requesting con- tract USER-SC. The ChainLink nodes are powered by the standard open source core implementation which handles standard blockchain interactions, scheduling, and con- necting with common external resources. Node operators may choose to add software 6 extensions, known as external adapters, that allow the operators to oﬀer additional specialized oﬀ-chain services. ChainLink nodes have already been deployed along- side both public blockchains and private networks in enterprise settings; enabling the nodes to run in a decentralized manner is the motivation for the ChainLink network. ChainLink Core. The core node software is responsible for interfacing with the blockchain, scheduling, and balancing work across its various external services. Work done by ChainLink nodes is formatted as assignments. Each assignment is a set of smaller job specifcations, known as subtasks, which are processed as a pipeline. Each subtask has a speciﬁc operation it performs, before passing its result onto the next subtask, and ultimately reaching a ﬁnal result. ChainLink’s node software comes with a few subtasks built in, including HTTP requests, JSON parsing, and conversion to various blockchain formats. External Adapters. Beyond the built-in subtask types, custom subtasks can be deﬁned by creating adapters. Adapters are external services with a minimal REST API. By modeling adapters in a service-oriented manner, programs in any program- ming language can be easily implemented simply by adding a small intermediate API in front of the program. Similarly, interacting with complicated multi-step APIs can be simpliﬁed to individual subtasks with parameters. Subtask Schemas. We anticipate that many adapters will be open sourced, so that services can be audited and run by various community members. With many diﬀerent types of adapters being developed by many diﬀerent developers, ensuring compatibility between adapters is essential. ChainLink currently operates with a schema system based on JSON Schema , to specify what inputs each adapter needs and how they should be formatted. Sim- ilarly, adapters specify an output schema to describe the format of each subtask’s output. 3 Oracle Security In order to explain ChainLink’s security architecture, we must ﬁrst explain why se- curity is important—and what it means. Why must oracles be secure? Returning to our simple examples in Section 1, if a smart contract security gets a false data feed, it may payout the incorrect party, if smart contract insurance data feeds can be tampered with by the insured party 7 Figure 1: ChainLink workﬂow: 1) USER-SC makes an on-chain request; 2) CHAINLINK-SC logs an event for the oracles; 3) ChainLink core picks up the event and routes the assignment to an adapter; 4) ChainLink adapter performs a request to an external API; 5) ChainLink adapter processes the response and passes it back to the core; 6) ChainLink core reports the data to CHAINLINK-SC; 7) CHAINLINK-SC aggregates responses and passes them back as a single response to USER-SC. there may be insurance fraud, and if GPS data given to a trade ﬁnance contract can be modiﬁed after it leaves the data provider, payment can be released for goods that haven’t arrived. More generally, a well-functioning blockchain, with its ledger or bulletin-board abstraction, oﬀers very strong security properties. Users rely on the blockchain as a functionality that correctly validates transactions and prevents data from being altered. They treat it in eﬀect like a trusted third party (a concept we discuss at length below). A supporting oracle service must oﬀer a level of security commensurate with that of the blockchain it supports. An oracle too must therefore serve users as an eﬀective trusted third party, providing correct and timely responses with very high probability. The security of any system is only as strong as its weakest link, so a highly trustworthy oracle is required to preserve the trustworthiness of a well- engineered blockchain. Deﬁning oracle security: An ideal view. In order to reason about oracle se- curity, we must ﬁrst deﬁne it. An instructive, principled way to reason about oracle security stems from the following thought experiment. Imagine that a trusted third party (TTP)—an ideal entity or functionality that always carries out instructions faithfully to the letter—were tasked with running an oracle. We’ll denote this oracle by ORACLE (using all caps in general to denote an entity fully trusted by users), and suppose that the TTP obtains data from a perfectly trustworthy data source Src. Given this magical service ORACLE, what instructions would we ask it to carry out? To achieve the property of integrity, also referred to as the authenticity prop- erty , we would simply ask that ORACLE perform the following steps: 8 Figure 2: Behavior of an ideal oracle ORACLE is deﬁned by steps: 1) Accept request; 2) Obtain data; 3) Return data. Additionally, to protect the conﬁdentiality of a request, upon decrypting it, ORACLE never uses or reveals the data it contains, except to query Src. 1. Accept request: Ingest from a smart contract USER-SC a request Req = (Src, τ, q) that speciﬁes a target data source Src, a time or range of times τ, and a query q; 2. Obtain data: Send query q to Src at time τ; 3. Return data: On receiving answer a, return a to the smart contract. These simple instructions, correctly carried out, deﬁne a strong, meaningful, but simple notion of security. Intuitively, they dictate that ORACLE acts as a trustworthy bridge between Src and USER-SC.2 For example, if Src is https://www.FountOfKnowledge.com, τ is 4 p.m., and q = “price for ticker INTC”, the integrity of ORACLE guarantees that it will provide USER-SC with exactly the price of INTC as queried at 4 p.m. at https://www.FountOfKnowledge.com. Conﬁdentiality is another desirable property for oracles. As USER-SC sends Req to ORACLE in the clear on the blockchain, Req is public. There are many situations in which Req is sensitive and its publication could be harmful. If USER-SC is a ﬂight insurance contract, for example, and sends ORACLE a query Req regarding a particular user’s ﬂight (q = “Ether Air Flight 338”), the result would be that a user’s ﬂight plans are revealed to the whole world. If USER-SC is a contract for 2Of course, many details are omitted here. ORACLE should communicate with both USER-SC and source Src over secure, i.e., tamperproof, channels. (If Src is a web server, TLS is required. To communicate with USER-SC, ORACLE must be sure to scrape the right blockchain and digitally sign A appropriately.) 9 ﬁnancial trading, Req could leak information about a user’s trades and portfolio. There are many other examples, of course. To protect the conﬁdentiality of Req, we can require that data in Req be encrypted under a (public key) belonging to ORACLE. Continuing to leverage the TTP nature of ORACLE, we could then simply give ORACLE the information-ﬂow constraint: Upon decrypting Req, never reveal or use data in Req except to query Src. There are other important oracle properties, such as availability, the last of the classical CIA (Conﬁdentiality-Integrity-Availability) triad. A truly ideal service OR- ACLE, of course, would never go down. Availability also encompasses more subtle properties such as censorship resistance: An honest ORACLE will not single out par- ticular smart contracts and deny their requests. The concept of a trusted third party is similar to the notion of an ideal function- ality used to prove the security of cryptographic protocols in certain models. We can also model a blockchain in similar terms, conceptualizing it in terms of a TTP that maintains an ideal bulletin board. Its instructions are to accept transactions, validate them, serialize them, and maintain them permanently on the bulletin board, an append-only data structure. Why the ideal oracle (ORACLE) is hard to achieve. There is, of course, no perfectly trustworthy data source Src. Data may be benignly or maliciously corrupted due to faulty web sites, cheating service providers, or honest mistakes. If Src isn’t trustworthy, then even if ORACLE does operate exactly like a TTP as in- structed above, it still doesn’t completely meet the notion of security we want. Given a faulty source Src, the integrity property deﬁned above no longer means that an oracle’s answer a is correct. If the true price of Intel is $40 and https://www.FountOfKnowledge.com misreports it as $50, for example, then ORACLE will send the incorrect value a = $50 to USER-SC. This problem is unavoidable when using a single source Src. ORACLE simply has no way to know whether the answers Src provides to its queries are correct. A bigger issue, of course, is the fact that our TTP for ORACLE is just an abstrac- tion. No service provider is unconditionally trustworthy. Even the best-intentioned may be buggy or hacked. So there is no way to for a user or smart contract to have absolute assurance that a service ORACLE will carry out its instructions faithfully. ChainLink reasons about its security protocols in terms of this ideal functionality ORACLE. Our goal in ChainLink is to achieve a real world system with properties as close as possible to those of ORACLE under realistic trust assumptions. We now explain how. For simplicity in what follows, we now denote by CHAINLINK-SC the complete set of ChainLink contracts, i.e., its full on-chain functionality (not just its interface 10 to requesting contracts). We thereby abstract away the multiple individual contracts actually used in the system architecture. 4 ChainLink Decentralization Approach We propose three basic complementary approaches to ensuring against faulty nodes: (1) Distribution of data sources; (2) Distribution of oracles; and (3) Use of trusted hardware. We discuss the ﬁrst two approaches, which involve decentralization, in this section. We discuss our long-term strategy for trusted hardware, a diﬀerent and complementary approach, in Section 6. 4.1 Distributing sources A simple way to deal with a faulty single source Src is to obtain data from multiple sources, i.e., distribute the data source. A trustworthy ORACLE can query a collection of sources Src1, Src2, . . . , Srck, obtain responses a1, a2, . . . , ak, and aggregate them into a single answer A = agg(a1, a2, . . . , ak). ORACLE might do this in any of a number of ways. One, for example, is majority voting. If a majority of sources return the identical value a, the function agg returns a; otherwise it returns an error. In this case, provided that a majority (> k/2) sources are functioning correctly, ORACLE will always return a correct value A. Many alternative functions agg can ensure robustness against erroneous data or handle ﬂuctuations in data values over time (e.g, stock prices). For example, agg might discard outliers (e.g., the largest and smallest values ai) and output the mean of the remaining ones. Of course, faults may be correlated across data sources in a way that weakens the assurances provided by aggregation. If site Src1 = EchoEcho.com obtains its data from Src2 = TheHorsesMouth.com, an error at Src2 will always imply an error at Src1. More subtle correlations between data sources can also occur. Chainlink also proposes to pursue research into mapping and reporting the independence of data sources in an easily digestible way so that oracles and users can avoid undesired correlations. 4.2 Distributing oracles Just as sources can be distributed, our ideal service ORACLE itself can be approxi- mated as a distributed system. This is to say that instead of a single monolithic oracle node O, we can instead have a collection of n diﬀerent oracle nodes {O1, O2, . . . , On}. Each oracle Oi contacts its own distinct set of data sources which may or may not 11 Figure 3: Requests are distributed across both oracles and data sources. This ﬁgure shows an example of such two-level distribution. overlap with those of other oracles. Oi aggregates responses from its data sources and outputs its own distinct answer Ai to a query Req. Some of these oracles may be faulty. So clearly the set of all oracles’ answers A1, A2, . . . , An will need to be aggregated in a trustworthy way into a single, author- itative value A. But given the possibility of faulty oracles, where and how will this aggregation happen in ChainLink? Initial solution: In-contract aggregation. Our initial proposed solution in ChainLink will be a simple one called in-contract aggregation. CHAINLINK-SC— which, again, denotes the on-chain part of ChainLink—will itself aggregate oracle responses. (Alternatively, CHAINLINK-SC may call another aggregation contract, but for conceptual simplicity we assume that the two components form a single contract.) In other words, CHAINLINK-SC will compute A = Agg(A1, A2, . . . , An) for some func- tion Agg (similar to agg, as described above), and send the result A to USER-SC. This approach is practical for small n, and has several distinct beneﬁts: • Conceptual simplicity: Despite the fact that the oracle is distributed, a single entity, CHAINLINK-SC, performs aggregation by executing Agg. • Trustworthiness: As CHAINLINK-SC’s code can be publicly inspected, its correct behavior can be veriﬁed. (CHAINLINK-SC will be a relatively small, simple piece of code.) Additionally, CHAINLINK-SC’s execution is fully visible on- 12 chain. Thus users, i.e., creators of USER-SC, can achieve a high degree of trust in CHAINLINK-SC. • Flexibility: CHAINLINK-SC can implement most desired aggregation functions Agg—the majority function, averaging, etc. Simple as it is, this approach presents a novel and interesting technical challenge, namely the problem of freeloading. A cheating oracle Oz can observe the response Ai of another oracle Oi and copy it. In this way, oracle Oz avoids the expense of querying data sources, which may charge per-query fees. Freeloading weakens security by undermining the diversity of data source queries and also disincentivizes oracles from responding quickly: Responding slowly and freeloading is a cheaper strategy. We suggest a well known solution to this problem, namely the use of a commit / reveal scheme. In a ﬁrst round, oracles send CHAINLINK-SC cryptographic commit- ments to their responses. After CHAINLINK-SC has received a quorum of responses, it initiates a second round in which oracles reveal their responses. Algorithm 1 shows a simple sequential protocol that guarantees availability given 3f + 1 nodes. It uses a commit / reveal scheme to prevent freeloading. Oracle responses are decommitted, and thus exposed to a potential freeloader only after all commitments have been made, thereby excluding the freeloader from copying other oracles’ responses. On-chain protocols can leverage block times to support synchronous protocol de- signs. In ChainLink, however, oracle nodes obtain data from sources that may have highly variable response times, and decommitment times by nodes can vary due to, e.g., use of diﬀerent gas prices in Ethereum. To ensure the fastest possible protocol responsiveness, therefore, Alg. 1 is designed as an asynchronous protocol. Here, Commitr(A) denotes a commitment of value A with witness r, while SID denotes the set of valid session ids. The protocol assumes authenticated channels among all players. It is easy to see that Alg. 1 will terminate successfully. Given 3f + 1 nodes in total, at most f are faulty, so at least 2f + 1 will send commitments in Step 4. Of those commitments, at most f come from faulty nodes, so at least f + 1 come from honest nodes. All such commitments will eventually be decommitted. Additionally, it is easy to see that A will be correct in Alg.1. Of the f + 1 decommitments on the single value A, at least one has to come from an honest node. In-contract aggregation via Alg. 1 will be the main approach supported by Chain- Link in the short term. The proposed initial implementation will involve a more so- phisticated, concurrent variant of the algorithm. Our longer-term proposal is reﬂected in the rather more complicated protocol OCA (Oﬀ-Chain Aggregation) speciﬁed in Algorithms 2 and 3 in Appendix A. OCA is an oﬀ-chain aggregation protocol that 13 Algorithm 1 InChainAgg({Oi}n i=1) (code for CHAINLINK-SC) 1: Wait until Req is received from USER-SC. 2: sid ←$ SID 3: Broadcast (request, sid). 4: Wait until set C of 2f +1 messages (commit, ci = Commitri(Ai), sid) from distinct Oi are received. 5: Broadcast (committed, sid). 6: Wait until set D of f + 1 distinct valid decommitments (decommit, (ri, Ai), sid) are received where, for some A, all Ai = A. 7: Send (Answer, A, sid) to USER-SC. minimizes on-chain transaction costs. That protocol also includes payment to oracle nodes and ensures against payments to freeloaders. Medium-term strategy: Oﬀ-chain aggregation. In-contract aggregation has a key disadvantage: Cost. It incurs the cost of transmitting and processing on- chain O(n) oracle messages (commits and reveals for A1, A2, . . . , An). In permissioned blockchains, this overhead may be acceptable. In permissionless blockchains with on- chain transaction fees such as Ethereum, if n is large, the costs can be prohibitive. A more cost-eﬀective approach is to aggregate oracle responses oﬀ-chain and transmit a single message to CHAINLINK-SC A. We propose deployment of this approach, called oﬀ-chain aggregation, in the medium-to-long term. The problem of achieving a consensus value A in the face of potentially faulty nodes is much like the problem of consensus that underpins blockchains themselves. Given a predetermined set of oracles, one might consider using a classical Byzantine Fault Tol- erant (BFT) consensus algorithm to compute A. Classical BFT protocols, however, aim to ensure that at the end of a protocol invocation, all honest nodes store the same value, e.g., in a blockchain, that all nodes store the same fresh block. In our oracle setting, the goal is slightly diﬀerent. We want to ensure that CHAINLINK-SC (and then USER-SC) obtains aggregate answer A = Agg(A1, A2, . . . , An) without partici- pating in the consensus protocol and without needing to receive answers from multiple oracles. The problem of freeloading, moreover, still needs to be addressed. The ChainLink system proposes the use of a simple protocol involving thresh- old signatures. Such signatures can be realized using any of a number of signature schemes, but are especially simple to implement using Schnorr signatures . In this approach, oracles have a collective public key pk and a corresponding private key sk that is shared among O1, O2, . . . , On in a (t, n)-threshold manner . Such a sharing means that every node Oi has a distinct private / public keypair (ski, pki). Oi can 14 Figure 4: Sigsk[A] can be achieved by any n/2+1 of the oracles. generate a partial signature σi = Sigski[Ai] that can be veriﬁed with respect to pki. The key feature of this setup is that partial signatures on the same value A can be aggregated across any set of t oracles to yield a single valid collective signature Σ = Sigsk[A] on an answer A. No set of t −1 oracles, however, can produce a valid signature on any value. The single signature Σ thus implicitly embodies the partial signatures of at least t oracles. Threshold signatures can be realized naïvely by letting Σ consist explicitly of a set of t valid, independent signatures from individual nodes. Threshold signatures have similar security properties to this naïve approach. But they provide a signiﬁcant on- chain performance improvement: They reduce the size and cost of verifying Σ by a factor of t. With this setup, it would seem that oracles can just generate and broadcast partial signatures until t such partial signatures enable the computation of Σ. Again, though, the problem of freeloading arises. We must therefore ensure that oracles genuinely obtain data from their designated sources, rather than cheating and copying Ai from another oracle. Our solution involves a ﬁnancial mechanism: An entity PROVIDER (realizable as a smart contract) rewards only oracles that have sourced original data for their partial signatures. In a distributed setting, determining which oracles qualify for payment turns out to be tricky. Oracles may intercommunicate oﬀ-chain and we no longer have a sin- gle authoritative entity (CHAINLINK-SC) receiving responses and are therefore un- 15 able to identify eligible payees directly among participating oracles. Consequently, PROVIDER must obtain evidence of misbehavior from the oracles themselves, some of which may be untrustworthy. We propose the use of consensus-like mechanisms in our solution for ChainLink to ensure that PROVIDER does not pay freeloading oracles. The oﬀ-chain aggregation system we propose for ChainLink, with accompanying security proof sketches, may be found in Appendix A. It makes use of a distributed protocol based on threshold signatures that provides resistance to freeloading by f < n/3 oracles. We believe resistance to freeloading is an interesting new technical problem. 5 ChainLink Security Services Thanks to the protocols we have just described in the previous section, ChainLink proposes to ensure availability and correctness in the face of up to f faulty oracles. Additionally, trusted hardware, as discussed in Section 6, is being actively considered as a secure approach toward protecting against corrupted oracles providing incorrect responses. Trusted hardware, however, may not provide deﬁnitive protection for three reasons. First, it will not be deployed in initial versions of the ChainLink network. Second, some users may not trust trusted hardware (see Appendix B for a discussion). Finally, trusted hardware cannot protect against node downtime, only against node misbehavior. Users will therefore wish to to ensure that they can choose the most reliable oracles and minimize the probability of USER-SC relying on > f faulty oracles. To this end, we propose the use of four key security services: a Validation System, a Reputation System, a Certiﬁcation Service, and a Contract-Upgrade Service. All of these services may initially be run by one company or group interested in launching the ChainLink network, but are designed to operate strictly accordingly to ChainLink’s philosophy of decentralized design. ChainLink’s proposed security services cannot block oracle node participation or alter oracle responses. The ﬁrst three services only provide ratings or guidance to users, while the Contract-Upgrade Service is entirely optional for users. Additionally, these services are designed to support independent providers, whose participation should be encouraged so that users will eventually have multiple security services among which to choose. 5.1 Validation System The ChainLink Validation System monitors on-chain oracle behavior, providing an objective performance metric that can guide user selection of oracles. It will seek to monitor oracles for: 16 • Availability: The Validation System should record failures by an oracle to re- spond in a timely way to queries. It will compile ongoing uptime statistics. • Correctness: The Validation System should record apparent erroneous responses by an oracle as measured by deviations from responses provided by peers.3 In our initial, on-chain aggregation system in ChainLink, such monitoring is straightforward, as all oracle activity is visible to CHAINLINK-SC. Recall, however, that in the oﬀ-chain aggregation system envisaged for ChainLink, it’s the oracles themselves that perform aggregation. Consequently, CHAINLINK-SC does not have direct visibility into oracle responses and cannot itself monitor avail- ability and correctness. Fortunately, oracles digitally sign their responses, and thus, as a side eﬀect, gen- erate non-repudiable evidence of their answers. Our proposed approach will therefore be to realize the validation service as a smart contract that would reward oracles for submitting evidence of deviating responses. In other words, oracles would be incentivized to report apparently erroneous behavior. Availability is somewhat trickier to monitor, as oracles of course don’t sign their failures to respond. Instead, a proposed protocol enhancement would require oracles to digitally sign attestations to the set of responses they have received from other oracles. The validation contract would then accept (and again reward) submission of sets of attestations that demonstrate consistent non-responsiveness by an underper- forming oracle to its peers. In both the on-chain and oﬀ-chain cases, availability and correctness statistics for oracles will be visible on-chain. Users / developers will thus be able to view them in real time through an appropriate front end, such as a Dapp in Ethereum or an equivalent application for a permissioned blockchain. 5.2 Reputation System The Reputation System proposed for ChainLink would record and publish user ratings of oracle providers and nodes, oﬀering a means for users to evaluate oracle performance holistically. Validator System reports are likely to be a major factor in determining oracle reputations and placing these reputations on a ﬁrm footing of trust. Factors beyond on-chain history, though, can provide essential information about oracle node 3“Deviation” must be deﬁned in a data-speciﬁc manner. For simple boolean responses—for ex- ample, whether a ﬂight arrived on time—deviation simply means a response opposite that of the majority. For, say, the temperature of a city, which may vary legitimately across sensors and sources, deviation may mean signiﬁcant numerical deviation. Of course, for various reasons, e.g., broken sen- sors, even a well-functioning oracle may deviate from the majority answer some fraction of the time. 17 security proﬁles. These may include users’ familiarity with oracles’ brands, operating entities, and architectures. We envision the ChainLink Reputation System to include a basic on-chain component where users’ ratings would be available for other smart contracts to reference. Additionally, reputation metrics should be easily accessible oﬀ-chain where larger amounts of data can be eﬃciently processed and more ﬂexibly weighted. For a given oracle operator, the Reputation System is initially proposed as sup- porting the following metrics, both at the granularity of speciﬁc assignment types (see Section 2), and also in general for all types supported by a node: • Total number of assigned requests: The total number of past requests that an oracle has agreed to, both fulﬁlled and unfulﬁlled. • Total number of completed requests: The total number of past requests that an oracle has fulﬁlled. This can be averaged over number of requests assigned to calculate completion rate. • Total number of accepted requests: The total number of requests that have been deemed acceptable by calculating contracts when compared with peer responses. This can be averaged over total assigned or total completed requests to get insight into accuracy rates. • Average time to respond: While it may be necessary to give oracle responses time for conﬁrmation, the timeliness of their responses will be helpful in determin- ing future timeliness. Average response time is calculated based on completed requests. • Amount of penalty payments: If penalty payments were locked in to assure a node operator’s performance, the result would be a ﬁnancial metric of an oracle provider’s commitment not to engage in an “exit scam” attack, where the provider takes users’ money and doesn’t provide services. This metric would involve both a temporal and a ﬁnancial dimension. High-reputation services are strongly incentivized in any market to behave cor- rectly and ensure high availability and performance. Negative user feedback will pose a signiﬁcant risk to brand value, as do the penalties associated with misbehavior. Consequently, we anticipate a virtuous circle in which well-functioning oracles de- velop good reputations and good reputations give rise to incentives for continued high performance. 18 5.3 Certiﬁcation Service While our Validation and Reputation Systems are intended to address a broad range of faulty behaviors by oracles and is proposed as a way to ensure system integrity in the vast majority of cases, ChainLink may also include an additional mechanism called a Certiﬁcation Service. Its goal is to prevent and/or remediate rare but catastrophic events, speciﬁcally en bloc cheating in the form of Sybil and mirroring attacks, which we now explain. Sybil and mirroring attacks. Both our simple and in-contract aggregation proto- cols seek to prevent freeloading in the sense of dishonest nodes copying honest nodes’ answers. But neither protects against Sybil attacks . Such attacks involve an ad- versary that controls multiple, ostensibly independent oracles. This adversary can attempt to dominate the oracle pool, causing more than f oracles to participate in the aggregation protocol and provide false data at strategic times, e.g., in order to inﬂuence large transactions in high-value contracts. Quorums of cheating oracles can also arise not just under the control of a single adversary, but also through collusion among multiple adversaries. Attacks or faults involving > f oracles are especially pernicious in that they are undetectable from on-chain behavior alone. Additionally, to reduce operational costs, a Sybil attacker can adopt a behavior called mirroring, in which it causes oracles to send individual responses based on data obtained from a single data-source query. In other words, misbehaving oracles may share data oﬀ-chain but pretend to source data independently. Mirroring beneﬁts an adversary whether or not it chooses to send false data. It poses a much less serious security threat than data falsiﬁcation, but does slightly degrade security in that it eliminates the error correction resulting from diversiﬁed queries against a given source Src. For example, if https://www.datasource.com emits erroneous data due to, say, a sporadically triggered bug, multiple queriers may still obtain a correct majority result. Sybil attacks resulting in false data, mirroring, and collusion in general may be eliminated by the use of trusted hardware in our long-term strategy (see Section 6). Certiﬁcation Service design. The ChainLink Certiﬁcation Service would seek to provide general integrity and availability assurance, detecting and helping prevent mirroring and colluding oracle quorums in the short-to-medium term. The Certiﬁca- tion Service would issue endorsements of high-quality oracle providers. We emphasize again, as noted above, that the service will only rate providers for the beneﬁt of users. It is not meant to dictate oracle node participation or non-participation in the system. The Certiﬁcation Service supports endorsements based on several features of or- acle deployment and behavior. It would monitor the Validation System statistics 19 on oracles and perform post-hoc spot-checking of on-chain answers—particularly for high-value transactions—comparing them with answers obtained directly from rep- utable data sources. With suﬃcient demand for an oracle provider’s data, we expect there to be enough economic incentive to justify oﬀ-chain audits of oracle providers, conﬁrming compliance with relevant security standards, such as relevant controls in the Cloud Security Alliance (CSA) Cloud Controls Matrix , as well as providing useful security information that they conduct proper audits of oracles’ source and bytecode for their smart contracts. In addition to the reputation metrics, automated on-chain and automated oﬀ- chain systems for fraud detection, the Certiﬁcation Service is planned as a means to identify Sybil attacks and other malfeasance that automated on-chain systems cannot. For example, if all nodes agree that the moon is made of green cheese, they can cause USER-SC to ingest this false fact. MOON COMPONENTS = {GREEN CHEESE} will be recorded on the blockchain, however, and visible in a post-hoc review. 5.4 Contract-Upgrade Service As recent smart contract hacks have shown, coding bulletproof smart contracts is an extremely challenging exercise , , . And even if a smart contract has been correctly programmed, environmental changes or bugs can still result in vulnerabili- ties, e.g., . For this reason, we propose a Contract-Upgrade Service. We emphasize that use of this service is entirely optional and in control of users. In the short term, if vulnerabilities are discovered, the Contract-Upgrade Service would simply make a new set of supporting oracle contracts available in ChainLink. Newly created requesting smart contracts will then be able to migrate to the new set of oracle contracts. Unfortunately, though, existing ones would be stuck with the old, potentially vulnerable set. In the longer term, therefore, CHAINLINK-SC would support a ﬂag (MIGFLAG) in oracle calls from requesting contracts indicating whether or not a call should be forwarded to a new CHAINLINK-SC should one become available. Set by default (i.e., if the ﬂag is missing) to false, MIGFLAG would enable requesting contracts to beneﬁt from automatic forwarding and thus migration to the new version of CHAINLINK-SC. In order to activate forwarding, a user will cause her requesting contract to issue ChainLink requests with MIGFLAG = true. (Users can engineer their smart contracts so that they change this ﬂag upon receiving an instruction to do so on-chain from an authorized contract administrator.) Migration of users to new oracle contracts functions as a kind of “escape hatch,” something long advocated for by blockchain researchers (see, e.g., ) as a mechanism to ﬁx bugs and remediate hacks without resorting to such cumbersome approaches as 20 whitehat hacking or hard forks. Migration to the updated contracts will be visible on the blockchain, and available to audit for users to review before upgrading. We recognize nonetheless that some users will not feel comfortable with any one group controlling an escape hatch in the form of migration / forwarding. Forced migration could empower the migrating contract’s controller, or a hacker who com- promises relevant credentials, to undertake malicious activity, such as changing oracle responses. It is for this reason that requesting contracts have full control of the for- warding feature and can thus opt out of escape-hatch activation. Additionally, in accordance with ChainLink’s focus on decentralization, we expect that providers will be able to support multiple versions of CHAINLINK-SC developed by the community. 5.5 LINK token usage The ChainLink network utilizes the LINK token** to pay ChainLink Node operators for the retrieval of data from oﬀ-chain data feeds, formatting of data into blockchain readable formats, oﬀ-chain computation, and uptime guarantees they provide as op- erators. In order for a smart contract on networks like Ethereum to use a ChainLink node, they will need to pay their chosen ChainLink Node Operator using LINK tokens, with prices being set by the node operator based on demand for the oﬀ-chain resource their ChainLink provides, and the supply of other similar resources. The LINK to- ken is an ERC20 token, with the additional ERC223 “transfer and call” functionality of transfer(address,uint256,bytes), allowing tokens to be received and processed by contracts within a single transaction. 6 Long-Term Technical Strategy The long-term technical strategy for ChainLink proposed in this whitepaper includes three key directions: Oracle conﬁdentiality, infrastructure changes, and oﬀ-chain com- putation. 6.1 Conﬁdentiality A distributed oracle network aims to oﬀer a high degree of protection against faulty oracles. In most deployment scenarios, it seeks to attain a correct response in the face of f Byzantine faults (for f < n/2 in our simple aggregation protocol). Trusted hardware can oﬀer much more and is proposed as a better approach to securing the ChainLink network. Trusted hardware is the keystone of the Town Crier (TC) oracle , which is currently operating on the Ethereum mainnet and whose creators partnered with SmartContract in the TC launch. 21 Certain forms of trusted hardware, most notably Intel’s recent Software Guard eXtensions (SGX) set of instruction-set architecture extensions –, , seek to provide a powerful adjunct to distributed forms of trust. Brieﬂy, SGX permits an application to be executed in an environment called an enclave that claims two critical security properties. First, enclaves protect the integrity of the application, meaning its data, code, and control ﬂow, against subversion by other processes. Second, an enclave protects the conﬁdentiality of an application, meaning that its data, code, and execution state are opaque to other processes. SGX seeks to protect enclaved applications even against a malicious operating system, and thus against even the administrator of the host on which an application is running. While alternative forms of trusted hardware, such as ARM TrustZone, have been in existence for some time, SGX provides an additional key feature lacking in these technologies. It enables a platform to generate an attestation to the execution of a particular application (identiﬁed by a build of its hash state). This attestation can be veriﬁed remotely and allows a speciﬁc application instance to be bound to a public key and thus to establish authenticated and conﬁdential channels with other parties. Running an oracle in an enclave and distributing attestations can provide very strong assurance that the oracle is executing a particular application, speciﬁcally one created or endorsed by developers in the ChainLink ecosystem. Additionally, an oracle running in an enclave that can connect to a data source via HTTPS can provide a strong assurance that the data it retrieves has not been tampered with. (See , for details.) These properties go a long way toward protecting against oracle misbehavior in the sense of data corruption, Sybil attacks, etc. A still greater opportunity, however, lies in the ability of trusted hardware to provide strong conﬁdentiality. The need for conﬁdentiality is in general one of the main hurdles to blockchain deployment. Conﬁdentiality-preserving oracles can be instrumental in solving the problem. Why distributed oracles don’t ensure conﬁdentiality. Conﬁdentiality is fun- damentally hard to achieve in any oracle system. If an oracle has a blockchain front end such as a smart contract, then any queries to the oracle will be publicly vis- ible. Queries can be encrypted on-chain and decrypted by the oracle service, but then the oracle service itself will see them. Even heavyweight tools such as secure multiparty computation, which permits computation over encrypted data, can’t solve this problem given existing infrastructure. (See, e.g., for an application-oriented perspective.) At some point a server needs to send a query to a target data source server. Thus it must see the query, irrespective of whatever conﬁdentiality the query previously enjoyed. It will also see the response to the query. 22 Conﬁdentality-preserving oracles via SGX. An oracle using SGX can ingest and process data within an enclave, in essence acting like a TTP trusted for integrity and conﬁdentiality. To begin with, such an oracle can decrypt queries within its enclave. It can then process them without exposing them to any other process (or any human being). The enclave can also process data from sources conﬁdentially and can securely manage sensitive information such as user credentials, a powerful capability, as we illustrate below. The Town Crier system supports conﬁdential ﬂight data queries. Flight infor- mation can be passed to a TC smart contract front end encrypted under the public key of the TC service. TC decrypts the query and then contacts a data source (e.g., ﬂightaware.com) over HTTPS. It returns to the querying smart contract a simple yes/no answer to the question “Has this ﬂight been delayed?” and exposes no other information on-chain. An even more interesting TC capability is its support for trading on the Steam gaming platform. TC can securely ingest user credentials (passwords) to check that game ownership has been transferred from a buyer to a seller. It can thereby create a secure marketplace that would be otherwise unachievable, with high assurance fair swaps of cryptocurrency for digital goods. (A simple distributed oracle, in contrast, could not securely manage users’ passwords on their behalf.) TC can also perform trusted oﬀ-chain aggregation of data from multiple sources, as well as trusted computation over data from multiple sources (e.g., averaging) and interactive querying of data sources (e.g., searching the database of one source in response to the answer of another). Trusted hardware oﬀers an exciting new approach to the scalable usage of blockchains , , in which large portions of blockchain infrastructure, including smart contracts, execute in enclaves. Such an architecture would combine transparency beneﬁts of blockchains with the conﬁdentiality properties of oﬀ-chain execution and trusted hardware. While similar ideas have been suggested using other techniques, such as zk-SNARKs , trusted hardware is far more practical (and less complicated). Our current research agenda includes this expansive vision, with oracles as a catalyzing service. We brieﬂy discuss the issue of Intel as a root of trust in Appendix B. Deﬁning security given SGX. It is possible, given the use of trusted hardware, to deﬁne oracle correctness more formally, starting with the formalism for Intel SGX proposed in . This formalism allows SGX to be treated as a global Universally Composable (UC) functionality Fsgx(Σsgx)[progencl, R]. Here, and in what follows, Σ denotes a signature scheme with signing and veriﬁcation functions Σ.Sign and Σ.Verify. An instance of Fsgx(Σsgx)[progencl, R] is parameterized by a group signature 23 scheme Σsgx. Argument progencl denotes the program running in an enclave, i.e., environment protected by the hardware. R denotes the untrusted code running on an SGX host, i.e., the software that calls the application running in the enclave. Figure 5 (taken from ) shows the operation of the functionality Fsgx. Upon ini- tialization, it runs outp := progencl.Initialize(), generating and attestation to the code of progencl and outp. An attestation σatt is a digitally signed statement by the platform that progencl is running in an enclave and has yielded output outp. In typical usage, progencl.Initalize() outputs an instance-speciﬁc public key that can be used to create a secure channel to the application instance. Upon a resume call with (id, params), Fsgx continues execution and outputs the result of progencl.Resume(id, params), where id denotes a session identiﬁer and params denotes parameters input to progencl. Fsgx[progencl, R]: abstraction for SGX Hardcoded: sksgx (private key for Σsgx) Assume: progencl has entry points Initialize and Resume Initialize: On receive (init) from R: Let outp := progencl.Initalize() σatt := Σsgx.Sign(sksgx, (progencl, outp)) Output (outp, σatt) Resume: On receive (resume, id, params) from R: Let outp := progencl.Resume(id, params) Output outp Figure 5: Formal abstraction for SGX execution capturing a subset of SGX features.. Given the formalism of Figure 5, it is possible to precisely deﬁne the integrity of an oracle. Deﬁnition 1 does so in a slight generalization of the deﬁnition given in that we call Authenticity of Oracle. Deﬁnition 1 (Authenticity of Oracle). We say that an oracle O running program progencl using Fsgx and outputting instance key pkO satisﬁes Authenticity of Oracle if, for any polynomial-time adversary A that can interact arbitrarily with Fsgx, A cannot cause an honest veriﬁer to accept (pkO, σatt, params := (url, , T), data, σ) where data is not the contents of url with the public key at time T (progencl.Resume(id, params) in 24 our model). More formally, for any probabilistic polynomial-time adversary A, Pr \\uf8ee \\uf8ef\\uf8ef\\uf8f0 (pkO, σatt, id, params, data, σ) ←AFsgx(1λ) : \\x00Σsgx.Verify(pksgx, σatt, (progencl, pkO)) = 1 \\x01 ∧ \\x00Σ.Verify(pkO, σ, (id, params, data)) = 1 \\x01 ∧ data ̸= progencl.Resume(id, params) \\uf8f9 \\uf8fa\\uf8fa\\uf8fb ≤negl(λ), for security parameter λ. 6.2 Infrastructure changes Many of the challenges in constructing secure oracles arise from the fact that existing data sources don’t digitally sign the data they serve. If they did, then oracles would not need to be trusted to refrain from tampering with data. HTTPS, the protocol for secure web communications, does not enable data signing. It does, though, have an underlying public-key infrastructure (PKI) that requires servers to possess certiﬁcates that could in principle support data signing. This observation is the basis of TLS-N, a TLS extension, that allows HTTPS servers to sign portions of their sessions with clients. The selective nature of the signing provides other nice features, such as the ability for clients to exclude from signed transcripts and thus protect the conﬁdentiality of credentials (e.g., passwords) they use to connect to servers. We believe infrastructure changes such as TLS-N are promising approaches to supporting oracle security. They will probably need to be used in concert with other technologies such as SGX, however, because of the following limitations: 1. Infrastructure modiﬁcations: Unfortunately, until and unless TLS-N becomes a standard, data sources must expressly deploy it for clients to beneﬁt. Few data sources are likely to in the near future. 2. Aggregation and computation: TLS-N cannot support aggregation or other forms of trusted computation over data from data sources, so some trusted mechanism will still be required to accomplish these tasks. 3. Cost: Veriﬁcation of TLS-N-signed data incurs relatively high on-chain costs compared with simple signature veriﬁcation. 4. Conﬁdentiality: TLS-N cannot support out-of-band conﬁdential management of user credentials or queries, but instead requires users to query a data source themselves for this purpose. For example, conﬁdential ﬂight information cannot be stored in a smart contract for later conﬁdential automated query of a website. 25 6.3 Oﬀ-chain computation Some intriguing uses of oracles, such as the use of credential-dependent APIs, require that an oracle do considerably more than just transmit data. It may need to manage credentials, log into accounts to scrape data, and so forth. Indeed, given truly trust- worthy and conﬁdential oracles, something that SGX-backed systems à la Town Crier and techniques such as zero-knowledge proofs can help achieve, the boundary between oracles and smart contracts may become ﬂuid. ChainLink already supports a regex-based language for queries that enables users to ﬂexibly specify the processing of oﬀ-chain data. Our long-term strategy, however, seeks to create a world where oracles are a key oﬀ-chain computation resource used by most smart contracts. We believe this will be enabled by building towards a model of fully general, private oﬀ-chain computation within oracles whose results are consumed by smart contracts. If this can be achieved with strong security, as we believe it can, pushing costly and sensitive computation logic into oracles will result in better conﬁdentiality, lower contract execution costs, and more ﬂexible architectures. 7 Existing Oracle Solutions ChainLink is designed to ﬁll a pervasive need for new oracle technology in smart contract systems. Unfortunately today there is a very limited supply of highly secure and ﬂexible oracle systems. We believe this lack of trustworthy oracles is a major impediment to the evolution of smart contracts. The most commonly used option for oracle services today are centralized oracle providers. This approach is problematic as it creates a centralized point of control, and thus does not meet the high standards of tamper resistance that trustless smart contracts require. Some such systems, e.g. , attempt to remedy this problem by relying on notarization, to “prove” correct behavior. This use of notarization services is worrisome in view of documented problems with these services , and the fact that their attestations cannot be feasibly veriﬁed on-chain, resulting in a (potentially recursive) need for further veriﬁcation. Another approach to delivering trustworthy oracle data is to rely on manual human input of unstructured data. These “manual-input oracle” are commonly proposed for use in prediction-markets , , . By creating appropriate ﬁnancial stakes and assuming economically rational players with limited ﬁnancial incentives for cheating, such oracles provide a high assurance of correct crowd-sourced answers. This approach is decentralized and ﬂexible. Since manual-input oracles obtain their responses from human beings, they can respond to questions for which structured data is hard to ﬁnd, or diﬃcult to extract in a reliable way, e.g., requires natural language processing 26 of news events. Unfortunately, though, because human cognition is costly and slow, manual-input oracles are resource-intensive, not real-time, and can handle only a limited set of questions at any given time. We believe that ChainLink could also be very useful for quickly and automatically resolving prediction-market contracts that can be resolved by structured data. A ﬁnal approach is to change the form of data at the source. If a data source digi- tally signed the data it provided, then the relaying server wouldn’t need to be trusted. USER-SC could simply check the signatures on data it receives. An excellent, general approach of this kind is provided by TLS-N, as discussed above. Unfortunately, as already mentioned, TLS-N requires changes to existing infrastructure. 8 Conclusion We have introduced ChainLink, a decentralized oracle network for smart contracts to securely interact with resources external to the blockchain. We have outlined the ChainLink architecture, describing both on and oﬀ-chain components. After deﬁning security in the context of oracles, we described ChainLink’s multilayered approach to decentralization. We proposed a novel protocol with new features such as protection against freeloading (with additional protocols and security-proof sketches in the paper appendix). We also laid out a roadmap for how ChainLink can harness technological and infrastructural advances, such as trusted hardware and digital signing of data by sources. Finally, having examined existing oracle solutions and their shortcomings, we have exposed the need today for a system such as ChainLink. Design Principles. As we continue our work on ChainLink, we will seek to prior- itize the following core values: • Decentralization for secure and open systems. Decentralization is not only the foundation of the tamperproof properties of blockchains, but the basis of their permissionless nature. By continuing to build decentralized systems, we aim to further enable permissionless development within the ecosystem. We believe that decentralization is a crucial component for a globally thriving ecosystem with long-term sustainability. • Modularity for simple, ﬂexible system design. We appreciate the philosophy of building small tools which do one thing well. Simple components can be easily reasoned about and thus securely combined into larger systems. We believe that modularity not only enables upgradable systems, but facilitates decentral- ization. Wherever key pieces of ChainLink depend on or are managed by too 27 few parties, we will seek to design an ecosystem which allows for competing implementations to be used. • Open source for secure, extensible systems. ChainLink is made possible by standing on the shoulders of many open source projects. We value the commu- nity and will continue to contribute by developing ChainLink in an open source manner. We plan to engage continually with developers, academics, and secu- rity experts for peer review. We encourage testing, audits, and formal proofs of security, all with the aim of creating a platform whose robustness and security can support future innovations. With these principles in mind, we look forward to extending the reach and impact of blockchains and smart contracts by making oracles a secure cornerstone of the ecosystem. References Parity. The Multi-sig hack: A postmortem. https://blog.ethcore.io/the-multi- sig-hack-a-postmortem/. 20 July 2017. Gun Sirer. Cross-Chain Replay Attacks. Hacking, Distributed blog. 17 July 2016. Adi Shamir. “How to share a secret”. In: Communications of the ACM 22.11 (1979), pp. 612–613. Claus-Peter Schnorr. “Eﬃcient signature generation by smart cards”. In: Journal of cryptology 4.3 (1991), pp. 161–174. Rosario Gennaro, Stanislaw Jarecki, Hugo Krawczyk, et al. “Secure distributed key generation for discrete-log based cryptosystems”. In: Eurocrypt. Vol. 99. Springer. 1999, pp. 295–310. R. Canetti. “Universally Composable Security: A New Paradigm for Crypto- graphic Protocols”. In: FOCS. 2001. Ran Canetti. “Universally composable security: A new paradigm for crypto- graphic protocols”. In: Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on. IEEE. 2001, pp. 136–145. Douglas R Stinson and Reto Strobl. “Provably secure distributed Schnorr signa- tures and a (t, n) threshold scheme for implicit certiﬁcates”. In: ACISP. Vol. 1. Springer. 2001, pp. 417–434. John R Douceur. “The sybil attack”. In: International Workshop on Peer-to- Peer Systems. Springer. 2002, pp. 251–260. 28 Aniket Kate and Ian Goldberg. “Distributed key generation for the internet”. In: Distributed Computing Systems, 2009. ICDCS’09. 29th IEEE International Conference on. IEEE. 2009, pp. 119–128. Claudio Orlandi. “Is multiparty computation any good in practice?” In: Acous- tics, Speech and Signal Processing (ICASSP), 2011 IEEE International Con- ference on. IEEE. 2011, pp. 5848–5851. Ittai Anati, Shay Gueron, Simon Johnson, et al. “Innovative technology for CPU based attestation and sealing”. In: Proceedings of the 2nd International Workshop on Hardware and Architectural Support for Security and Privacy. Vol. 13. 2013. url: https : / / software . intel . com / en - us / articles / innovative-technology-for-cpu-based-attestation-and-sealing (vis- ited on 05/23/2016). Matthew Hoekstra, Reshma Lal, Pradeep Pappachan, et al. “Using Innovative Instructions to Create Trustworthy Software Solutions”. In: Proceedings of the 2Nd International Workshop on Hardware and Architectural Support for Se- curity and Privacy. HASP ’13. Tel-Aviv, Israel: ACM, 2013, 11:1–11:1. isbn: 978-1-4503-2118-1. doi: 10.1145/2487726.2488370. url: http://doi.acm. org/10.1145/2487726.2488370. Frank McKeen, Ilya Alexandrovich, Alex Berenzon, et al. “Innovative instruc- tions and software model for isolated execution.” In: Proceedings of the 2nd International Workshop on Hardware and Architectural Support for Security and Privacy. 2013, p. 10. url: http://css.csail.mit.edu/6.858/2015/ readings/intel-sgx.pdf (visited on 05/23/2016). Intel. Intel Software Guard Extensions Programming Reference. 2014. (Visited on 05/23/2016). Gavin Wood. “Ethereum: A secure decentralised generalised transaction ledger”. In: Ethereum Project Yellow Paper (2014). Jack Peterson and Joseph Krug. “Augur: a decentralized, open-source platform for prediction markets”. In: arXiv preprint arXiv:1501.01042 (2015). Victor Costan and Srinivas Devadas. “Intel SGX Explained”. In: Cryptology ePrint Archive (2016). url: https : / / eprint . iacr . org / 2016 / 086 . pdf (visited on 05/24/2016). Victor Costan, Ilia A Lebedev, and Srinivas Devadas. “Sanctum: Minimal Hard- ware Extensions for Strong Software Isolation.” In: USENIX Security Sympo- sium. 2016, pp. 857–874. 29 Kevin Delmolino, Mitchell Arnett, Ahmed Kosba, et al. “Step by step towards creating a safe smart contract: Lessons and insights from a cryptocurrency lab”. In: International Conference on Financial Cryptography and Data Security. Springer. 2016, pp. 79–94. Ahmed Kosba, Andrew Miller, Elaine Shi, et al. “Hawk: The blockchain model of cryptography and privacy-preserving smart contracts”. In: S&P’16. IEEE. 2016. Loi Luu, Duc-Hiep Chu, Hrishi Olickel, et al. “Making smart contracts smarter”. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Com- munications Security. ACM. 2016, pp. 254–269. Bill Marino and Ari Juels. “Setting standards for altering and undoing smart contracts”. In: International Symposium on Rules and Rule Markup Languages for the Semantic Web. Springer. 2016, pp. 151–166. Fan Zhang, Ethan Cecchetti, Kyle Croman, et al. “Town Crier: An authenti- cated data feed for smart contracts”. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM. 2016, pp. 270– 282. Augur project page. https://augur.net. 2017. CSA Cloud Controls Matrix. URL: https://cloudsecurityalliance.org/group/cloud- controls-matrix. 2017. Mark Flood and Oliver Goodenough. Contract as Automaton: The Computa- tional Representation of Financial Agreements. https://www.financialresearch. gov/working-papers/files/OFRwp-2015-04_Contract-as-Automaton- The-Computational-Representation-of-Financial-Agreements.pdf. Of- ﬁce of Financial Research, 2017. Gnosis project page. https://gnosis.pm. 2017. Hyperledger Sawtooth. https://intelledger.github.io/introduction.html. 2017. Abhiram Kothapalli, Andrew Miller, and Nikita Borisov. “SmartCast: An In- centive Compatible Consensus Protocol Using Smart Contracts”. In: Financial Cryptography and Data Security (FC). 2017. Oraclize project page. http://www.oraclize.it. 2017. Rafael Pass, Elaine Shi, and Florian Tramer. “Formal abstractions for attested execution secure processors”. In: Eurocrypt. Springer. 2017, pp. 260–289. Town Crier Ethereum service. http://www.town-crier.org/. 2017. 30 Florian Tramer, Fan Zhang, Huang Lin, et al. “Sealed-glass proofs: Using trans- parent enclaves to prove and sell knowledge”. In: Security and Privacy (Eu- roS&amp;P), 2017 IEEE European Symposium on. IEEE. 2017, pp. 19–34. Vitalik Buterin et al. Ethereum white paper. https://github.com/ethereum/ wiki/wiki/White-Paper. JSON Schema. http://json-schema.org/. Hubert Ritzdorf, Karl Wüst, Arthur Gervais, et al. TLS-N: Non-repudiation over TLS Enabling Ubiquitous Content Signing for Disintermediation. IACR ePrint report 2017/578. URL: https://eprint.iacr.org/2017/578. 31 Disclosures † Ari Juels is a faculty member at the Jacobs Institute at Cornell Tech. He co-authored this work in his separate capacity as an advisor to SmartContract ChainLink Ltd., in which he has a ﬁnancial interest. * This whitepaper is being provided by SmartContract ChainLink, Ltd. (“SCCL”), a British Virgin Islands corporation, in support of the ChainLink Platform. Secure Asset Exchange, Inc. (“SAE”) dba SmartContract.com, is providing administrative, technical and other support to SCCL, including support of SCCL’s LINK Token sale, and is receiving compensation from SCCL. The technological, social and business structures utilizing blockchain technology are continually developing and will evolve for the foreseeable future. Accordingly, the plans, strategies and implementation de- tails described in this whitepaper will likely evolve as well and, accordingly, may never be adopted. SCCL and SAE reserve all rights to develop or pursue additional or al- ternative plans, strategies or implementation details associated with the ChainLink Platform. ** LINK Tokens are being sold by SCCL pursuant to the Terms and Conditions of the token sale terms available at https://link.smartcontract.com/terms. For complete details, review the terms. LINK Tokens are not securities, investments or currency, and are not sold or marketed as such. Also: participating in the sale involves signiﬁ- cant technological and systemic risks; the sale is not open to individuals who reside in or are citizens of the United States or Canada. The sale period, duration, pricing, and other provisions may change, as stated in the token sale terms. The LINK To- ken sale involves known and unknown risks, uncertainties, and other factors that may cause the actual functionality, utility, or levels of use of LINK Tokens to be materially diﬀerent from any projected future results, use, functionality or utility expressed or implied by SCCL in the terms. 32 A Oﬀ-Chain Aggregation To ensure both valid, signed answers and to prevent freeloading, our oﬀ-chain aggre- gation protocol, discussed in Section 4.2, will rely on a simple distributed protocol based on threshold signatures . The beneﬁt of this approach is that for a given query, a single signature can be generated oﬀ-chain by a collection of n oracle nodes. As a result, only a single authenticated message needs to be handled on-chain, instead of O(n) messages from distinct oracle nodes. This approach greatly reduces costs by comparison with those incurred by Algorithm 1. The idea can be further extended, as in , to aggregate the answers to multiple queries within a single threshold signature, an idea we don’t explore here but may consider in our architecture. Suppose that f < n/3 oracles are faulty and t = f + 1. Faulty nodes may try to perform freeloading and/or any of a range of other dishonest behaviors, such as signing invalid answers. Our complete protocol for oﬀ-chain aggregation consists of a pair of algorithms OCA = (DistOracle, RewardOracles) for computing a signature Sigsk[A] on value A = Agg(A1, A2, . . . , An), for the majority function Agg. A simpliﬁed, single-round version of these protocols is shown respectively in Algorithms 2 and 3. The ﬁrst is executed by the participating oracles, while the second is executed by an entity PROVIDER that may, as mentioned above, take the form of a smart contract. Before presenting OCA, we give some basic background on Schnorr signatures and the threshold scheme for computing them given in . Schnorr signatures. The Schnorr signature scheme makes use of a group G of prime order p with generator g for which the discrete log problem is presumed to be hard. A user’s key pair takes the form (sk, pk) = (x, y = gx) for x ←$ Z× p , where Z× p = Zp −0. Without loss of generality, we denote group operations multiplicatively. Schnorr signatures may be computed over elliptic curve groups, and indeed typically are in modern crypto implementations. Figure 6 shows the Schnorr signature scheme. Threshold Schnorr signature scheme. We make use of the threshold signature scheme of . This scheme generates a global private / public keypair (sk, pk). It enables threshold generation of a full signature Sigsk[m] for a desired message m. In the initial key generation protocol, each player is assigned a key share xi = ski. This setup is a one-time operation and can be done using a distributed key generation protocol, e.g., or, for an asynchronous setting, . To generate a signature, players (oracles in our setting) ﬁrst perform a distributed key generation protocol, as for key-share generation at setup. The output of this protocol is a global ephemeral secret key e. Each player (Oi) receives a (secret) share ei of e. 33 Schnorr signature: Signer input (m, sk = x); Veriﬁer input pk = y Signer Veriﬁer r ←$ Zx p e ←gr c = H(m || e) s = cx + e (e,s),m −−−−→ c = H(m || e) gs ?= ryc Figure 6: Schnorr signature scheme. A partial signature of Oi given ephemeral key e takes the form σ(e) i = cxi + ei, where c = H(m || e), as in the full signature. For each player Oi there also exists a function validi(σi; (pk, e)) that veriﬁes its partial signature. We refer to a partial signature as valid for Oi if it veriﬁes correctly under validi. We have somewhat modiﬁed notation and greatly condensed the scheme for our exposition here. We refer the reader to for details. A.1 OCA protocol We now present the DistOracle and RewardOracles algorithms that compose OCA. These are speciﬁed below in Algorithms 2 and 3. Using condensed notation (rather than including witnesses explicitly, as in Algorithm 1) we let Commit denote a com- mitment function. Note that all players can see the messages received by CHAINLINK-SC, as it is on- chain. Let Σ∗be the ﬁrst valid signature Σ sent to CHAINLINK-SC. In Alg. 3, we let PS∗denote a set of decommitments received by PROVIDER whose partial signatures yield Σ∗. (PS∗could come from the oracle that sent Σ∗, but need not. Any oracle with a partial signature in PS∗is incentivized to send PS∗.) 34 Algorithm 2 DistOracle(f, n, i, ski = xi, pk, Src) (code for Oi) Jointly generate ephemeral key: 1: Execute distributed key-generation protocol and receive (ei, e). Obtain data: 2: Obtain Ai from Src. Generate partial signature: 3: Compute σ(e) i (= cxi + ei, for c = H(m || e), where m = Ai). Commit round: 4: Broadcast commitment commi = \\x00Commit(σ(e) i , Ai); i \\x01 . 5: Wait until a set Ci of n −f valid commitments from distinct oracles is received. 6: Send Ci to PROVIDER. Prepare round: 7: Broadcast prepared. 8: Wait until n −f distinct prepared messages are received. Reveal / decommit round: 9: Broadcast decommitment of (σ(e) i , Ai) for commi. 10: Wait until a set PS of ≥t valid decommitments are received. Full signature computation: 11: if no valid Σ has yet been received by CHAINLINK-SC then 12: Aggregate partial signatures in PS into Σ = Sigsk[A]. 13: Send Σ to CHAINLINK-SC. 14: Send PS to PROVIDER. 15: end if 35 Algorithm 3 RewardOracles (code for PROVIDER) 1: Wait until set C of n −f commitment sets (Ci) from distinct oracles and PS∗are received. 2: for every oracle Oj do 3: if σj ∈PS∗and > 2f sets in C include commitments to σj from Oj then 4: Send $reward to Oj. 5: end if 6: end for A.2 Proof sketches We oﬀer proof sketches of the key properties of OCA. We show that assuming at most f faulty nodes, the protocol always generates a valid signature on a correct answer, and never rewards freeloading oracle nodes. Claim 2. The protocol OCA will never reward a freeloading node. Proof. (Sketch) Suppose that Oz is freeloading. Then it can broadcast a valid com- mitment only after time τ, the time at which the ﬁrst honest Oi decommits in Step 9 of Alg. 2. Oi has received n −f prepared messages of which at least n −2f come from honest nodes. Let Oj denote one of these at least n −2f honest nodes. Oj will no longer accept commitments after sending a prepared message, so the set Cj of any such honest node Oj will no longer change after time τ, and thus Cj will ex- clude a commitment with a correct partial signature σz from Oz. Therefore, at most n −(n −2f) = 2f sets in C in Alg. 3 will include Oz. Thus Oz will not receive a reward. Unfortunately, OCA cannot ensure that non-freeloading nodes are paid. A cheat- ing adversary can, after receiving a decommitment, rush honest nodes in Step 13 of Alg. 2 by generating its own partial signatures and including only one honest node’s partial signature in the generation of Σ. That honest node’s commitment may not have been included among the n −f collected by any node. It could have come afterward. Claim 3. OCA will always result in a valid signature Σ = Sigsk[A] eventually being sent to CHAINLINK-SC. Proof. (Sketch) There are n−f honest nodes, and f < n/3, so there are > 2f ≥f +1 honest nodes, and thus at least t honest nodes. So Step 1 of Alg. 2 will complete successfully. Similarly, since there are n−f honest nodes, every oracle will eventually complete Step 7 of Alg. 2, sending a prepared message. Honest nodes will eventually then 36 receive at least n −f prepared messages and will decommit, permitting Step 13 to be completed by some honest node. Claim 4. Any valid signature Sigsk[A] received by CHAINLINK-SC in oca will be on a valid value A. Proof. (Sketch) It is easy to see that a valid signature Sigsk[A] includes a correct value A. As t partial signatures are needed to compute Sigsk[A], and at most f < t nodes are faulty, at least one partial signature on A was provided by an honest node and thus it must be correct. A.3 Discussion OCA introduces a few design challenges that we brieﬂy discuss here. Payment for honest nodes. Unfortunately, while it penalizes freeloaders with non-payment, OCA is not able to guarantee conversely that honest nodes are paid. Indeed, even in the benign case where no nodes are faulty, unlucky message ordering can result in honest nodes that have contributed partial signatures to Σ not receiving payment. This problem could be addressed in part by making Alg. 2 synchronous. Speciﬁ- cally, “Wait” steps could require that nodes wait a period of time ∆such that receipt of messages from honest nodes is guaranteed. In this case, all nodes with partial signatures incorporated into Σ would be guaranteed payment. Side eﬀects, however, would be slower execution and the challenge of setting ∆correctly. The problem of designing an asynchronous protocol with strong payment guaran- tees is an open research problem that we are currently exploring. Redundant messages. OCA aims to minimize on-chain communication and ide- ally involves just one on-chain message, namely a signed response Σ, from the set of participating oracles. In practice, however, because a signature Σ will not post imme- diately to the blockchain, multiple oracles could independently send signed responses to the blockchain. The best way to limit such redundant messages is for oracles to monitor not just the blockchain, but the mempool, i.e., pending messages. Key management. Of course, as is often the case, key management is a major challenge in protocols of this kind. The distribution of shares of sk can be performed in a distributed manner and updates can be made to accommodate new nodes and remove departing nodes, as well as to provide proactive security, i.e., ongoing resilience against compromise of nodes’ keys. Additionally, nodes can be organized 37 into distinct cliques to bound the size of n. We propose that ChainLink employ these techniques to ensure a ﬂexible, responsive, and secure distributed oracle. B SGX Trust Assumptions In Intel’s role of providing stronger assurance of correctness, SGX enhances but does not supplant other integrity protections in ChainLink. In other words, use of SGX makes the system strictly stronger. Trusting SGX for conﬁdentiality does require trust in Intel, but this trust is cir- cumscribed. Assuming that Intel does not have a backdoor in its CPUs enabling leakage of enclave data, it does not have a means of inspecting enclave state. (Such a backdoor is possible, but which would require the presence of physical evidence on every user’s machine and pose a serious reputational risk.) Intel or an adversary that corrupts Intel’s manufacturing processes could in prin- ciple falsify attestation keys (platform EPID keys). Such an adversary could generate EPID keys that are not embedded in SGX-enabled servers, but instead permit attes- tations to be generated in a non-SGX platform. In eﬀect, this adversary could create bogus servers that generate valid-looking SGX attestations, but provide no protec- tions for enclaved code. Should there be more than f such nodes, of course, the adversary could corrupt oracle responses. More problematically, such nodes would expose sensitive data handled by oracle nodes to the adversary. The ability to fal- sify EPID keys, however, does not imply an ability to corrupt existing, valid SGX instances. It is also important to recognize that of course today, whether or not we like it, trust in Intel is inescapable. The CPU in the machine on which you are reading this paper bears witness to this fact—or, if not, the CPU in the server from which you downloaded this paper. Of course, it would be preferable to make use of trusted hardware from multiple vendors, and it is to be hoped that others will create equivalent capabilities. New, open architectures for trusted hardware, and ways to weaken the trust assumptions required of such hardware, are active areas of research, e.g., , . The ability to diversify across vendors or architectures per se would not ensure data conﬁdentiality, however. We are also interested in research into techniques for conﬁdentiality assurance in distributed networks through the use of cover traﬃc. 38'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jId0d5yPNgmh",
        "outputId": "bfcec30f-508e-4bb3-d1f5-64527770e398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.12)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPDw2JqaQyL7",
        "outputId": "94ec35b9-4563-495e-d015-3bf8fd456c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.41.2)\n",
            "Collecting transformers\n",
            "  Using cached transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Using cached huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.21.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: typer>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (0.1.2)\n",
            "Using cached transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
            "Using cached huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
            "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 0.36.2\n",
            "    Uninstalling huggingface_hub-0.36.2:\n",
            "      Successfully uninstalled huggingface_hub-0.36.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.1 which is incompatible.\n",
            "sentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 5.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.4.1 tokenizers-0.22.2 transformers-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_documents(processed_docs, chunk_size=512, chunk_overlap=80):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunked_library = {}\n",
        "    for doc_name, text in processed_docs.items():\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        chunked_library[doc_name] = chunks\n",
        "        print(f\"{doc_name}: {len(chunks)} chunks\")\n",
        "\n",
        "    return chunked_library\n",
        "\n",
        "final_chunks = chunk_documents(processed_docs)\n",
        "\n",
        "# Quick check\n",
        "if \"uniswap.pdf\" in final_chunks:\n",
        "    print(f\"\\nFirst Uniswap chunk length: {len(final_chunks['uniswap.pdf'][0])} chars\")"
      ],
      "metadata": {
        "id": "IIUhUVLHsjOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce5b7ee-2503-4bbf-9674-bc92e71cab08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bitcoin.pdf: 51 chunks\n",
            "solana.pdf: 109 chunks\n",
            "uniswap.pdf: 39 chunks\n",
            "chainlink.pdf: 203 chunks\n",
            "\n",
            "First Uniswap chunk length: 442 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding metatdata to the corpus along with additional checks to verify the chunking\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "def prepare_corpus(\n",
        "    chunked_docs: Dict[str, List[str]]\n",
        ") -> Tuple[List[str], List[str]]:\n",
        "# returns a tuple for embedding and retrieval by using flatten chunks\n",
        "    texts: List[str] = []\n",
        "    sources: List[str] = []\n",
        "\n",
        "    for doc_name, chunks in chunked_docs.items():\n",
        "        for chunk in chunks:\n",
        "            texts.append(chunk.strip()) # ensures no leading/trailing whitespace\n",
        "            sources.append(doc_name)\n",
        "\n",
        "    print(f\"Prepared corpus:\")\n",
        "    print(f\"  → {len(texts):,} chunks\")\n",
        "    print(f\"  → Documents: {sorted(set(sources))}\")\n",
        "    print(f\"  → Avg chunk length: {sum(len(t) for t in texts) / len(texts):.0f} chars\")\n",
        "\n",
        "    if not texts:\n",
        "        raise ValueError(\"No chunks found – check chunk_documents step\")\n",
        "\n",
        "    return texts, sources\n",
        "\n",
        "texts, sources = prepare_corpus(final_chunks)"
      ],
      "metadata": {
        "id": "6LFcTCYYsjRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bde2696-0e90-4424-9dc2-6a6ccdb58426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared corpus:\n",
            "  → 402 chunks\n",
            "  → Documents: ['bitcoin.pdf', 'chainlink.pdf', 'solana.pdf', 'uniswap.pdf']\n",
            "  → Avg chunk length: 438 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "uxDNtIJm2raN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "print(f\"Loading {model_name} on CPU...\")\n",
        "embedder = SentenceTransformer(model_name, device=\"cpu\")\n",
        "print(f\"Success! Dimension: {embedder.get_sentence_embedding_dimension()}\")"
      ],
      "metadata": {
        "id": "MrQTe28f2nTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb60546-f1b8-4722-8535-7114c5d31a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BAAI/bge-small-en-v1.5 on CPU...\n",
            "Success! Dimension: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# generating normalised embeddings for sentences along with safe batching\n",
        "# model used - bge-small-en=v1.5 sentence transformer (defined in the above cell)\n",
        "# normalize: L2-normalize (required for cosine sim with IndexFlatIP)\n",
        "# returns a numpy array\n",
        "def compute_embeddings(\n",
        "    texts: List[str],\n",
        "    model: SentenceTransformer,\n",
        "    batch_size: int = 16,\n",
        "    normalize: bool = True\n",
        ") -> np.ndarray:\n",
        "    print(f\"Computing embeddings ({len(texts):,} texts) ...\")\n",
        "\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_emb = model.encode(\n",
        "            batch,\n",
        "            batch_size=len(batch),\n",
        "            show_progress_bar=False,\n",
        "            normalize_embeddings=normalize,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "        embeddings_list.append(batch_emb)\n",
        "\n",
        "    embeddings = np.vstack(embeddings_list).astype(np.float32)\n",
        "\n",
        "    print(f\"Embeddings ready → shape {embeddings.shape}\")\n",
        "    print(f\"  → Norm of first vector: {np.linalg.norm(embeddings[0]):.4f} (should ≈1.0)\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "embeddings = compute_embeddings(texts, embedder, batch_size=16)"
      ],
      "metadata": {
        "id": "_dyxu1Vl2nV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "75bdf5d344a6454aacff4f595a7e777b",
            "0fac1ddaf0264dc7b2656ad1e6b0d5e1",
            "8dc5c179189c434baac77a1ff8a45f59",
            "03ff41f7faec434b94f5a973261c0398",
            "a7b1634488c342ff905c5be049e7eeec",
            "6cd1bdf721de4535a61d9f03b8aec3b8",
            "e2b5038b48a94296ac6ed0b2054206a5",
            "66556401a47d4c55ad2d68f23fc84633",
            "48c6492cae2243d593b494b7843e9adb",
            "4c367cd7e5504c9891fab88c72de5eee",
            "8308e633c6d448c59df226ed058b17b3"
          ]
        },
        "outputId": "142905e5-88a7-47b6-eeb5-0bc361c89894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing embeddings (402 texts) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Embedding batches:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75bdf5d344a6454aacff4f595a7e777b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings ready → shape (402, 384)\n",
            "  → Norm of first vector: 1.0000 (should ≈1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#have to reduce numpy from v 2. to v 1.0 as faiss cpu is compatible with a lower version\n",
        "!pip uninstall -y numpy\n",
        "!pip install \"numpy<2\" --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm4ENunA33dF",
        "outputId": "4ad13c58-e3d6-4b7f-b91f-ef7355aec63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 5.2.0 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "# restart runtime again to check if the numpy version has updated"
      ],
      "metadata": {
        "id": "T6UfYoUF4Ls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")      # Should be 1.26.x\n",
        "print(f\"faiss version: {faiss.__version__}\")   # Should show without crash\n",
        "print(\"faiss import successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDHaWDug2nYw",
        "outputId": "38f915ca-5ca8-4691-dc15-2f9e9da15bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "faiss version: 1.8.0\n",
            "faiss import successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# The embeddings variable should already exist from the previous cell\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension) # Inner Product = cosine similarity (because vectors are normalized)\n",
        "\n",
        "index.add(embeddings.astype(np.float32))  # FAISS expects float32\n",
        "\n",
        "print(\"FAISS index successfully created\")\n",
        "print(f\"  → number of vectors: {index.ntotal:,}\")\n",
        "print(f\"  → dimensionality:    {dimension}\")\n",
        "print(f\"  → index type:        {index.__class__.__name__}\")"
      ],
      "metadata": {
        "id": "oXik4WQbsjTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cabf265-c262-4461-c19d-5b28483e8b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index successfully created\n",
            "  → number of vectors: 402\n",
            "  → dimensionality:    384\n",
            "  → index type:        IndexFlatIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(\n",
        "    query: str,\n",
        "    k: int = 4,\n",
        "    min_score: float = 0.35\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar chunks for a given query.\n",
        "    Returns list of dicts with score, source document and text preview.\n",
        "    \"\"\"\n",
        "    # Embed the query (same model & normalization as the corpus)\n",
        "    q_embedding = embedder.encode(\n",
        "        [query],\n",
        "        normalize_embeddings=True,\n",
        "        convert_to_numpy=True\n",
        "    ).astype('float32')\n",
        "\n",
        "    # Search the index\n",
        "    distances, indices = index.search(q_embedding, k)\n",
        "\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        if idx == -1:  # no more results\n",
        "            continue\n",
        "        score = float(dist)  # cosine similarity (higher = better)\n",
        "        if score < min_score:\n",
        "            continue\n",
        "        results.append({\n",
        "            'rank': len(results) + 1,\n",
        "            'score': round(score, 3),\n",
        "            'source': sources[idx],\n",
        "            'text_preview': texts[idx][:180].replace('\\n', ' ').strip() + \"...\"\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Test with crypto questions\n",
        "test_questions = [\n",
        "    \"What is Solana's Proof of History and why does it matter?\",\n",
        "    \"How does Uniswap v3 concentrated liquidity work?\",\n",
        "    \"What problem does Chainlink solve for smart contracts?\",\n",
        "    \"According to the Bitcoin whitepaper, what is double-spending and how is it prevented?\",\n",
        "    \"What are the key differences between Bitcoin and traditional electronic cash systems?\"\n",
        "]\n",
        "\n",
        "print(\"RAG Retrieval Test Results\\n\" + \"═\"*65 + \"\\n\")\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"Query: {question}\")\n",
        "    hits = retrieve(question, k=4, min_score=0.35)\n",
        "\n",
        "    if not hits:\n",
        "        print(\"  → No matches above min_score threshold\\n\")\n",
        "        continue\n",
        "\n",
        "    for hit in hits:\n",
        "        print(f\"  {hit['rank']}. {hit['score']:>5.3f}   {hit['source']}\")\n",
        "        print(f\"     {hit['text_preview']}\\n\")\n",
        "\n",
        "    print(\"─\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LwNrQpKT3MS",
        "outputId": "f0459dcf-1842-4d52-ca97-6b3b6e0a99f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Retrieval Test Results\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "Query: What is Solana's Proof of History and why does it matter?\n",
            "  1. 0.707   solana.pdf\n",
            "     . That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architecture based on Proof of History (PoH) - a proof for verifying order and passage of t...\n",
            "\n",
            "  2. 0.703   solana.pdf\n",
            "     . Elections for the proposed PoS algorithm are covered in depth in Section 5.6. In terms of CAP theorem, Consistency is almost always picked over Avail- ability in an event of a Pa...\n",
            "\n",
            "  3. 0.684   solana.pdf\n",
            "     . The Leader would then censor the Byzantine bond holders from participating. Proof of History generator would have to continue generating a sequence, to prove the passage of time,...\n",
            "\n",
            "  4. 0.669   solana.pdf\n",
            "     . This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up in history length. Additionally, a single source...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: How does Uniswap v3 concentrated liquidity work?\n",
            "  1. 0.877   uniswap.pdf\n",
            "     . Uniswap v3 introduced concentrated liquidity, enabling more capital efficient liquidity through positions that provide liquidity within a limited price range, and multiple fee ti...\n",
            "\n",
            "  2. 0.861   uniswap.pdf\n",
            "     . In Uniswap v3, users were required to utilize the concentrated liquidity AMM introduced in the same version. Since their intro- duction, concentrated liquidity AMMs have become w...\n",
            "\n",
            "  3. 0.818   uniswap.pdf\n",
            "     . This functionality relies on the fee accounting system to facilitate efficient payments. The fee payment system can only support either of the tokens in the token pair for the po...\n",
            "\n",
            "  4. 0.815   uniswap.pdf\n",
            "     . Uniswap v4 offers cus- tomizability via arbitrary code hooks, allowing developers to aug- ment the concentrated liquidity model introduced in Uniswap v3 with new functionality. I...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: What problem does Chainlink solve for smart contracts?\n",
            "  1. 0.820   chainlink.pdf\n",
            "     . Because of the mechanics of the consensus mechanisms underpinning blockchains, a blockchain cannot directly fetch such critical data. We propose a solution to the smart contract...\n",
            "\n",
            "  2. 0.807   chainlink.pdf\n",
            "     . 8 Conclusion We have introduced ChainLink, a decentralized oracle network for smart contracts to securely interact with resources external to the blockchain. We have outlined the...\n",
            "\n",
            "  3. 0.799   chainlink.pdf\n",
            "     . In order for a smart contract on networks like Ethereum to use a ChainLink node, they will need to pay their chosen ChainLink Node Operator using LINK tokens, with prices being s...\n",
            "\n",
            "  4. 0.781   chainlink.pdf\n",
            "     . We believe that the current focus on tokens to the exclusion of many other possible applications is due to a lack of adequate oracle services, a situation ChainLink speciﬁcally a...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: According to the Bitcoin whitepaper, what is double-spending and how is it prevented?\n",
            "  1. 0.818   bitcoin.pdf\n",
            "     . A common solution is to introduce a trusted central authority, or mint, that checks every transaction for double spending. After each transaction, the coin must be returned to th...\n",
            "\n",
            "  2. 0.776   bitcoin.pdf\n",
            "     Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.com www.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payme...\n",
            "\n",
            "  3. 0.763   bitcoin.pdf\n",
            "     . Once a predetermined number of coins have entered circulation, the incentive can transition entirely to transaction fees and be completely inflation free. The incentive may help...\n",
            "\n",
            "  4. 0.748   bitcoin.pdf\n",
            "     . The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity t...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: What are the key differences between Bitcoin and traditional electronic cash systems?\n",
            "  1. 0.803   bitcoin.pdf\n",
            "     Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.com www.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payme...\n",
            "\n",
            "  2. 0.737   bitcoin.pdf\n",
            "     . 1 2. Transactions We define an electronic coin as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction...\n",
            "\n",
            "  3. 0.713   bitcoin.pdf\n",
            "     .15 z=8 q=0.20 z=11 q=0.25 z=15 q=0.30 z=24 q=0.35 z=41 q=0.40 z=89 q=0.45 z=340 12. Conclusion We have proposed a system for electronic transactions without relying on trust. We s...\n",
            "\n",
            "  4. 0.706   bitcoin.pdf\n",
            "     . The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity t...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "\n",
        "np.save(\"crypto_embeddings.npy\", embeddings)\n",
        "faiss.write_index(index, \"crypto_rag_index.faiss\")\n",
        "\n",
        "with open(\"crypto_corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"texts\": texts, \"sources\": sources}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved: embeddings, index, corpus metadata\")\n",
        "# saving all the metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdNMdlKkT3PE",
        "outputId": "caef758d-ba12-454a-83b7-648d2127cc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: embeddings, index, corpus metadata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "import os\n",
        "\n",
        "np.save(\"crypto_embeddings.npy\", embeddings)\n",
        "print(f\"Saved embeddings array with shape: {embeddings.shape}\")\n",
        "\n",
        "faiss.write_index(index, \"crypto_rag_index.faiss\")\n",
        "print(f\"Saved FAISS index containing {index.ntotal} vectors\")\n",
        "\n",
        "# Corpus metadata (texts, sources, pages, embedder name)\n",
        "corpus = {\n",
        "    \"texts\": texts,\n",
        "    \"sources\": sources,\n",
        "    \"pages\": pages if 'pages' in globals() else [None] * len(texts),\n",
        "    \"embedder_model\": \"BAAI/bge-small-en-v1.5\"  # change if you used a different embedder\n",
        "}\n",
        "\n",
        "with open(\"crypto_corpus_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved corpus metadata (texts, sources, pages, embedder info)\")\n",
        "\n",
        "# Manifest file with reloading instructions\n",
        "manifest = {\n",
        "    \"files_saved\": [\n",
        "        \"crypto_embeddings.npy\",\n",
        "        \"crypto_rag_index.faiss\",\n",
        "        \"crypto_corpus_metadata.json\"\n",
        "    ],\n",
        "    \"reload_instructions\": (\n",
        "        \"In new notebook:\\n\"\n",
        "        \"embeddings = np.load('crypto_embeddings.npy')\\n\"\n",
        "        \"index = faiss.read_index('crypto_rag_index.faiss')\\n\"\n",
        "        \"with open('crypto_corpus_metadata.json') as f: corpus = json.load(f)\\n\"\n",
        "        \"texts = corpus['texts']\\n\"\n",
        "        \"sources = corpus['sources']\\n\"\n",
        "        \"pages = corpus['pages']\\n\"\n",
        "        \"from sentence_transformers import SentenceTransformer\\n\"\n",
        "        \"embedder = SentenceTransformer(corpus['embedder_model'])\"\n",
        "    )\n",
        "}\n",
        "\n",
        "with open(\"reload_manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"All files saved successfully.\")\n",
        "print(\"Files in current directory:\")\n",
        "print(os.listdir(\".\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpOk_7ZVT3Ry",
        "outputId": "d65518e7-aa05-4754-9661-ae489462aa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings array with shape: (402, 384)\n",
            "Saved FAISS index containing 402 vectors\n",
            "Saved corpus metadata (texts, sources, pages, embedder info)\n",
            "All files saved successfully.\n",
            "Files in current directory:\n",
            "['.config', 'chainlink.pdf', 'solana.pdf', 'crypto_embeddings.npy', 'crypto_corpus.json', 'crypto_rag_eval_dataset.json', 'crypto_corpus_metadata.json', 'uniswap.pdf', 'bitcoin.pdf', 'reload_manifest.json', 'crypto_rag_index.faiss', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "qs = [\n",
        "    \"What is Solana's Proof of History?\",\n",
        "    \"How does Uniswap v3 concentrated liquidity work?\",\n",
        "    \"What problem does Chainlink solve?\",\n",
        "    \"Bitcoin double-spending prevention\",\n",
        "] * 5   # 20 queries\n",
        "\n",
        "times = []\n",
        "for q in qs:\n",
        "    t0 = time.time()\n",
        "    _ = retrieve(q)\n",
        "    times.append(time.time() - t0)\n",
        "\n",
        "print(f\"Average query time: {np.mean(times)*1000:.1f} ms\")\n",
        "print(f\"Median:             {np.median(times)*1000:.1f} ms\")\n",
        "print(f\"Min / Max:          {np.min(times)*1000:.1f} – {np.max(times)*1000:.1f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ-903qLT3UL",
        "outputId": "b588813f-a531-4927-f01b-b85c81e29b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average query time: 42.6 ms\n",
            "Median:             41.9 ms\n",
            "Min / Max:          37.2 – 52.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "qs = [\n",
        "    \"What is Solana's Proof of History?\",\n",
        "    \"How does Uniswap v3 concentrated liquidity work?\",\n",
        "    \"What problem does Chainlink solve?\",\n",
        "    \"Bitcoin double-spending prevention\",\n",
        "] * 5   # 20 queries\n",
        "\n",
        "times = []\n",
        "for q in qs:\n",
        "    t0 = time.time()\n",
        "    _ = retrieve(q)\n",
        "    times.append(time.time() - t0)\n",
        "\n",
        "print(f\"Average query time: {np.mean(times)*1000:.1f} ms\")\n",
        "print(f\"Median:             {np.median(times)*1000:.1f} ms\")\n",
        "print(f\"Min / Max:          {np.min(times)*1000:.1f} – {np.max(times)*1000:.1f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IeRmHT_T3Wj",
        "outputId": "6d396dc6-2dd8-40ef-92e2-7d1657a84bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average query time: 43.7 ms\n",
            "Median:             43.8 ms\n",
            "Min / Max:          36.8 – 54.2 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "#Bitcoin pairs (10)\n",
        "bitcoin_pairs = [\n",
        "    {\n",
        "        \"question\": \"How is an electronic coin defined within the Bitcoin system?\",\n",
        "        \"ground_truth_excerpt\": \"An electronic coin is defined as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the primary purpose of the Proof-of-Work system in the Bitcoin network?\",\n",
        "        \"ground_truth_excerpt\": \"The proof-of-work system is used to implement a distributed timestamp server on a peer-to-peer basis and to solve the problem of determining representation in majority decision making (one-CPU-one-vote).\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the network handle a situation where two nodes broadcast different versions of the next block simultaneously?\",\n",
        "        \"ground_truth_excerpt\": \"Nodes work on the first version they receive but save the other branch. The tie is broken when the next proof-of-work is found and one branch becomes longer; nodes then switch to the longer chain.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the two components that fund the incentive for nodes to support the network?\",\n",
        "        \"ground_truth_excerpt\": \"The incentive is funded by a special first transaction in a block that starts a new coin owned by the block creator, and by transaction fees (the difference between transaction input and output values).\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can disk space be reclaimed without breaking a block's hash?\",\n",
        "        \"ground_truth_excerpt\": \"Transactions are hashed in a Merkle Tree, with only the root included in the block's hash. Old blocks can be compacted by stubbing off branches of the tree and discarding spent transactions.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is 'Simplified Payment Verification' (SPV) and what does a user need to maintain it?\",\n",
        "        \"ground_truth_excerpt\": \"SPV allows a user to verify payments without running a full node. The user only needs to keep a copy of the block headers of the longest proof-of-work chain and obtain the Merkle branch linking the transaction to its block.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 5,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the Bitcoin privacy model differ from the traditional banking model?\",\n",
        "        \"ground_truth_excerpt\": \"The traditional model limits information to the parties involved and a trusted third party. Bitcoin's model announces all transactions publicly but maintains privacy by keeping public keys anonymous.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 6,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What specific attack can a dishonest sender attempt if they control significant CPU power?\",\n",
        "        \"ground_truth_excerpt\": \"An attacker can only try to change one of his own transactions to take back money he recently spent by generating an alternate chain faster than the honest chain.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 6,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How is the proof-of-work difficulty adjusted over time?\",\n",
        "        \"ground_truth_excerpt\": \"The difficulty is determined by a moving average targeting an average number of blocks per hour. If blocks are generated too fast, the difficulty increases.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the requirement for the network to remain secure against attacker nodes?\",\n",
        "        \"ground_truth_excerpt\": \"The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 1,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "#Chainlink pairs (10)\n",
        "chainlink_pairs = [\n",
        "    {\n",
        "        \"question\": \"What are the two primary functions of the Chainlink core node software?\",\n",
        "        \"ground_truth_excerpt\": \"The core node software is responsible for interfacing with the blockchain, scheduling, and balancing work across its various external services.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 7,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does Chainlink define the concept of 'External Adapters' in its off-chain architecture?\",\n",
        "        \"ground_truth_excerpt\": \"Adapters are external services with a minimal REST API. By modeling adapters in a service-oriented manner, programs in any programming language can be easily implemented simply by adding a small intermediate API in front of the program.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 7,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the paper, what is the 'freeloading' problem in oracle networks?\",\n",
        "        \"ground_truth_excerpt\": \"A cheating oracle Oz can observe the response Ai of another oracle Oi and copy it. In this way, oracle Oz avoids the expense of querying data sources, which may charge per-query fees.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 13,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Explain the commit/reveal scheme used in the In-Contract Aggregation protocol.\",\n",
        "        \"ground_truth_excerpt\": \"In a first round, oracles send CHAINLINK-SC cryptographic commitments to their responses. After CHAINLINK-SC has received a quorum of responses, it initiates a second round in which oracles reveal their responses.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 13,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the primary disadvantage of in-contract aggregation that the paper identifies?\",\n",
        "        \"ground_truth_excerpt\": \"In-contract aggregation has a key disadvantage: Cost. It incurs the cost of transmitting and processing on chain O(n) oracle messages (commits and reveals for A1, A2, . . . , An).\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the proposed use of threshold signatures improve off-chain aggregation?\",\n",
        "        \"ground_truth_excerpt\": \"Partial signatures on the same value A can be aggregated across any set of t oracles to yield a single valid collective signature. This approach allows CHAINLINK-SC to obtain an aggregate answer without needing to receive answers from multiple oracles.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What security properties does Intel SGX provide to Chainlink enclaves?\",\n",
        "        \"ground_truth_excerpt\": \"First, enclaves protect the integrity of the application, meaning its data, code, and control flow, against subversion by other processes. Second, an enclave protects the confidentiality of an application, meaning that its data, code, and execution state are opaque to other processes.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 22,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Describe the function of the MIGFLAG in the proposed Contract-Upgrade Service.\",\n",
        "        \"ground_truth_excerpt\": \"CHAINLINK-SC would support a flag (MIGFLAG) in oracle calls from requesting contracts indicating whether or not a call should be forwarded to a new CHAINLINK-SC should one become available.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 20,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the purpose of the Chainlink Certification Service?\",\n",
        "        \"ground_truth_excerpt\": \"The Certification Service is planned as a means to identify Sybil attacks and other malfeasance that automated on-chain systems cannot.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 20,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the paper suggest dealing with potential correlations between different data sources?\",\n",
        "        \"ground_truth_excerpt\": \"Chainlink also proposes to pursue research into mapping and reporting the independence of data sources in an easily digestible way so that oracles and users can avoid undesired correlations.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 11,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "import json\n",
        "\n",
        "#Solana pairs\n",
        "solana_pairs = [\n",
        "    {\n",
        "        \"question\": \"What is the primary function of Proof of History (PoH) in the Solana architecture?\",\n",
        "        \"ground_truth_excerpt\": \"Proof of History is a sequence of computation that can provide a way to cryptographically verify passage of time between two events. PoH is used to encode trustless passage of time into a ledger an append only data structure.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 1,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does Proof of History allow for parallel verification by external computers?\",\n",
        "        \"ground_truth_excerpt\": \"The output can then be re-computed and verified by external computers in parallel by checking each sequence segment on a separate core. Given some number of cores... the verifier can split up the sequence of hashes and their indexes into 4000 slices, and in parallel make sure that each slice is correct.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the paper, how are events 'timestamped' into the PoH sequence?\",\n",
        "        \"ground_truth_excerpt\": \"Data can be timestamped into this sequence by appending the data (or a hash of some data) into the state of the function. The recording of the state, index and data as it was appended into the sequences provides a timestamp that can guarantee that the data was created sometime before the next hash was generated.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What role do 'Verifiers' play in the Solana network design?\",\n",
        "        \"ground_truth_excerpt\": \"Verifiers execute the same transactions on their copies of the state, and publish their computed signatures of the state as confirmations. The published confirmations serve as votes for the consensus algorithm.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Describe how horizontal scaling is achieved for PoH generators without sharding.\",\n",
        "        \"ground_truth_excerpt\": \"Its possible to synchronize multiple Proof of History generators by mixing the sequence state from each generator to each other generator. This property can be transitive... we can trace the dependency between A and C even though they were not synchronized directly.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 9,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does Proof of History protect the network against long-range attacks?\",\n",
        "        \"ground_truth_excerpt\": \"A malicious user that gains access to old private keys would have to recreate a historical record that takes as much time as the original one they are trying to forge. This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 13,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What constitutes a 'super majority' in the Solana Proof of Stake consensus?\",\n",
        "        \"ground_truth_excerpt\": \"A super majority is 2/3rds of the validators weighted by their bonds. A super majority vote indicates that the network has reached consensus.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the network handle the 'nothing at stake' problem in its Proof of Stake system?\",\n",
        "        \"ground_truth_excerpt\": \"Slashing is the proposed solution... When a proof of voting for a different branch is published, that branch can destroy the validators bond. This is an economic incentive designed to discourage validators from confirming multiple branches.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What mechanism allows the network to recover from a large partition where more than 1/2 of verifiers are missing?\",\n",
        "        \"ground_truth_excerpt\": \"In a large partition... the unstaking process is very very slow. Full 2/3rds consensus will not be achieved until a very large amount of hashes have been generated and the unavailable verifiers have been unstaked.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 17,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How is a new Leader elected if the current Proof of History generator fails?\",\n",
        "        \"ground_truth_excerpt\": \"Election for a new PoH generator occur when the PoH generator failure is detected. The validator with the largest voting power, or highest public key address if there is a tie is picked as the new PoH generator.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 15,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "#Uniswap pairs\n",
        "uniswap_pairs = [\n",
        "    {\n",
        "        \"question\": \"How is 'concentrated liquidity' defined in Uniswap v3, and how does it differ from the liquidity distribution in earlier versions?\",\n",
        "        \"ground_truth_excerpt\": \"The defining idea of UNISWAP V3 is that of concentrated liquidity: liquidity bounded within some price range. In earlier versions, liquidity was distributed uniformly along the x * y = k reserves curve... designed to provide liquidity across the entire price range (0, infinity).\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are 'virtual reserves' in the context of a concentrated liquidity position?\",\n",
        "        \"ground_truth_excerpt\": \"A position only needs to maintain enough reserves to support trading within its range, and therefore can act like a constant product pool with larger reserves (we call these the virtual reserves) within that range.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the paper, what happens to a position's liquidity and fee earnings when the price exits its specified range?\",\n",
        "        \"ground_truth_excerpt\": \"When the price exits a position's range, the position's liquidity is no longer active, and no longer earns fees. At that point, its liquidity is composed entirely of a single asset.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are 'range orders' in Uniswap v3 and how do they relate to traditional limit orders?\",\n",
        "        \"ground_truth_excerpt\": \"Positions on very small ranges act similarly to limit orders—if the range is crossed, the position flips from being composed entirely of one asset, to being composed entirely of the other asset (plus accrued fees).\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How are swap fees handled differently in Uniswap v3 compared to v2 regarding compounding?\",\n",
        "        \"ground_truth_excerpt\": \"Fees earned in earlier versions were continuously deposited in the pool as liquidity... that fee earnings compounded. In UNISWAP V3, due to the non-fungible nature of positions, this is no longer possible. Instead, fee earnings are stored separately and held as the tokens in which the fees are paid.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What mechanism does Uniswap v3 use to track prices in its oracle, and why was this change made?\",\n",
        "        \"ground_truth_excerpt\": \"Instead of accumulating the sum of prices, allowing users to compute the arithmetic mean TWAP, UNISWAP v3 tracks the sum of log prices, allowing users to compute the geometric mean TWAP. Using the time-weighted geometric mean price... avoids the need to track separate accumulators for these ratios.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the purpose of the 'liquidity accumulator' introduced in the Uniswap v3 oracle?\",\n",
        "        \"ground_truth_excerpt\": \"This liquidity accumulator is useful for external contracts that want to implement liquidity mining on top of Uniswap v3. It can also be used by other contracts to inform a decision on which of the pools corresponding to a pair... will have the most reliable TWAP.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How are 'ticks' used to demarcate price space in Uniswap v3?\",\n",
        "        \"ground_truth_excerpt\": \"To implement custom liquidity provision, the space of possible prices is demarcated by discrete ticks. Liquidity providers can provide liquidity in a range between any two ticks... Conceptually, there is a tick at every price p that is an integer power of 1.0001.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 5,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What determines which ticks can be 'initialized' for a liquidity position?\",\n",
        "        \"ground_truth_excerpt\": \"Not every tick can be initialized. The pool is instantiated with a parameter, tickSpacing (ts); only ticks with indexes that are divisible by tickSpacing can be initialized.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 5,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Why does Uniswap v3 track the square root of price (sqrtPrice) and liquidity (L) instead of virtual reserves (x and y)?\",\n",
        "        \"ground_truth_excerpt\": \"Using L and sqrtP is convenient because only one of them changes at a time. Price (and thus sqrtP) changes when swapping within a tick; liquidity changes when crossing a tick... This avoids some rounding errors that could be encountered if tracking virtual reserves.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 6,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Solana pairs: {len(solana_pairs)}\")\n",
        "print(f\"Uniswap pairs: {len(uniswap_pairs)}\")\n",
        "\n",
        "print(\"All lists defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68O9UlwKWcbZ",
        "outputId": "329255d1-f628-49f9-a29c-7ad276074bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solana pairs: 10\n",
            "Uniswap pairs: 10\n",
            "All lists defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine everything into one list (exactly 40 pairs)\n",
        "full_eval_dataset = (\n",
        "    bitcoin_pairs +\n",
        "    chainlink_pairs +\n",
        "    solana_pairs +\n",
        "    uniswap_pairs\n",
        ")\n",
        "\n",
        "# Verify count\n",
        "assert len(full_eval_dataset) == 40, f\"Expected 40 pairs, got {len(full_eval_dataset)}\"\n",
        "\n",
        "random.seed(42)  # reproducible shuffle\n",
        "random.shuffle(full_eval_dataset)\n",
        "\n",
        "# Save – this overwrites any previous file\n",
        "output_file = \"crypto_rag_eval_dataset.json\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(full_eval_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Dataset saved: {output_file}\")\n",
        "print(f\"Total pairs: {len(full_eval_dataset)}\")\n",
        "print(f\"Sources: {sorted(set(d['source'] for d in full_eval_dataset))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKC6iRPhWcfz",
        "outputId": "d66597ab-320b-41c2-ddbe-1b211e8795e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved: crypto_rag_eval_dataset.json\n",
            "Total pairs: 40\n",
            "Sources: ['bitcoin.pdf', 'chainlink.pdf', 'solana.pdf', 'uniswap.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distribution by source\n",
        "from collections import Counter\n",
        "print(\"Count per source:\")\n",
        "print(Counter(d['source'] for d in full_eval_dataset))\n",
        "\n",
        "# Show first 3 and last 3 for sanity check\n",
        "print(\"\\nFirst 3:\")\n",
        "for item in full_eval_dataset[:3]:\n",
        "    print(json.dumps(item, indent=2))\n",
        "    print(\"---\")\n",
        "\n",
        "print(\"\\nLast 3:\")\n",
        "for item in full_eval_dataset[-3:]:\n",
        "    print(json.dumps(item, indent=2))\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSJkG_9tWciz",
        "outputId": "e9d21ef5-dd4c-43ad-eff7-a5e26eee91fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count per source:\n",
            "Counter({'bitcoin.pdf': 10, 'chainlink.pdf': 10, 'uniswap.pdf': 10, 'solana.pdf': 10})\n",
            "\n",
            "First 3:\n",
            "{\n",
            "  \"question\": \"What is the requirement for the network to remain secure against attacker nodes?\",\n",
            "  \"ground_truth_excerpt\": \"The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 1,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What are the two components that fund the incentive for nodes to support the network?\",\n",
            "  \"ground_truth_excerpt\": \"The incentive is funded by a special first transaction in a block that starts a new coin owned by the block creator, and by transaction fees (the difference between transaction input and output values).\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 4,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What are the two primary functions of the Chainlink core node software?\",\n",
            "  \"ground_truth_excerpt\": \"The core node software is responsible for interfacing with the blockchain, scheduling, and balancing work across its various external services.\",\n",
            "  \"source\": \"chainlink.pdf\",\n",
            "  \"page\": 7,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "\n",
            "Last 3:\n",
            "{\n",
            "  \"question\": \"Describe the function of the MIGFLAG in the proposed Contract-Upgrade Service.\",\n",
            "  \"ground_truth_excerpt\": \"CHAINLINK-SC would support a flag (MIGFLAG) in oracle calls from requesting contracts indicating whether or not a call should be forwarded to a new CHAINLINK-SC should one become available.\",\n",
            "  \"source\": \"chainlink.pdf\",\n",
            "  \"page\": 20,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What is the primary purpose of the Proof-of-Work system in the Bitcoin network?\",\n",
            "  \"ground_truth_excerpt\": \"The proof-of-work system is used to implement a distributed timestamp server on a peer-to-peer basis and to solve the problem of determining representation in majority decision making (one-CPU-one-vote).\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 3,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What specific attack can a dishonest sender attempt if they control significant CPU power?\",\n",
            "  \"ground_truth_excerpt\": \"An attacker can only try to change one of his own transactions to take back money he recently spent by generating an alternate chain faster than the honest chain.\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 6,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming retrieve() is already defined and working\n",
        "\n",
        "updated_dataset = []\n",
        "\n",
        "for item in full_eval_dataset:  # your list of 40 items\n",
        "    q = item[\"question\"]\n",
        "    hits = retrieve(q, k=1, min_score=0.0)  # top-1, even low score\n",
        "\n",
        "    retrieved_score = hits[0][\"score\"] if hits else None\n",
        "    retrieved_text_preview = hits[0][\"text_preview\"] if hits else None  # <-- FIXED HERE\n",
        "\n",
        "    new_item = item.copy()\n",
        "    new_item[\"retrieved_score\"] = round(retrieved_score, 3) if retrieved_score is not None else None\n",
        "    new_item[\"retrieved_text_preview\"] = retrieved_text_preview\n",
        "\n",
        "    updated_dataset.append(new_item)\n",
        "\n",
        "# Re-save the enriched JSON\n",
        "with open(\"crypto_rag_eval_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(updated_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"All 40 questions processed. Scores and previews added to JSON.\")\n",
        "print(\"Example (first item):\")\n",
        "print(json.dumps(updated_dataset[0], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gB3ot_uWo0W",
        "outputId": "ebe256b7-a09b-4c12-e5a8-80a7dd2856bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 40 questions processed. Scores and previews added to JSON.\n",
            "Example (first item):\n",
            "{\n",
            "  \"question\": \"What is the requirement for the network to remain secure against attacker nodes?\",\n",
            "  \"ground_truth_excerpt\": \"The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 1,\n",
            "  \"type\": \"factual-extraction\",\n",
            "  \"retrieved_score\": 0.782,\n",
            "  \"retrieved_text_preview\": \". As such, the verification is reliable as long as honest nodes control the network, but is more vulnerable if the network is overpowered by an attacker. While network nodes can ve...\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "lN6Hdxrzadyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages - quiet mode to reduce output noise\n",
        "!pip install -q --upgrade \\\n",
        "    transformers==4.44.2 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    accelerate==0.33.0 \\\n",
        "    peft==0.12.0 \\\n",
        "    datasets==2.21.0 \\\n",
        "    rouge-score==0.1.2 \\\n",
        "    nltk==3.8.1 \\\n",
        "    tabulate==0.9.0 \\\n",
        "    scikit-learn==1.5.1\n",
        "\n",
        "print(\"Installs finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymm1lhHgWo3F",
        "outputId": "e76957a1-78bd-421b-9a44-9add4ab0e09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalls finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Metrics\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Imports successful.\")\n",
        "\n",
        "# Quick checks that core pieces from your previous notebook are still alive\n",
        "print(f\"FAISS index exists? → { 'index' in globals() }\")\n",
        "print(f\"retrieve() function exists? → { 'retrieve' in globals() }\")\n",
        "print(f\"Evaluation dataset exists? → { 'full_eval_dataset' in globals() or 'updated_dataset' in globals() }\")\n",
        "\n",
        "try:\n",
        "    print(f\"Number of eval items: {len(updated_dataset) if 'updated_dataset' in globals() else len(full_eval_dataset)}\")\n",
        "except:\n",
        "    print(\"Warning: eval dataset variable not found — load it now if needed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mFICPzRWo5s",
        "outputId": "a18c870b-5b39-4f5a-ce9d-41777e340b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports successful.\n",
            "FAISS index exists? → True\n",
            "retrieve() function exists? → True\n",
            "Evaluation dataset exists? → True\n",
            "Number of eval items: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall current version first to avoid conflicts\n",
        "!pip uninstall -y transformers accelerate bitsandbytes\n",
        "\n",
        "# Install known-good versions to avoid crashes\n",
        "!pip install -q \\\n",
        "    transformers==4.37.2 \\\n",
        "    accelerate==0.27.2 \\\n",
        "    bitsandbytes==0.43.0\n",
        "\n",
        "print(\"Clean install of stable transformers + accelerate finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev1Jr7GVWo8b",
        "outputId": "c92e027f-9d36-421c-f86a-d6094644f65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.44.2\n",
            "Uninstalling transformers-4.44.2:\n",
            "  Successfully uninstalled transformers-4.44.2\n",
            "Found existing installation: accelerate 0.33.0\n",
            "Uninstalling accelerate-0.33.0:\n",
            "  Successfully uninstalled accelerate-0.33.0\n",
            "Found existing installation: bitsandbytes 0.43.3\n",
            "Uninstalling bitsandbytes-0.43.3:\n",
            "  Successfully uninstalled bitsandbytes-0.43.3\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 1.2.0 requires tokenizers<1.0.0,>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mClean install of stable transformers + accelerate finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart os for using the updated library versions\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "VQBWmayJWo-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading distilgpt2 with stable transformers version...\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"distilgpt2\",\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float32,  # safe & fast on CPU\n",
        "    )\n",
        "    # Fix pad token (very common with gpt2 variants)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    print(\"DistilGPT2 loaded SUCCESSFULLY.\")\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "except Exception as e:\n",
        "    print(\"Still failed:\", str(e))\n",
        "    print(\"\\nIf still broken → try transformers==4.36.2 instead (rerun install cell with that version)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIbyIMtDWpBT",
        "outputId": "b444a579-eee8-4818-faaf-6e6a7d5980c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading distilgpt2 with stable transformers version...\n",
            "DistilGPT2 loaded SUCCESSFULLY.\n",
            "Model device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data = updated_dataset if 'updated_dataset' in globals() else full_eval_dataset\n",
        "print(f\"Generating for {len(eval_data)} questions...\")\n",
        "\n",
        "rag_results = []\n",
        "\n",
        "# Settings optimized for CPU speed & short answers\n",
        "MAX_NEW_TOKENS = 96          # shorter than 128 — crypto excerpts are concise\n",
        "BATCH_SIZE = 4               # distilgpt2 is small → safe to batch 4–8 on CPU\n",
        "\n",
        "for i in tqdm(range(0, len(eval_data), BATCH_SIZE), desc=\"Generating (batched)\"):\n",
        "    batch_items = eval_data[i:i+BATCH_SIZE]\n",
        "    prompts = []\n",
        "\n",
        "    for item in batch_items:\n",
        "        q = item[\"question\"]\n",
        "        hits = retrieve(q, k=3, min_score=0.35)\n",
        "\n",
        "        if not hits:\n",
        "            context = \"No relevant context found.\"\n",
        "            retrieved_chunks = []\n",
        "            cosine_scores = []\n",
        "        else:\n",
        "            context = \"\\n\\n\".join([hit.get(\"text_preview\", hit.get(\"text\", \"\"))[:400] for hit in hits])  # cap length\n",
        "            retrieved_chunks = [hit.get(\"text_preview\", hit.get(\"text\", \"\"))[:300] + \"...\" for hit in hits]\n",
        "            cosine_scores = [hit[\"score\"] for hit in hits]\n",
        "\n",
        "        prompt = f\"\"\"Use only the following context to answer factually. If unsure, say so.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {q}\n",
        "\n",
        "Concise Answer:\"\"\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Batch generate\n",
        "    start_gen = time.time()\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False, # greedy = faster & consistent\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "    answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Clean: remove prompt prefix from each answer\n",
        "    cleaned_answers = []\n",
        "    for prompt, ans in zip(prompts, answers):\n",
        "        if ans.startswith(prompt):\n",
        "            cleaned = ans[len(prompt):].strip()\n",
        "        else:\n",
        "            cleaned = ans.strip()\n",
        "        cleaned_answers.append(cleaned)\n",
        "\n",
        "    gen_time = time.time() - start_gen\n",
        "\n",
        "    #time retrieval separately\n",
        "    for j, item in enumerate(batch_items):\n",
        "        result = {\n",
        "            \"question\": item[\"question\"],\n",
        "            \"ground_truth_excerpt\": item.get(\"ground_truth_excerpt\", \"\"),\n",
        "            \"rag_answer\": cleaned_answers[j],\n",
        "            \"retrieved_chunks_preview\": retrieved_chunks if 'retrieved_chunks' in locals() else [],\n",
        "            \"cosine_scores\": cosine_scores if 'cosine_scores' in locals() else [],\n",
        "            \"generate_time_sec\": round(gen_time / len(batch_items), 2),  # avg per item\n",
        "        }\n",
        "        rag_results.append(result)\n",
        "\n",
        "    # Save partial every ~10 items\n",
        "    current_idx = i + len(batch_items)\n",
        "    if current_idx % 10 == 0 or current_idx >= len(eval_data):\n",
        "        partial_file = f\"rag_outputs_partial_{current_idx}.json\"\n",
        "        with open(partial_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(rag_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Saved partial: {partial_file} ({current_idx}/{len(eval_data)})\")\n",
        "\n",
        "# Final save\n",
        "with open(\"rag_outputs_final.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rag_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nDone! Generated {len(rag_results)} answers. Saved to rag_outputs_final.json\")\n",
        "print(\"Average generate time per question (batched): ~\", round(sum(r[\"generate_time_sec\"] for r in rag_results) / len(rag_results), 1), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341,
          "referenced_widgets": [
            "ed192991ff8c499e92bfcb1dc513cb06",
            "3e06cd9528c8403ea5c28b62b96f3095",
            "6b461c2415b84864ad0a0f9a64a5602d",
            "1c4d0a232f1841d29d9b84b4d54a0886",
            "8836f5471f9a4a089031a3ffa65582f0",
            "1447b34470e244b5a928cf969f95b1ed",
            "1d0d953397ea4564bbc87430ee6b2827",
            "1df4fe9e330d429785366bb2f53556c3",
            "7f42c9c3740746e5bb232bcfc52ef691",
            "d139da5186c048ad86799b10ef959dd1",
            "20ca5384e91946ccb4760c1ab5022af4"
          ]
        },
        "id": "YITPlRSDbcrP",
        "outputId": "fa86fb7a-764b-49da-abb0-e8b4d55f6db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating for 40 questions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating (batched):   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed192991ff8c499e92bfcb1dc513cb06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved partial: rag_outputs_partial_20.json (20/40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved partial: rag_outputs_partial_40.json (40/40)\n",
            "\n",
            "Done! Generated 40 answers. Saved to rag_outputs_final.json\n",
            "Average generate time per question (batched): ~ 6.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# load generated results\n",
        "with open(\"rag_outputs_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    rag_results = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(rag_results)} generated answers.\")\n",
        "\n",
        "# Get embedding for a text\n",
        "def get_embedding(text: str):\n",
        "    if not text.strip():\n",
        "        return np.zeros(384)  # fallback zero vector (dim of bge-small-en-v1.5)\n",
        "    emb = embedder.encode([text], normalize_embeddings=True, convert_to_numpy=True)\n",
        "    return emb[0].astype(np.float32)\n",
        "\n",
        "# prepare ground-truth embeddings one time only\n",
        "gt_embeddings = []\n",
        "for item in rag_results:\n",
        "    gt_text = item.get(\"ground_truth_excerpt\", \"\").strip()\n",
        "    gt_emb = get_embedding(gt_text)\n",
        "    gt_embeddings.append(gt_emb)\n",
        "\n",
        "gt_embeddings = np.array(gt_embeddings)  # shape: (40, 384)\n",
        "\n",
        "# Scorer setup\n",
        "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Compute metrics per item\n",
        "metrics_list = []\n",
        "\n",
        "for idx, res in enumerate(rag_results):\n",
        "    q = res[\"question\"]\n",
        "    gt = res.get(\"ground_truth_excerpt\", \"\").strip()\n",
        "    answer = res[\"rag_answer\"].strip()\n",
        "\n",
        "    # retrieve the actual retrieved chunks\n",
        "    retrieved_texts = res.get(\"retrieved_chunks_preview\", [])\n",
        "    retrieved_scores = res.get(\"cosine_scores\", [])\n",
        "\n",
        "    #if no chunks retrieved then, skip some metrics or mark low\n",
        "    has_retrieval = len(retrieved_texts) > 0 and any(t.strip() for t in retrieved_texts)\n",
        "\n",
        "    # 1. Retrieval metrics - if they are present\n",
        "    precision_3 = 0.0\n",
        "    mrr = 0.0\n",
        "\n",
        "    if has_retrieval and idx < len(gt_embeddings):\n",
        "        gt_emb = gt_embeddings[idx]\n",
        "\n",
        "        #embed retrieved chunks (short previews, but good enough)\n",
        "        ret_embs = np.array([get_embedding(t) for t in retrieved_texts])\n",
        "\n",
        "        cosines = cosine_similarity([gt_emb], ret_embs)[0]  # shape (3,)\n",
        "\n",
        "        # Relevant if cosine >= 0.5 (adjustable threshold)\n",
        "        relevant = cosines >= 0.5\n",
        "        precision_3 = relevant.mean()  # fraction of top-3 relevant\n",
        "\n",
        "        #mRR: reciprocal rank of first relevant\n",
        "        ranks = np.where(relevant)[0]\n",
        "        if len(ranks) > 0:\n",
        "            mrr = 1.0 / (ranks[0] + 1)\n",
        "        else:\n",
        "            mrr = 0.0\n",
        "\n",
        "    # 2. Generation metrics\n",
        "    rouge_l = rouge.score(gt, answer)['rougeL'].fmeasure if gt else 0.0\n",
        "\n",
        "    # Semantic similarity (answer vs ground truth)\n",
        "    ans_emb = get_embedding(answer)\n",
        "    gt_emb = gt_embeddings[idx] if idx < len(gt_embeddings) else np.zeros(384)\n",
        "    semantic_sim = cosine_similarity([ans_emb], [gt_emb])[0][0] if np.any(gt_emb) else 0.0\n",
        "\n",
        "    # 3. Hallucination proxy: % sentences with low support from any retrieved chunk\n",
        "    halluc_rate = 0.0\n",
        "    if has_retrieval and answer:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        if sentences:\n",
        "            sent_embs = np.array([get_embedding(s) for s in sentences])\n",
        "            chunk_embs = np.array([get_embedding(t) for t in retrieved_texts if t.strip()])\n",
        "\n",
        "            if len(chunk_embs) > 0:\n",
        "                max_cosines = cosine_similarity(sent_embs, chunk_embs).max(axis=1)\n",
        "                unsupported = max_cosines < 0.30  # threshold: below 0.3 = likely hallucinated\n",
        "                halluc_rate = unsupported.mean()\n",
        "            else:\n",
        "                halluc_rate = 1.0  # all unsupported if no chunks\n",
        "    elif not gt:\n",
        "        halluc_rate = 0.0  # neutral if no ground truth\n",
        "\n",
        "    metrics = {\n",
        "        \"question\": q[:80] + \"...\" if len(q) > 80 else q,  # shorten for table\n",
        "        \"precision@3\": round(precision_3, 3),\n",
        "        \"mrr\": round(mrr, 3),\n",
        "        \"rougeL\": round(rouge_l, 3),\n",
        "        \"semantic_sim\": round(semantic_sim, 3),\n",
        "        \"halluc_rate\": round(halluc_rate, 3),\n",
        "        \"answer_length\": len(answer.split()),\n",
        "    }\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "#aggregate & show table\n",
        "df = pd.DataFrame(metrics_list)\n",
        "\n",
        "#summary stats\n",
        "summary = df[[\"precision@3\", \"mrr\", \"rougeL\", \"semantic_sim\", \"halluc_rate\"]].mean().round(3)\n",
        "print(\"\\nAggregate Metrics (average over 40 questions):\")\n",
        "print(summary.to_string())\n",
        "\n",
        "print(\"\\nFull table preview (first 5):\")\n",
        "print(tabulate(df.head(5), headers=\"keys\", tablefmt=\"simple\", showindex=False))\n",
        "\n",
        "#save all the data\n",
        "df.to_csv(\"rag_metrics_detailed.csv\", index=False)\n",
        "summary.to_csv(\"rag_metrics_summary.csv\")\n",
        "\n",
        "print(\"\\nSaved: rag_metrics_detailed.csv + rag_metrics_summary.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibrtTxa3c2WE",
        "outputId": "da86e839-b943-4797-8bc7-eaf494d6a46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 40 generated answers.\n",
            "\n",
            "Aggregate Metrics (average over 40 questions):\n",
            "precision@3     0.917\n",
            "mrr             0.938\n",
            "rougeL          0.138\n",
            "semantic_sim    0.708\n",
            "halluc_rate     0.000\n",
            "\n",
            "Full table preview (first 5):\n",
            "question                                                                               precision@3    mrr    rougeL    semantic_sim    halluc_rate    answer_length\n",
            "-----------------------------------------------------------------------------------  -------------  -----  --------  --------------  -------------  ---------------\n",
            "What is the requirement for the network to remain secure against attacker nodes?             0          0     0.074           0.634              0               87\n",
            "What are the two components that fund the incentive for nodes to support the net...          1          1     0.085           0.692              0               84\n",
            "What are the two primary functions of the Chainlink core node software?                      0.667      1     0.465           0.86               0               69\n",
            "What is the purpose of the 'liquidity accumulator' introduced in the Uniswap v3 ...          1          1     0.084           0.798              0               49\n",
            "How does Proof of History protect the network against long-range attacks?                    1          1     0.211           0.774              0              184\n",
            "\n",
            "Saved: rag_metrics_detailed.csv + rag_metrics_summary.csv\n"
          ]
        }
      ]
    }
  ]
}