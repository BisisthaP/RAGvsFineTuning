{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32b23485638d45078db7bd7fdf26f370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee59b02a8feb43bb99060167b71cfe91",
              "IPY_MODEL_b481b3c0fec345f998bffdeac572962f",
              "IPY_MODEL_4839d82c148e40e3830cce049707e980"
            ],
            "layout": "IPY_MODEL_7a9fee3d12dd417d9e9a1ca79481487b"
          }
        },
        "ee59b02a8feb43bb99060167b71cfe91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4cf14a7e923402eb360aa196d8c900f",
            "placeholder": "​",
            "style": "IPY_MODEL_36e09165613e497fbb1e40a3c4da5ccd",
            "value": "Embedding batches: 100%"
          }
        },
        "b481b3c0fec345f998bffdeac572962f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_518cfb66e0034dc8839666494f6ba928",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_309aa3b4c0fa4cdda986375b88c0ba1f",
            "value": 29
          }
        },
        "4839d82c148e40e3830cce049707e980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9daedd40755a4a49a5d67e5531991a4a",
            "placeholder": "​",
            "style": "IPY_MODEL_2632d52f7fab4fc4ac725ccbbd052b6c",
            "value": " 29/29 [01:18&lt;00:00,  2.29s/it]"
          }
        },
        "7a9fee3d12dd417d9e9a1ca79481487b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4cf14a7e923402eb360aa196d8c900f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e09165613e497fbb1e40a3c4da5ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518cfb66e0034dc8839666494f6ba928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309aa3b4c0fa4cdda986375b88c0ba1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9daedd40755a4a49a5d67e5531991a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2632d52f7fab4fc4ac725ccbbd052b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13fe55e29a694274a78ead8144ca6398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5dea46bd8af4f1b8b017aa6aad98d8c",
              "IPY_MODEL_a13427b77e344b008a318ab2dc8bc9ab",
              "IPY_MODEL_9983acc6b57a4a6d8deaab75e1dfccdc"
            ],
            "layout": "IPY_MODEL_1332351074a94020aa88ff720dd549b4"
          }
        },
        "a5dea46bd8af4f1b8b017aa6aad98d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5eea711f74c4abeb3ab47903b77931a",
            "placeholder": "​",
            "style": "IPY_MODEL_befc9a0f768a4ecc9e3400668e356bce",
            "value": "model.safetensors: 100%"
          }
        },
        "a13427b77e344b008a318ab2dc8bc9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce28a9a35d214a11bc91ded1c7a948a1",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c28745db68264a9b9706df0b8862c0ce",
            "value": 352824413
          }
        },
        "9983acc6b57a4a6d8deaab75e1dfccdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b41bb03ac9d4578bed5a2dbb74b2f4a",
            "placeholder": "​",
            "style": "IPY_MODEL_6bfa0d8b99db4190ae0949b5072da424",
            "value": " 353M/353M [00:03&lt;00:00, 166MB/s]"
          }
        },
        "1332351074a94020aa88ff720dd549b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5eea711f74c4abeb3ab47903b77931a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "befc9a0f768a4ecc9e3400668e356bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce28a9a35d214a11bc91ded1c7a948a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28745db68264a9b9706df0b8862c0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b41bb03ac9d4578bed5a2dbb74b2f4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bfa0d8b99db4190ae0949b5072da424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eb96ac29ec540a6b860b594904f68b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cc3b770cd804aaf90ec5f6450b31ac0",
              "IPY_MODEL_8955a8f810de4eca97d668342ca2db8d",
              "IPY_MODEL_c0ceb5b35766424f9187591fb8242fcd"
            ],
            "layout": "IPY_MODEL_993c322828114cfa8342b470688f03ab"
          }
        },
        "9cc3b770cd804aaf90ec5f6450b31ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f06fdee97cec4983ba4d0c7d437facca",
            "placeholder": "​",
            "style": "IPY_MODEL_bf1b8e5ab0294450b2cd73062e06635c",
            "value": "generation_config.json: 100%"
          }
        },
        "8955a8f810de4eca97d668342ca2db8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a2829b10764d2caec74e59f3778843",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40c5b18d7e2d4ac498391e3ba4e6193f",
            "value": 124
          }
        },
        "c0ceb5b35766424f9187591fb8242fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60fbd6fd7134132b517a7c5c5b97125",
            "placeholder": "​",
            "style": "IPY_MODEL_da391a0c3b0b487d9a1934ebd669ef71",
            "value": " 124/124 [00:00&lt;00:00, 10.0kB/s]"
          }
        },
        "993c322828114cfa8342b470688f03ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f06fdee97cec4983ba4d0c7d437facca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf1b8e5ab0294450b2cd73062e06635c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66a2829b10764d2caec74e59f3778843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c5b18d7e2d4ac498391e3ba4e6193f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d60fbd6fd7134132b517a7c5c5b97125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da391a0c3b0b487d9a1934ebd669ef71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69856c31a1854914b54a94aa195699aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22689260c19942758e7001c1999a9414",
              "IPY_MODEL_df58fd3422e046deb390fe92eebb61be",
              "IPY_MODEL_f537f42404134829b213c5c02a867579"
            ],
            "layout": "IPY_MODEL_463de65f3ef04f74a1e5bc45a542a094"
          }
        },
        "22689260c19942758e7001c1999a9414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1a6a7b019246e79a51d20c0cb27b68",
            "placeholder": "​",
            "style": "IPY_MODEL_c9ff85fdcfce4a458999b5c837dd387f",
            "value": "Generating (batched): 100%"
          }
        },
        "df58fd3422e046deb390fe92eebb61be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79870de742be4094b1c256d0d8d14d77",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ebe8bb4f7bc40418888bd7986e8a5ab",
            "value": 10
          }
        },
        "f537f42404134829b213c5c02a867579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a776c2bea6f8468596c2f2ea3895117c",
            "placeholder": "​",
            "style": "IPY_MODEL_9760277a03de434688d45b8f03bf187e",
            "value": " 10/10 [02:17&lt;00:00, 13.14s/it]"
          }
        },
        "463de65f3ef04f74a1e5bc45a542a094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1a6a7b019246e79a51d20c0cb27b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ff85fdcfce4a458999b5c837dd387f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79870de742be4094b1c256d0d8d14d77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ebe8bb4f7bc40418888bd7986e8a5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a776c2bea6f8468596c2f2ea3895117c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9760277a03de434688d45b8f03bf187e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# FORCE-CLEAN + Install known-compatible versions (fixes EncoderDecoderCache / trainer import)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip uninstall -y transformers sentence-transformers torch torchvision torchaudio faiss-cpu peft -q\n",
        "\n",
        "# CPU-only torch (no CUDA conflicts)\n",
        "!pip install --quiet torch==2.3.1 --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# Critical: transformers 4.41.2 → stable with sentence-transformers 3.0.1\n",
        "# Avoid anything >=4.45 or 5.x\n",
        "!pip install --quiet transformers==4.41.2 sentence-transformers==3.0.1 faiss-cpu==1.8.0\n",
        "\n",
        "print(\"Packages installed.\")\n",
        "print(\"→ Restart runtime NOW (use code below if menu is missing)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsIj_4z8ke5x",
        "outputId": "798c225f-f029-4900-8418-6c0f4d615de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.6 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.24 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mPackages installed.\n",
            "→ Restart runtime NOW (use code below if menu is missing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-gpu sentence-transformers langchain-huggingface langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brGVHLARkfyL",
        "outputId": "2080a0cc-1517-4c90-d486-87ce9ebff7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf -q"
      ],
      "metadata": {
        "id": "wlYh5F0lkf5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "qRyfTXEHklsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbRGDb6wk-b5",
        "outputId": "42195034-b27d-4cb1-d4b2-1d6a0975b74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.8)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "print(\"Ready – transformers should now import cleanly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daGQf83sklvL",
        "outputId": "539cce21-acea-4bc8-a370-035097585df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready – transformers should now import cleanly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Silence most warnings (especially from transformers & sentence-transformers)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using device: cpu\")\n",
        "print(\"Environment looks ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3Dj-3Kvklxu",
        "outputId": "250a05c4-a6e3-499a-f0e5-042bd113ea70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.3.1+cpu\n",
            "Using device: cpu\n",
            "Environment looks ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'(?i)page\\s+\\d+(\\s+of\\s+\\d+)?', '', text)\n",
        "    text = re.sub(r'\\[\\d+(?:,\\s*\\d+|-?\\d+)*\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def process_crypto_documents(doc_list):\n",
        "    document_library = {}\n",
        "    for doc_name in doc_list:\n",
        "        if not os.path.exists(doc_name):\n",
        "            print(f\"Warning: {doc_name} not found.\")\n",
        "            continue\n",
        "        print(f\"Processing: {doc_name}...\")\n",
        "        try:\n",
        "            doc = fitz.open(doc_name)\n",
        "            raw_content = \"\"\n",
        "            for page in doc:\n",
        "                raw_content += page.get_text(\"text\") + \" \"\n",
        "            cleaned = clean_text(raw_content)\n",
        "            document_library[doc_name] = cleaned\n",
        "            doc.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {doc_name}: {e}\")\n",
        "    return document_library\n",
        "\n",
        "# Your files (must be uploaded to Colab first)\n",
        "big_5_files = [\n",
        "    \"bitcoin.pdf\",\n",
        "    \"solana.pdf\",\n",
        "    \"uniswap.pdf\",\n",
        "    \"chainlink.pdf\"\n",
        "]\n",
        "\n",
        "processed_docs = process_crypto_documents(big_5_files)\n",
        "\n",
        "print(\"\\nExtraction Summary\")\n",
        "for name, content in processed_docs.items():\n",
        "    print(f\"{name}: {len(content):,} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UWH7rgtktbn",
        "outputId": "d64f8b38-c654-4e37-f4a9-02713c1eaae4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: bitcoin.pdf...\n",
            "Processing: solana.pdf...\n",
            "Processing: uniswap.pdf...\n",
            "Processing: chainlink.pdf...\n",
            "\n",
            "Extraction Summary\n",
            "bitcoin.pdf: 21,165 characters\n",
            "solana.pdf: 46,029 characters\n",
            "uniswap.pdf: 39,739 characters\n",
            "chainlink.pdf: 86,266 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_documents(processed_docs, chunk_size=512, chunk_overlap=80):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunked_library = {}\n",
        "    for doc_name, text in processed_docs.items():\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        chunked_library[doc_name] = chunks\n",
        "        print(f\"{doc_name}: {len(chunks)} chunks\")\n",
        "\n",
        "    return chunked_library\n",
        "\n",
        "final_chunks = chunk_documents(processed_docs)\n",
        "\n",
        "# Quick check\n",
        "if \"uniswap.pdf\" in final_chunks:\n",
        "    print(f\"\\nFirst Uniswap chunk length: {len(final_chunks['uniswap.pdf'][0])} chars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPFB01gjkteK",
        "outputId": "4cd3f74b-76a2-4389-ae5e-c906b445cf4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bitcoin.pdf: 51 chunks\n",
            "solana.pdf: 109 chunks\n",
            "uniswap.pdf: 98 chunks\n",
            "chainlink.pdf: 203 chunks\n",
            "\n",
            "First Uniswap chunk length: 291 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# Cell: Prepare corpus with metadata (improved – with type hints & docstring)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "def prepare_corpus(\n",
        "    chunked_docs: Dict[str, List[str]]\n",
        ") -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Flatten chunked documents into parallel lists of texts and sources.\n",
        "\n",
        "    Args:\n",
        "        chunked_docs: Dict[document_name → list_of_chunks]\n",
        "\n",
        "    Returns:\n",
        "        Tuple (texts, sources) – parallel lists for embedding & retrieval\n",
        "    \"\"\"\n",
        "    texts: List[str] = []\n",
        "    sources: List[str] = []\n",
        "\n",
        "    for doc_name, chunks in chunked_docs.items():\n",
        "        for chunk in chunks:\n",
        "            texts.append(chunk.strip())      # ensure no leading/trailing whitespace\n",
        "            sources.append(doc_name)\n",
        "\n",
        "    print(f\"Prepared corpus:\")\n",
        "    print(f\"  → {len(texts):,} chunks\")\n",
        "    print(f\"  → Documents: {sorted(set(sources))}\")\n",
        "    print(f\"  → Avg chunk length: {sum(len(t) for t in texts) / len(texts):.0f} chars\")\n",
        "\n",
        "    if not texts:\n",
        "        raise ValueError(\"No chunks found – check chunk_documents step\")\n",
        "\n",
        "    return texts, sources\n",
        "\n",
        "\n",
        "# ─── Execute ────────────────────────────────────────────────────────────\n",
        "texts, sources = prepare_corpus(final_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcEyQKcclFRv",
        "outputId": "7f0bee6b-24d7-47ba-8faf-8469ebcd7e1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared corpus:\n",
            "  → 461 chunks\n",
            "  → Documents: ['bitcoin.pdf', 'chainlink.pdf', 'solana.pdf', 'uniswap.pdf']\n",
            "  → Avg chunk length: 435 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "print(f\"Loading {model_name} on CPU...\")\n",
        "embedder = SentenceTransformer(model_name, device=\"cpu\")\n",
        "print(f\"Success! Dimension: {embedder.get_sentence_embedding_dimension()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lwcW_UYktg0",
        "outputId": "7232bdaf-1259-4e5d-d439-bd946bfa30e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BAAI/bge-small-en-v1.5 on CPU...\n",
            "Success! Dimension: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# Cell: Generate normalized embeddings (robust batching + progress)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def compute_embeddings(\n",
        "    texts: List[str],\n",
        "    model: SentenceTransformer,\n",
        "    batch_size: int = 16,\n",
        "    normalize: bool = True\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute sentence embeddings with progress bar and safe batching.\n",
        "\n",
        "    Args:\n",
        "        texts: List of text chunks\n",
        "        model: Loaded SentenceTransformer\n",
        "        batch_size: Adjust lower if CPU memory is tight\n",
        "        normalize: L2-normalize (required for cosine sim with IndexFlatIP)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: shape (n_chunks, embedding_dim), dtype float32\n",
        "    \"\"\"\n",
        "    print(f\"Computing embeddings ({len(texts):,} texts) ...\")\n",
        "\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_emb = model.encode(\n",
        "            batch,\n",
        "            batch_size=len(batch),\n",
        "            show_progress_bar=False,\n",
        "            normalize_embeddings=normalize,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "        embeddings_list.append(batch_emb)\n",
        "\n",
        "    embeddings = np.vstack(embeddings_list).astype(np.float32)\n",
        "\n",
        "    print(f\"Embeddings ready → shape {embeddings.shape}\")\n",
        "    print(f\"  → Norm of first vector: {np.linalg.norm(embeddings[0]):.4f} (should ≈1.0)\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# ─── Execute ────────────────────────────────────────────────────────────\n",
        "embeddings = compute_embeddings(texts, embedder, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "32b23485638d45078db7bd7fdf26f370",
            "ee59b02a8feb43bb99060167b71cfe91",
            "b481b3c0fec345f998bffdeac572962f",
            "4839d82c148e40e3830cce049707e980",
            "7a9fee3d12dd417d9e9a1ca79481487b",
            "d4cf14a7e923402eb360aa196d8c900f",
            "36e09165613e497fbb1e40a3c4da5ccd",
            "518cfb66e0034dc8839666494f6ba928",
            "309aa3b4c0fa4cdda986375b88c0ba1f",
            "9daedd40755a4a49a5d67e5531991a4a",
            "2632d52f7fab4fc4ac725ccbbd052b6c"
          ]
        },
        "id": "RzIX4LSqlIxO",
        "outputId": "124abe07-15b2-4d3b-bf70-aed149c42d62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing embeddings (461 texts) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Embedding batches:   0%|          | 0/29 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32b23485638d45078db7bd7fdf26f370"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings ready → shape (461, 384)\n",
            "  → Norm of first vector: 1.0000 (should ≈1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Force-downgrade NumPy to latest 1.x series (1.26.4 is very stable and widely compatible)\n",
        "!pip uninstall -y numpy\n",
        "!pip install \"numpy<2\" --quiet   # or specifically: numpy==1.26.4\n",
        "\n",
        "# Step 2: Re-install faiss-cpu (after NumPy downgrade, so it links correctly)\n",
        "!pip uninstall -y faiss-cpu\n",
        "!pip install faiss-cpu --quiet\n",
        "\n",
        "print(\"NumPy and faiss reinstalled. Now restart the runtime.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ0LogYSlnkd",
        "outputId": "58aae9e7-aecd-4c4c-e647-3ae928692f9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.6 requires torchvision>=0.11, which is not installed.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mFound existing installation: faiss-cpu 1.13.2\n",
            "Uninstalling faiss-cpu-1.13.2:\n",
            "  Successfully uninstalled faiss-cpu-1.13.2\n",
            "NumPy and faiss reinstalled. Now restart the runtime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")      # Should be 1.26.x\n",
        "print(f\"faiss version: {faiss.__version__}\")   # Should show without crash\n",
        "print(\"faiss import successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oECUk4zMlure",
        "outputId": "0b3bf6d2-95c2-4aa1-8c05-da2f2c0c0f3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "faiss version: 1.13.2\n",
            "faiss import successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# The embeddings variable should already exist from the previous cell\n",
        "# If not → re-run the embedding cell first\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)          # Inner Product = cosine similarity (because vectors are normalized)\n",
        "\n",
        "index.add(embeddings.astype(np.float32))      # FAISS expects float32\n",
        "\n",
        "print(\"FAISS index successfully created\")\n",
        "print(f\"  → number of vectors: {index.ntotal:,}\")\n",
        "print(f\"  → dimensionality:    {dimension}\")\n",
        "print(f\"  → index type:        {index.__class__.__name__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtt2svv_l475",
        "outputId": "d6fc3fd1-480a-4ee5-9ad4-a5203faaf984"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index successfully created\n",
            "  → number of vectors: 461\n",
            "  → dimensionality:    384\n",
            "  → index type:        IndexFlatIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(\n",
        "    query: str,\n",
        "    k: int = 4,\n",
        "    min_score: float = 0.35\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar chunks for a given query.\n",
        "    Returns list of dicts with score, source document and text preview.\n",
        "    \"\"\"\n",
        "    # Embed the query (same model & normalization as the corpus)\n",
        "    q_embedding = embedder.encode(\n",
        "        [query],\n",
        "        normalize_embeddings=True,\n",
        "        convert_to_numpy=True\n",
        "    ).astype('float32')\n",
        "\n",
        "    # Search the index\n",
        "    distances, indices = index.search(q_embedding, k)\n",
        "\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        if idx == -1:  # no more results\n",
        "            continue\n",
        "        score = float(dist)  # cosine similarity (higher = better)\n",
        "        if score < min_score:\n",
        "            continue\n",
        "        results.append({\n",
        "            'rank': len(results) + 1,\n",
        "            'score': round(score, 3),\n",
        "            'source': sources[idx],\n",
        "            'text_preview': texts[idx][:180].replace('\\n', ' ').strip() + \"...\"\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ─── Test with meaningful crypto questions ───────────────────────────────\n",
        "\n",
        "test_questions = [\n",
        "    \"What is Solana's Proof of History and why does it matter?\",\n",
        "    \"How does Uniswap v3 concentrated liquidity work?\",\n",
        "    \"What problem does Chainlink solve for smart contracts?\",\n",
        "    \"According to the Bitcoin whitepaper, what is double-spending and how is it prevented?\",\n",
        "    \"What are the key differences between Bitcoin and traditional electronic cash systems?\"\n",
        "]\n",
        "\n",
        "print(\"RAG Retrieval Test Results\\n\" + \"═\"*65 + \"\\n\")\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"Query: {question}\")\n",
        "    hits = retrieve(question, k=4, min_score=0.35)\n",
        "\n",
        "    if not hits:\n",
        "        print(\"  → No matches above min_score threshold\\n\")\n",
        "        continue\n",
        "\n",
        "    for hit in hits:\n",
        "        print(f\"  {hit['rank']}. {hit['score']:>5.3f}   {hit['source']}\")\n",
        "        print(f\"     {hit['text_preview']}\\n\")\n",
        "\n",
        "    print(\"─\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWo3hPLtl7vQ",
        "outputId": "68c82093-44a3-4990-f7ab-af7980a2448f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Retrieval Test Results\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "Query: What is Solana's Proof of History and why does it matter?\n",
            "  1. 0.707   solana.pdf\n",
            "     . That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architecture based on Proof of History (PoH) - a proof for verifying order and passage of t...\n",
            "\n",
            "  2. 0.703   solana.pdf\n",
            "     . Elections for the proposed PoS algorithm are covered in depth in Section 5.6. In terms of CAP theorem, Consistency is almost always picked over Avail- ability in an event of a Pa...\n",
            "\n",
            "  3. 0.684   solana.pdf\n",
            "     . The Leader would then censor the Byzantine bond holders from participating. Proof of History generator would have to continue generating a sequence, to prove the passage of time,...\n",
            "\n",
            "  4. 0.669   solana.pdf\n",
            "     . This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up in history length. Additionally, a single source...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: How does Uniswap v3 concentrated liquidity work?\n",
            "  1. 0.870   uniswap.pdf\n",
            "     . 2 CONCENTRATED LIQUIDITY The defining idea of Uniswap v3 is that of concentrated liquidity: liquidity bounded within some price range. In earlier versions, liquidity was distribu...\n",
            "\n",
            "  2. 0.849   uniswap.pdf\n",
            "     . Uniswap v3 is based on the same constant product reserves curve as earlier versions , but offers several significant new features: • Concentrated Liquidity: Liquidity providers (...\n",
            "\n",
            "  3. 0.814   uniswap.pdf\n",
            "     . In this paper, we present Uniswap v3, a novel AMM that gives liquidity providers more control over the price ranges in which their capital is used, with limited effect on liquidi...\n",
            "\n",
            "  4. 0.808   uniswap.pdf\n",
            "     . Finally, Uniswap v3 adds a liquidity accumulator that is tracked alongside the price accumulator, which accumulates 1 𝐿for each second. This liquidity accumulator is useful for e...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: What problem does Chainlink solve for smart contracts?\n",
            "  1. 0.820   chainlink.pdf\n",
            "     . Because of the mechanics of the consensus mechanisms underpinning blockchains, a blockchain cannot directly fetch such critical data. We propose a solution to the smart contract...\n",
            "\n",
            "  2. 0.807   chainlink.pdf\n",
            "     . 8 Conclusion We have introduced ChainLink, a decentralized oracle network for smart contracts to securely interact with resources external to the blockchain. We have outlined the...\n",
            "\n",
            "  3. 0.799   chainlink.pdf\n",
            "     . In order for a smart contract on networks like Ethereum to use a ChainLink node, they will need to pay their chosen ChainLink Node Operator using LINK tokens, with prices being s...\n",
            "\n",
            "  4. 0.781   chainlink.pdf\n",
            "     . We believe that the current focus on tokens to the exclusion of many other possible applications is due to a lack of adequate oracle services, a situation ChainLink speciﬁcally a...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: According to the Bitcoin whitepaper, what is double-spending and how is it prevented?\n",
            "  1. 0.818   bitcoin.pdf\n",
            "     . A common solution is to introduce a trusted central authority, or mint, that checks every transaction for double spending. After each transaction, the coin must be returned to th...\n",
            "\n",
            "  2. 0.776   bitcoin.pdf\n",
            "     Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.com www.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payme...\n",
            "\n",
            "  3. 0.763   bitcoin.pdf\n",
            "     . Once a predetermined number of coins have entered circulation, the incentive can transition entirely to transaction fees and be completely inflation free. The incentive may help...\n",
            "\n",
            "  4. 0.748   bitcoin.pdf\n",
            "     . The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity t...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Query: What are the key differences between Bitcoin and traditional electronic cash systems?\n",
            "  1. 0.803   bitcoin.pdf\n",
            "     Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.com www.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payme...\n",
            "\n",
            "  2. 0.737   bitcoin.pdf\n",
            "     . 1 2. Transactions We define an electronic coin as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction...\n",
            "\n",
            "  3. 0.713   bitcoin.pdf\n",
            "     .15 z=8 q=0.20 z=11 q=0.25 z=15 q=0.30 z=24 q=0.35 z=41 q=0.40 z=89 q=0.45 z=340 12. Conclusion We have proposed a system for electronic transactions without relying on trust. We s...\n",
            "\n",
            "  4. 0.706   bitcoin.pdf\n",
            "     . The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity t...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "\n",
        "np.save(\"crypto_embeddings.npy\", embeddings)\n",
        "faiss.write_index(index, \"crypto_rag_index.faiss\")\n",
        "\n",
        "with open(\"crypto_corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"texts\": texts, \"sources\": sources}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved: embeddings, index, corpus metadata\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4IPFv-yl-uq",
        "outputId": "f0595805-e3e1-4ae5-d666-3dc403c2a917"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: embeddings, index, corpus metadata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "qs = [\n",
        "    \"What is Solana's Proof of History?\",\n",
        "    \"How does Uniswap v3 concentrated liquidity work?\",\n",
        "    \"What problem does Chainlink solve?\",\n",
        "    \"Bitcoin double-spending prevention\",\n",
        "] * 5   # 20 queries\n",
        "\n",
        "times = []\n",
        "for q in qs:\n",
        "    t0 = time.time()\n",
        "    _ = retrieve(q)\n",
        "    times.append(time.time() - t0)\n",
        "\n",
        "print(f\"Average query time: {np.mean(times)*1000:.1f} ms\")\n",
        "print(f\"Median:             {np.median(times)*1000:.1f} ms\")\n",
        "print(f\"Min / Max:          {np.min(times)*1000:.1f} – {np.max(times)*1000:.1f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPKvwHJEl_iz",
        "outputId": "409e30d0-bb51-4f81-b843-eec2c5d5cbc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average query time: 66.7 ms\n",
            "Median:             62.7 ms\n",
            "Min / Max:          47.9 – 122.6 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "qs = [\n",
        "    \"What is Solana's Proof of History?\",\n",
        "    \"How does Uniswap v3 concentrated liquidity work?\",\n",
        "    \"What problem does Chainlink solve?\",\n",
        "    \"Bitcoin double-spending prevention\",\n",
        "] * 5   # 20 queries\n",
        "\n",
        "times = []\n",
        "for q in qs:\n",
        "    t0 = time.time()\n",
        "    _ = retrieve(q)\n",
        "    times.append(time.time() - t0)\n",
        "\n",
        "print(f\"Average query time: {np.mean(times)*1000:.1f} ms\")\n",
        "print(f\"Median:             {np.median(times)*1000:.1f} ms\")\n",
        "print(f\"Min / Max:          {np.min(times)*1000:.1f} – {np.max(times)*1000:.1f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNeI2GTgmD7F",
        "outputId": "052c2437-62fb-4260-f152-0faf05783fb6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average query time: 63.8 ms\n",
            "Median:             58.0 ms\n",
            "Min / Max:          36.5 – 115.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# === Bitcoin pairs (10) ===\n",
        "bitcoin_pairs = [\n",
        "    {\n",
        "        \"question\": \"How is an electronic coin defined within the Bitcoin system?\",\n",
        "        \"ground_truth_excerpt\": \"An electronic coin is defined as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the primary purpose of the Proof-of-Work system in the Bitcoin network?\",\n",
        "        \"ground_truth_excerpt\": \"The proof-of-work system is used to implement a distributed timestamp server on a peer-to-peer basis and to solve the problem of determining representation in majority decision making (one-CPU-one-vote).\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the network handle a situation where two nodes broadcast different versions of the next block simultaneously?\",\n",
        "        \"ground_truth_excerpt\": \"Nodes work on the first version they receive but save the other branch. The tie is broken when the next proof-of-work is found and one branch becomes longer; nodes then switch to the longer chain.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the two components that fund the incentive for nodes to support the network?\",\n",
        "        \"ground_truth_excerpt\": \"The incentive is funded by a special first transaction in a block that starts a new coin owned by the block creator, and by transaction fees (the difference between transaction input and output values).\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can disk space be reclaimed without breaking a block's hash?\",\n",
        "        \"ground_truth_excerpt\": \"Transactions are hashed in a Merkle Tree, with only the root included in the block's hash. Old blocks can be compacted by stubbing off branches of the tree and discarding spent transactions.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is 'Simplified Payment Verification' (SPV) and what does a user need to maintain it?\",\n",
        "        \"ground_truth_excerpt\": \"SPV allows a user to verify payments without running a full node. The user only needs to keep a copy of the block headers of the longest proof-of-work chain and obtain the Merkle branch linking the transaction to its block.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 5,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the Bitcoin privacy model differ from the traditional banking model?\",\n",
        "        \"ground_truth_excerpt\": \"The traditional model limits information to the parties involved and a trusted third party. Bitcoin's model announces all transactions publicly but maintains privacy by keeping public keys anonymous.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 6,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What specific attack can a dishonest sender attempt if they control significant CPU power?\",\n",
        "        \"ground_truth_excerpt\": \"An attacker can only try to change one of his own transactions to take back money he recently spent by generating an alternate chain faster than the honest chain.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 6,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How is the proof-of-work difficulty adjusted over time?\",\n",
        "        \"ground_truth_excerpt\": \"The difficulty is determined by a moving average targeting an average number of blocks per hour. If blocks are generated too fast, the difficulty increases.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the requirement for the network to remain secure against attacker nodes?\",\n",
        "        \"ground_truth_excerpt\": \"The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.\",\n",
        "        \"source\": \"bitcoin.pdf\",\n",
        "        \"page\": 1,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# === Chainlink pairs (10) ===\n",
        "chainlink_pairs = [\n",
        "    {\n",
        "        \"question\": \"What are the two primary functions of the Chainlink core node software?\",\n",
        "        \"ground_truth_excerpt\": \"The core node software is responsible for interfacing with the blockchain, scheduling, and balancing work across its various external services.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 7,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does Chainlink define the concept of 'External Adapters' in its off-chain architecture?\",\n",
        "        \"ground_truth_excerpt\": \"Adapters are external services with a minimal REST API. By modeling adapters in a service-oriented manner, programs in any programming language can be easily implemented simply by adding a small intermediate API in front of the program.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 7,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the paper, what is the 'freeloading' problem in oracle networks?\",\n",
        "        \"ground_truth_excerpt\": \"A cheating oracle Oz can observe the response Ai of another oracle Oi and copy it. In this way, oracle Oz avoids the expense of querying data sources, which may charge per-query fees.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 13,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Explain the commit/reveal scheme used in the In-Contract Aggregation protocol.\",\n",
        "        \"ground_truth_excerpt\": \"In a first round, oracles send CHAINLINK-SC cryptographic commitments to their responses. After CHAINLINK-SC has received a quorum of responses, it initiates a second round in which oracles reveal their responses.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 13,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the primary disadvantage of in-contract aggregation that the paper identifies?\",\n",
        "        \"ground_truth_excerpt\": \"In-contract aggregation has a key disadvantage: Cost. It incurs the cost of transmitting and processing on chain O(n) oracle messages (commits and reveals for A1, A2, . . . , An).\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the proposed use of threshold signatures improve off-chain aggregation?\",\n",
        "        \"ground_truth_excerpt\": \"Partial signatures on the same value A can be aggregated across any set of t oracles to yield a single valid collective signature. This approach allows CHAINLINK-SC to obtain an aggregate answer without needing to receive answers from multiple oracles.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What security properties does Intel SGX provide to Chainlink enclaves?\",\n",
        "        \"ground_truth_excerpt\": \"First, enclaves protect the integrity of the application, meaning its data, code, and control flow, against subversion by other processes. Second, an enclave protects the confidentiality of an application, meaning that its data, code, and execution state are opaque to other processes.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 22,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Describe the function of the MIGFLAG in the proposed Contract-Upgrade Service.\",\n",
        "        \"ground_truth_excerpt\": \"CHAINLINK-SC would support a flag (MIGFLAG) in oracle calls from requesting contracts indicating whether or not a call should be forwarded to a new CHAINLINK-SC should one become available.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 20,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the purpose of the Chainlink Certification Service?\",\n",
        "        \"ground_truth_excerpt\": \"The Certification Service is planned as a means to identify Sybil attacks and other malfeasance that automated on-chain systems cannot.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 20,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the paper suggest dealing with potential correlations between different data sources?\",\n",
        "        \"ground_truth_excerpt\": \"Chainlink also proposes to pursue research into mapping and reporting the independence of data sources in an easily digestible way so that oracles and users can avoid undesired correlations.\",\n",
        "        \"source\": \"chainlink.pdf\",\n",
        "        \"page\": 11,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "import json\n",
        "\n",
        "# === Solana pairs ===\n",
        "solana_pairs = [\n",
        "    {\n",
        "        \"question\": \"What is the primary function of Proof of History (PoH) in the Solana architecture?\",\n",
        "        \"ground_truth_excerpt\": \"Proof of History is a sequence of computation that can provide a way to cryptographically verify passage of time between two events. PoH is used to encode trustless passage of time into a ledger an append only data structure.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 1,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does Proof of History allow for parallel verification by external computers?\",\n",
        "        \"ground_truth_excerpt\": \"The output can then be re-computed and verified by external computers in parallel by checking each sequence segment on a separate core. Given some number of cores... the verifier can split up the sequence of hashes and their indexes into 4000 slices, and in parallel make sure that each slice is correct.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the paper, how are events 'timestamped' into the PoH sequence?\",\n",
        "        \"ground_truth_excerpt\": \"Data can be timestamped into this sequence by appending the data (or a hash of some data) into the state of the function. The recording of the state, index and data as it was appended into the sequences provides a timestamp that can guarantee that the data was created sometime before the next hash was generated.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What role do 'Verifiers' play in the Solana network design?\",\n",
        "        \"ground_truth_excerpt\": \"Verifiers execute the same transactions on their copies of the state, and publish their computed signatures of the state as confirmations. The published confirmations serve as votes for the consensus algorithm.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Describe how horizontal scaling is achieved for PoH generators without sharding.\",\n",
        "        \"ground_truth_excerpt\": \"Its possible to synchronize multiple Proof of History generators by mixing the sequence state from each generator to each other generator. This property can be transitive... we can trace the dependency between A and C even though they were not synchronized directly.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 9,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does Proof of History protect the network against long-range attacks?\",\n",
        "        \"ground_truth_excerpt\": \"A malicious user that gains access to old private keys would have to recreate a historical record that takes as much time as the original one they are trying to forge. This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 13,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What constitutes a 'super majority' in the Solana Proof of Stake consensus?\",\n",
        "        \"ground_truth_excerpt\": \"A super majority is 2/3rds of the validators weighted by their bonds. A super majority vote indicates that the network has reached consensus.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the network handle the 'nothing at stake' problem in its Proof of Stake system?\",\n",
        "        \"ground_truth_excerpt\": \"Slashing is the proposed solution... When a proof of voting for a different branch is published, that branch can destroy the validators bond. This is an economic incentive designed to discourage validators from confirming multiple branches.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 14,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What mechanism allows the network to recover from a large partition where more than 1/2 of verifiers are missing?\",\n",
        "        \"ground_truth_excerpt\": \"In a large partition... the unstaking process is very very slow. Full 2/3rds consensus will not be achieved until a very large amount of hashes have been generated and the unavailable verifiers have been unstaked.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 17,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How is a new Leader elected if the current Proof of History generator fails?\",\n",
        "        \"ground_truth_excerpt\": \"Election for a new PoH generator occur when the PoH generator failure is detected. The validator with the largest voting power, or highest public key address if there is a tie is picked as the new PoH generator.\",\n",
        "        \"source\": \"solana.pdf\",\n",
        "        \"page\": 15,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# === Uniswap pairs ===\n",
        "uniswap_pairs = [\n",
        "    {\n",
        "        \"question\": \"How is 'concentrated liquidity' defined in Uniswap v3, and how does it differ from the liquidity distribution in earlier versions?\",\n",
        "        \"ground_truth_excerpt\": \"The defining idea of UNISWAP V3 is that of concentrated liquidity: liquidity bounded within some price range. In earlier versions, liquidity was distributed uniformly along the x * y = k reserves curve... designed to provide liquidity across the entire price range (0, infinity).\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are 'virtual reserves' in the context of a concentrated liquidity position?\",\n",
        "        \"ground_truth_excerpt\": \"A position only needs to maintain enough reserves to support trading within its range, and therefore can act like a constant product pool with larger reserves (we call these the virtual reserves) within that range.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the paper, what happens to a position's liquidity and fee earnings when the price exits its specified range?\",\n",
        "        \"ground_truth_excerpt\": \"When the price exits a position's range, the position's liquidity is no longer active, and no longer earns fees. At that point, its liquidity is composed entirely of a single asset.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are 'range orders' in Uniswap v3 and how do they relate to traditional limit orders?\",\n",
        "        \"ground_truth_excerpt\": \"Positions on very small ranges act similarly to limit orders—if the range is crossed, the position flips from being composed entirely of one asset, to being composed entirely of the other asset (plus accrued fees).\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 2,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How are swap fees handled differently in Uniswap v3 compared to v2 regarding compounding?\",\n",
        "        \"ground_truth_excerpt\": \"Fees earned in earlier versions were continuously deposited in the pool as liquidity... that fee earnings compounded. In UNISWAP V3, due to the non-fungible nature of positions, this is no longer possible. Instead, fee earnings are stored separately and held as the tokens in which the fees are paid.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 3,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What mechanism does Uniswap v3 use to track prices in its oracle, and why was this change made?\",\n",
        "        \"ground_truth_excerpt\": \"Instead of accumulating the sum of prices, allowing users to compute the arithmetic mean TWAP, UNISWAP v3 tracks the sum of log prices, allowing users to compute the geometric mean TWAP. Using the time-weighted geometric mean price... avoids the need to track separate accumulators for these ratios.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the purpose of the 'liquidity accumulator' introduced in the Uniswap v3 oracle?\",\n",
        "        \"ground_truth_excerpt\": \"This liquidity accumulator is useful for external contracts that want to implement liquidity mining on top of Uniswap v3. It can also be used by other contracts to inform a decision on which of the pools corresponding to a pair... will have the most reliable TWAP.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 4,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How are 'ticks' used to demarcate price space in Uniswap v3?\",\n",
        "        \"ground_truth_excerpt\": \"To implement custom liquidity provision, the space of possible prices is demarcated by discrete ticks. Liquidity providers can provide liquidity in a range between any two ticks... Conceptually, there is a tick at every price p that is an integer power of 1.0001.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 5,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What determines which ticks can be 'initialized' for a liquidity position?\",\n",
        "        \"ground_truth_excerpt\": \"Not every tick can be initialized. The pool is instantiated with a parameter, tickSpacing (ts); only ticks with indexes that are divisible by tickSpacing can be initialized.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 5,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Why does Uniswap v3 track the square root of price (sqrtPrice) and liquidity (L) instead of virtual reserves (x and y)?\",\n",
        "        \"ground_truth_excerpt\": \"Using L and sqrtP is convenient because only one of them changes at a time. Price (and thus sqrtP) changes when swapping within a tick; liquidity changes when crossing a tick... This avoids some rounding errors that could be encountered if tracking virtual reserves.\",\n",
        "        \"source\": \"uniswap.pdf\",\n",
        "        \"page\": 6,\n",
        "        \"type\": \"factual-extraction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Solana pairs: {len(solana_pairs)}\")\n",
        "print(f\"Uniswap pairs: {len(uniswap_pairs)}\")\n",
        "\n",
        "print(\"All lists defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKcnUKI2mHcA",
        "outputId": "31122e42-e370-443c-a24a-98be9b280deb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solana pairs: 10\n",
            "Uniswap pairs: 10\n",
            "All lists defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine everything into one list (exactly 40 pairs)\n",
        "full_eval_dataset = (\n",
        "    bitcoin_pairs +\n",
        "    chainlink_pairs +\n",
        "    solana_pairs +\n",
        "    uniswap_pairs\n",
        ")\n",
        "\n",
        "# Verify count\n",
        "assert len(full_eval_dataset) == 40, f\"Expected 40 pairs, got {len(full_eval_dataset)}\"\n",
        "\n",
        "# Optional: shuffle for random order\n",
        "random.seed(42)  # reproducible shuffle\n",
        "random.shuffle(full_eval_dataset)\n",
        "\n",
        "# Save – this overwrites any previous file\n",
        "output_file = \"crypto_rag_eval_dataset.json\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(full_eval_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Dataset saved: {output_file}\")\n",
        "print(f\"Total pairs: {len(full_eval_dataset)}\")\n",
        "print(f\"Sources: {sorted(set(d['source'] for d in full_eval_dataset))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqrMoKblmMLd",
        "outputId": "6caf27f9-c2fd-49c5-9877-31044b1a3b4e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved: crypto_rag_eval_dataset.json\n",
            "Total pairs: 40\n",
            "Sources: ['bitcoin.pdf', 'chainlink.pdf', 'solana.pdf', 'uniswap.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distribution by source\n",
        "from collections import Counter\n",
        "print(\"Count per source:\")\n",
        "print(Counter(d['source'] for d in full_eval_dataset))\n",
        "\n",
        "# Show first 3 and last 3 for sanity check\n",
        "print(\"\\nFirst 3:\")\n",
        "for item in full_eval_dataset[:3]:\n",
        "    print(json.dumps(item, indent=2))\n",
        "    print(\"---\")\n",
        "\n",
        "print(\"\\nLast 3:\")\n",
        "for item in full_eval_dataset[-3:]:\n",
        "    print(json.dumps(item, indent=2))\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7a2_27nmROV",
        "outputId": "315d119e-0bb2-40ae-ee8d-e68c1625841f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count per source:\n",
            "Counter({'bitcoin.pdf': 10, 'chainlink.pdf': 10, 'uniswap.pdf': 10, 'solana.pdf': 10})\n",
            "\n",
            "First 3:\n",
            "{\n",
            "  \"question\": \"What is the requirement for the network to remain secure against attacker nodes?\",\n",
            "  \"ground_truth_excerpt\": \"The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 1,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What are the two components that fund the incentive for nodes to support the network?\",\n",
            "  \"ground_truth_excerpt\": \"The incentive is funded by a special first transaction in a block that starts a new coin owned by the block creator, and by transaction fees (the difference between transaction input and output values).\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 4,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What are the two primary functions of the Chainlink core node software?\",\n",
            "  \"ground_truth_excerpt\": \"The core node software is responsible for interfacing with the blockchain, scheduling, and balancing work across its various external services.\",\n",
            "  \"source\": \"chainlink.pdf\",\n",
            "  \"page\": 7,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "\n",
            "Last 3:\n",
            "{\n",
            "  \"question\": \"Describe the function of the MIGFLAG in the proposed Contract-Upgrade Service.\",\n",
            "  \"ground_truth_excerpt\": \"CHAINLINK-SC would support a flag (MIGFLAG) in oracle calls from requesting contracts indicating whether or not a call should be forwarded to a new CHAINLINK-SC should one become available.\",\n",
            "  \"source\": \"chainlink.pdf\",\n",
            "  \"page\": 20,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What is the primary purpose of the Proof-of-Work system in the Bitcoin network?\",\n",
            "  \"ground_truth_excerpt\": \"The proof-of-work system is used to implement a distributed timestamp server on a peer-to-peer basis and to solve the problem of determining representation in majority decision making (one-CPU-one-vote).\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 3,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n",
            "{\n",
            "  \"question\": \"What specific attack can a dishonest sender attempt if they control significant CPU power?\",\n",
            "  \"ground_truth_excerpt\": \"An attacker can only try to change one of his own transactions to take back money he recently spent by generating an alternate chain faster than the honest chain.\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 6,\n",
            "  \"type\": \"factual-extraction\"\n",
            "}\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming retrieve() is already defined and working\n",
        "\n",
        "updated_dataset = []\n",
        "\n",
        "for item in full_eval_dataset:  # your list of 40 items\n",
        "    q = item[\"question\"]\n",
        "    hits = retrieve(q, k=1, min_score=0.0)  # top-1, even low score\n",
        "\n",
        "    retrieved_score = hits[0][\"score\"] if hits else None\n",
        "    retrieved_text_preview = hits[0][\"text_preview\"] if hits else None  # <-- FIXED HERE\n",
        "\n",
        "    new_item = item.copy()\n",
        "    new_item[\"retrieved_score\"] = round(retrieved_score, 3) if retrieved_score is not None else None\n",
        "    new_item[\"retrieved_text_preview\"] = retrieved_text_preview\n",
        "\n",
        "    updated_dataset.append(new_item)\n",
        "\n",
        "# Re-save the enriched JSON\n",
        "with open(\"crypto_rag_eval_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(updated_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"All 40 questions processed. Scores and previews added to JSON.\")\n",
        "print(\"Example (first item):\")\n",
        "print(json.dumps(updated_dataset[0], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R00Y0093mSPN",
        "outputId": "80bbd38d-99b6-4ce9-b8a0-7c575d62c451"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 40 questions processed. Scores and previews added to JSON.\n",
            "Example (first item):\n",
            "{\n",
            "  \"question\": \"What is the requirement for the network to remain secure against attacker nodes?\",\n",
            "  \"ground_truth_excerpt\": \"The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.\",\n",
            "  \"source\": \"bitcoin.pdf\",\n",
            "  \"page\": 1,\n",
            "  \"type\": \"factual-extraction\",\n",
            "  \"retrieved_score\": 0.782,\n",
            "  \"retrieved_text_preview\": \". As such, the verification is reliable as long as honest nodes control the network, but is more vulnerable if the network is overpowered by an attacker. While network nodes can ve...\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages - quiet mode to reduce output noise\n",
        "!pip install -q --upgrade \\\n",
        "    transformers==4.44.2 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    accelerate==0.33.0 \\\n",
        "    peft==0.12.0 \\\n",
        "    datasets==2.21.0 \\\n",
        "    rouge-score==0.1.2 \\\n",
        "    nltk==3.8.1 \\\n",
        "    tabulate==0.9.0 \\\n",
        "    scikit-learn==1.5.1\n",
        "\n",
        "print(\"Installs finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj4rNuJHyL30",
        "outputId": "94c60cb8-eab0-4cd0-e732-c28c8d7a2751"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m971.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.6 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.24 requires torchvision, which is not installed.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalls finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Metrics\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Imports successful.\")\n",
        "\n",
        "# Quick checks that core pieces from your previous notebook are still alive\n",
        "print(f\"FAISS index exists? → { 'index' in globals() }\")\n",
        "print(f\"retrieve() function exists? → { 'retrieve' in globals() }\")\n",
        "print(f\"Evaluation dataset exists? → { 'full_eval_dataset' in globals() or 'updated_dataset' in globals() }\")\n",
        "\n",
        "try:\n",
        "    print(f\"Number of eval items: {len(updated_dataset) if 'updated_dataset' in globals() else len(full_eval_dataset)}\")\n",
        "except:\n",
        "    print(\"Warning: eval dataset variable not found — load it now if needed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkRNkg6-yNWg",
        "outputId": "2af68d1f-5f1b-40a2-a224-f2e1d2fadfb2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports successful.\n",
            "FAISS index exists? → True\n",
            "retrieve() function exists? → True\n",
            "Evaluation dataset exists? → True\n",
            "Number of eval items: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall current version first to avoid conflicts\n",
        "!pip uninstall -y transformers accelerate bitsandbytes\n",
        "\n",
        "# Install known-good versions (Feb 2026 compatible, avoids speech_to_text_2 import crash)\n",
        "!pip install -q \\\n",
        "    transformers==4.37.2 \\\n",
        "    accelerate==0.27.2 \\\n",
        "    bitsandbytes==0.43.0\n",
        "\n",
        "print(\"Clean install of stable transformers + accelerate finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL-5gjbyzO2N",
        "outputId": "1a724c32-c09c-41da-8510-b63426a1cc98"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.44.2\n",
            "Uninstalling transformers-4.44.2:\n",
            "  Successfully uninstalled transformers-4.44.2\n",
            "Found existing installation: accelerate 0.33.0\n",
            "Uninstalling accelerate-0.33.0:\n",
            "  Successfully uninstalled accelerate-0.33.0\n",
            "Found existing installation: bitsandbytes 0.43.3\n",
            "Uninstalling bitsandbytes-0.43.3:\n",
            "  Successfully uninstalled bitsandbytes-0.43.3\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hClean install of stable transformers + accelerate finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "WzDKsiz1zvjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading distilgpt2 with stable transformers version...\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"distilgpt2\",\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float32,  # safe & fast on CPU\n",
        "    )\n",
        "    # Fix pad token (very common with gpt2 variants)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    print(\"DistilGPT2 loaded SUCCESSFULLY.\")\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "except Exception as e:\n",
        "    print(\"Still failed:\", str(e))\n",
        "    print(\"\\nIf still broken → try transformers==4.36.2 instead (rerun install cell with that version)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "13fe55e29a694274a78ead8144ca6398",
            "a5dea46bd8af4f1b8b017aa6aad98d8c",
            "a13427b77e344b008a318ab2dc8bc9ab",
            "9983acc6b57a4a6d8deaab75e1dfccdc",
            "1332351074a94020aa88ff720dd549b4",
            "e5eea711f74c4abeb3ab47903b77931a",
            "befc9a0f768a4ecc9e3400668e356bce",
            "ce28a9a35d214a11bc91ded1c7a948a1",
            "c28745db68264a9b9706df0b8862c0ce",
            "0b41bb03ac9d4578bed5a2dbb74b2f4a",
            "6bfa0d8b99db4190ae0949b5072da424",
            "4eb96ac29ec540a6b860b594904f68b9",
            "9cc3b770cd804aaf90ec5f6450b31ac0",
            "8955a8f810de4eca97d668342ca2db8d",
            "c0ceb5b35766424f9187591fb8242fcd",
            "993c322828114cfa8342b470688f03ab",
            "f06fdee97cec4983ba4d0c7d437facca",
            "bf1b8e5ab0294450b2cd73062e06635c",
            "66a2829b10764d2caec74e59f3778843",
            "40c5b18d7e2d4ac498391e3ba4e6193f",
            "d60fbd6fd7134132b517a7c5c5b97125",
            "da391a0c3b0b487d9a1934ebd669ef71"
          ]
        },
        "id": "ziDOT8uez-_w",
        "outputId": "5b5aaa35-712c-4fde-dbd4-af3fe63d7f2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading distilgpt2 with stable transformers version...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13fe55e29a694274a78ead8144ca6398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eb96ac29ec540a6b860b594904f68b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilGPT2 loaded SUCCESSFULLY.\n",
            "Model device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your eval dataset variable (updated_dataset or full_eval_dataset)\n",
        "eval_data = updated_dataset if 'updated_dataset' in globals() else full_eval_dataset\n",
        "print(f\"Generating for {len(eval_data)} questions...\")\n",
        "\n",
        "rag_results = []\n",
        "\n",
        "# Settings optimized for CPU speed & short answers\n",
        "MAX_NEW_TOKENS = 96          # shorter than 128 — crypto excerpts are concise\n",
        "BATCH_SIZE = 4               # distilgpt2 is small → safe to batch 4–8 on CPU\n",
        "\n",
        "for i in tqdm(range(0, len(eval_data), BATCH_SIZE), desc=\"Generating (batched)\"):\n",
        "    batch_items = eval_data[i:i+BATCH_SIZE]\n",
        "    prompts = []\n",
        "\n",
        "    for item in batch_items:\n",
        "        q = item[\"question\"]\n",
        "        hits = retrieve(q, k=3, min_score=0.35)\n",
        "\n",
        "        if not hits:\n",
        "            context = \"No relevant context found.\"\n",
        "            retrieved_chunks = []\n",
        "            cosine_scores = []\n",
        "        else:\n",
        "            context = \"\\n\\n\".join([hit.get(\"text_preview\", hit.get(\"text\", \"\"))[:400] for hit in hits])  # cap length\n",
        "            retrieved_chunks = [hit.get(\"text_preview\", hit.get(\"text\", \"\"))[:300] + \"...\" for hit in hits]\n",
        "            cosine_scores = [hit[\"score\"] for hit in hits]\n",
        "\n",
        "        prompt = f\"\"\"Use only the following context to answer factually. If unsure, say so.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {q}\n",
        "\n",
        "Concise Answer:\"\"\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Batch generate\n",
        "    start_gen = time.time()\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,                # greedy = faster & consistent\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "    answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Clean: remove prompt prefix from each answer\n",
        "    cleaned_answers = []\n",
        "    for prompt, ans in zip(prompts, answers):\n",
        "        if ans.startswith(prompt):\n",
        "            cleaned = ans[len(prompt):].strip()\n",
        "        else:\n",
        "            cleaned = ans.strip()\n",
        "        cleaned_answers.append(cleaned)\n",
        "\n",
        "    gen_time = time.time() - start_gen\n",
        "\n",
        "    # Also time retrieval separately if you want (but batched gen dominates)\n",
        "    for j, item in enumerate(batch_items):\n",
        "        result = {\n",
        "            \"question\": item[\"question\"],\n",
        "            \"ground_truth_excerpt\": item.get(\"ground_truth_excerpt\", \"\"),\n",
        "            \"rag_answer\": cleaned_answers[j],\n",
        "            \"retrieved_chunks_preview\": retrieved_chunks if 'retrieved_chunks' in locals() else [],\n",
        "            \"cosine_scores\": cosine_scores if 'cosine_scores' in locals() else [],\n",
        "            \"generate_time_sec\": round(gen_time / len(batch_items), 2),  # avg per item\n",
        "        }\n",
        "        rag_results.append(result)\n",
        "\n",
        "    # Save partial every ~10 items\n",
        "    current_idx = i + len(batch_items)\n",
        "    if current_idx % 10 == 0 or current_idx >= len(eval_data):\n",
        "        partial_file = f\"rag_outputs_partial_{current_idx}.json\"\n",
        "        with open(partial_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(rag_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Saved partial: {partial_file} ({current_idx}/{len(eval_data)})\")\n",
        "\n",
        "# Final save\n",
        "with open(\"rag_outputs_final.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rag_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nDone! Generated {len(rag_results)} answers. Saved to rag_outputs_final.json\")\n",
        "print(\"Average generate time per question (batched): ~\", round(sum(r[\"generate_time_sec\"] for r in rag_results) / len(rag_results), 1), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "69856c31a1854914b54a94aa195699aa",
            "22689260c19942758e7001c1999a9414",
            "df58fd3422e046deb390fe92eebb61be",
            "f537f42404134829b213c5c02a867579",
            "463de65f3ef04f74a1e5bc45a542a094",
            "ef1a6a7b019246e79a51d20c0cb27b68",
            "c9ff85fdcfce4a458999b5c837dd387f",
            "79870de742be4094b1c256d0d8d14d77",
            "0ebe8bb4f7bc40418888bd7986e8a5ab",
            "a776c2bea6f8468596c2f2ea3895117c",
            "9760277a03de434688d45b8f03bf187e"
          ]
        },
        "id": "P2EJ4rZ20VbG",
        "outputId": "9de8af33-aa5a-4c9b-ad27-de0b9e86f403"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating for 40 questions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating (batched):   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69856c31a1854914b54a94aa195699aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved partial: rag_outputs_partial_20.json (20/40)\n",
            "Saved partial: rag_outputs_partial_40.json (40/40)\n",
            "\n",
            "Done! Generated 40 answers. Saved to rag_outputs_final.json\n",
            "Average generate time per question (batched): ~ 3.4 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# ─── Load generated results ────────────────────────────────────────────────\n",
        "with open(\"rag_outputs_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    rag_results = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(rag_results)} generated answers.\")\n",
        "\n",
        "# ─── Helper: Get embedding for a text (reuse your embedder) ─────────────────\n",
        "def get_embedding(text: str):\n",
        "    if not text.strip():\n",
        "        return np.zeros(384)  # fallback zero vector (dim of bge-small-en-v1.5)\n",
        "    emb = embedder.encode([text], normalize_embeddings=True, convert_to_numpy=True)\n",
        "    return emb[0].astype(np.float32)\n",
        "\n",
        "# ─── Prepare ground-truth embeddings once ──────────────────────────────────\n",
        "gt_embeddings = []\n",
        "for item in rag_results:\n",
        "    gt_text = item.get(\"ground_truth_excerpt\", \"\").strip()\n",
        "    gt_emb = get_embedding(gt_text)\n",
        "    gt_embeddings.append(gt_emb)\n",
        "\n",
        "gt_embeddings = np.array(gt_embeddings)  # shape: (40, 384)\n",
        "\n",
        "# ─── Scorer setup ──────────────────────────────────────────────────────────\n",
        "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# ─── Compute metrics per item ──────────────────────────────────────────────\n",
        "metrics_list = []\n",
        "\n",
        "for idx, res in enumerate(rag_results):\n",
        "    q = res[\"question\"]\n",
        "    gt = res.get(\"ground_truth_excerpt\", \"\").strip()\n",
        "    answer = res[\"rag_answer\"].strip()\n",
        "\n",
        "    # Retrieve the actual retrieved chunks (from generation step)\n",
        "    retrieved_texts = res.get(\"retrieved_chunks_preview\", [])\n",
        "    retrieved_scores = res.get(\"cosine_scores\", [])\n",
        "\n",
        "    # If no chunks retrieved → skip some metrics or mark low\n",
        "    has_retrieval = len(retrieved_texts) > 0 and any(t.strip() for t in retrieved_texts)\n",
        "\n",
        "    # 1. Retrieval metrics (if we have hits)\n",
        "    precision_3 = 0.0\n",
        "    mrr = 0.0\n",
        "\n",
        "    if has_retrieval and idx < len(gt_embeddings):\n",
        "        gt_emb = gt_embeddings[idx]\n",
        "\n",
        "        # Embed retrieved chunks (short previews, but good enough)\n",
        "        ret_embs = np.array([get_embedding(t) for t in retrieved_texts])\n",
        "\n",
        "        cosines = cosine_similarity([gt_emb], ret_embs)[0]  # shape (3,)\n",
        "\n",
        "        # Relevant if cosine >= 0.5 (adjustable threshold)\n",
        "        relevant = cosines >= 0.5\n",
        "        precision_3 = relevant.mean()  # fraction of top-3 relevant\n",
        "\n",
        "        # MRR: reciprocal rank of first relevant\n",
        "        ranks = np.where(relevant)[0]\n",
        "        if len(ranks) > 0:\n",
        "            mrr = 1.0 / (ranks[0] + 1)\n",
        "        else:\n",
        "            mrr = 0.0\n",
        "\n",
        "    # 2. Generation metrics\n",
        "    rouge_l = rouge.score(gt, answer)['rougeL'].fmeasure if gt else 0.0\n",
        "\n",
        "    # Semantic similarity (answer vs ground truth) — your embedder as proxy for BERTScore\n",
        "    ans_emb = get_embedding(answer)\n",
        "    gt_emb = gt_embeddings[idx] if idx < len(gt_embeddings) else np.zeros(384)\n",
        "    semantic_sim = cosine_similarity([ans_emb], [gt_emb])[0][0] if np.any(gt_emb) else 0.0\n",
        "\n",
        "    # 3. Hallucination proxy: % sentences with low support from any retrieved chunk\n",
        "    halluc_rate = 0.0\n",
        "    if has_retrieval and answer:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        if sentences:\n",
        "            sent_embs = np.array([get_embedding(s) for s in sentences])\n",
        "            chunk_embs = np.array([get_embedding(t) for t in retrieved_texts if t.strip()])\n",
        "\n",
        "            if len(chunk_embs) > 0:\n",
        "                max_cosines = cosine_similarity(sent_embs, chunk_embs).max(axis=1)\n",
        "                unsupported = max_cosines < 0.30  # threshold: below 0.3 = likely hallucinated\n",
        "                halluc_rate = unsupported.mean()\n",
        "            else:\n",
        "                halluc_rate = 1.0  # all unsupported if no chunks\n",
        "    elif not gt:\n",
        "        halluc_rate = 0.0  # neutral if no ground truth\n",
        "\n",
        "    metrics = {\n",
        "        \"question\": q[:80] + \"...\" if len(q) > 80 else q,  # shorten for table\n",
        "        \"precision@3\": round(precision_3, 3),\n",
        "        \"mrr\": round(mrr, 3),\n",
        "        \"rougeL\": round(rouge_l, 3),\n",
        "        \"semantic_sim\": round(semantic_sim, 3),\n",
        "        \"halluc_rate\": round(halluc_rate, 3),\n",
        "        \"answer_length\": len(answer.split()),\n",
        "    }\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "# ─── Aggregate & show table ────────────────────────────────────────────────\n",
        "df = pd.DataFrame(metrics_list)\n",
        "\n",
        "# Summary stats\n",
        "summary = df[[\"precision@3\", \"mrr\", \"rougeL\", \"semantic_sim\", \"halluc_rate\"]].mean().round(3)\n",
        "print(\"\\nAggregate Metrics (average over 40 questions):\")\n",
        "print(summary.to_string())\n",
        "\n",
        "print(\"\\nFull table preview (first 5):\")\n",
        "print(tabulate(df.head(5), headers=\"keys\", tablefmt=\"simple\", showindex=False))\n",
        "\n",
        "# Save everything\n",
        "df.to_csv(\"rag_metrics_detailed.csv\", index=False)\n",
        "summary.to_csv(\"rag_metrics_summary.csv\")\n",
        "\n",
        "print(\"\\nSaved: rag_metrics_detailed.csv + rag_metrics_summary.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2mIstvf11NF",
        "outputId": "d94c6c52-7fa5-4c1f-f502-acb81f4505af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 40 generated answers.\n",
            "\n",
            "Aggregate Metrics (average over 40 questions):\n",
            "precision@3     0.892\n",
            "mrr             0.925\n",
            "rougeL          0.158\n",
            "semantic_sim    0.715\n",
            "halluc_rate     0.000\n",
            "\n",
            "Full table preview (first 5):\n",
            "question                                                                               precision@3    mrr    rougeL    semantic_sim    halluc_rate    answer_length\n",
            "-----------------------------------------------------------------------------------  -------------  -----  --------  --------------  -------------  ---------------\n",
            "What is the requirement for the network to remain secure against attacker nodes?             0          0     0.074           0.634              0               87\n",
            "What are the two components that fund the incentive for nodes to support the net...          1          1     0.085           0.692              0               84\n",
            "What are the two primary functions of the Chainlink core node software?                      0.333      1     0.465           0.86               0               69\n",
            "What is the purpose of the 'liquidity accumulator' introduced in the Uniswap v3 ...          1          1     0.196           0.777              0               67\n",
            "How does Proof of History protect the network against long-range attacks?                    1          1     0.211           0.774              0              184\n",
            "\n",
            "Saved: rag_metrics_detailed.csv + rag_metrics_summary.csv\n"
          ]
        }
      ]
    }
  ]
}